{"chunk_id": "525fdfff378cc1ca15548144", "file_path": "mcp_server_enhanced.py", "start_line": 1, "end_line": 80, "content": "# mcp_server_enhanced.py\n\"\"\"\nServidor MCP melhorado com busca semÃ¢ntica e auto-indexaÃ§Ã£o\nUsando FastMCP para API simplificada com decorators\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, List\n\ntry:\n    from mcp.server.fastmcp import FastMCP\n    HAS_MCP = True\n    HAS_FASTMCP = True\nexcept ImportError:\n    try:\n        # Fallback para versÃµes mais antigas\n        from mcp.server import Server\n        from mcp import types\n        HAS_MCP = True\n        HAS_FASTMCP = False\n    except ImportError:\n        sys.stderr.write(\"[mcp_server_enhanced] ERROR: MCP SDK nÃ£o encontrado. Instale `mcp`.\\n\")\n        raise\n\n# Importa funcionalidades melhoradas\ntry:\n    from code_indexer_enhanced import (\n        EnhancedCodeIndexer,\n        enhanced_search_code,\n        enhanced_build_context_pack,\n        enhanced_index_repo_paths,\n        BaseCodeIndexer,\n        search_code,\n        build_context_pack,\n        index_repo_paths\n    )\n    HAS_ENHANCED = True\n    sys.stderr.write(\"[mcp_server_enhanced] âœ… Funcionalidades melhoradas carregadas\\n\")\nexcept ImportError as e:\n    sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Funcionalidades melhoradas nÃ£o disponÃ­veis: {e}\\n\")\n    sys.stderr.write(\"[mcp_server_enhanced] ðŸ”„ Usando versÃ£o base integrada\\n\")\n\n    # Fallback para versÃ£o base integrada\n    try:\n        from code_indexer_enhanced import (\n            BaseCodeIndexer,\n            search_code,\n            build_context_pack,\n            index_repo_paths\n        )\n        HAS_ENHANCED = False\n    except ImportError as e2:\n        sys.stderr.write(f\"[mcp_server_enhanced] âŒ Erro crÃ­tico: {e2}\\n\")\n        raise\n\nHAS_ENHANCED_FEATURES = HAS_ENHANCED\n\n# Config / instÃ¢ncias\nINDEX_DIR = os.environ.get(\"INDEX_DIR\", \".mcp_index\")\nINDEX_ROOT = os.environ.get(\"INDEX_ROOT\", os.getcwd())\n\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)", "mtime": 1755725603.7595248, "terms": ["mcp_server_enhanced", "py", "servidor", "mcp", "melhorado", "com", "busca", "sem", "ntica", "auto", "indexa", "usando", "fastmcp", "para", "api", "simplificada", "com", "decorators", "import", "os", "import", "sys", "from", "typing", "import", "any", "dict", "list", "try", "from", "mcp", "server", "fastmcp", "import", "fastmcp", "has_mcp", "true", "has_fastmcp", "true", "except", "importerror", "try", "fallback", "para", "vers", "es", "mais", "antigas", "from", "mcp", "server", "import", "server", "from", "mcp", "import", "types", "has_mcp", "true", "has_fastmcp", "false", "except", "importerror", "sys", "stderr", "write", "mcp_server_enhanced", "error", "mcp", "sdk", "encontrado", "instale", "mcp", "raise", "importa", "funcionalidades", "melhoradas", "try", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "true", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "carregadas", "except", "importerror", "as", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "dispon", "veis", "sys", "stderr", "write", "mcp_server_enhanced", "usando", "vers", "base", "integrada", "fallback", "para", "vers", "base", "integrada", "try", "from", "code_indexer_enhanced", "import", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "false", "except", "importerror", "as", "e2", "sys", "stderr", "write", "mcp_server_enhanced", "erro", "cr", "tico", "e2", "raise", "has_enhanced_features", "has_enhanced", "config", "inst", "ncias", "index_dir", "os", "environ", "get", "index_dir", "mcp_index", "index_root", "os", "environ", "get", "index_root", "os", "getcwd", "if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path"]}
{"chunk_id": "d64aec4e862e9210eb3b413d", "file_path": "mcp_server_enhanced.py", "start_line": 69, "end_line": 148, "content": "    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n        \n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n            \n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive\n            )\n        \n        return result\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, ", "mtime": 1755725603.7595248, "terms": ["_indexer", "basecodeindexer", "index_dir", "index_dir", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer"]}
{"chunk_id": "402f31fa1234ea354ac0bd8c", "file_path": "mcp_server_enhanced.py", "start_line": 73, "end_line": 152, "content": "def _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n        \n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n            \n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive\n            )\n        \n        return result\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        ", "mtime": 1755725603.7595248, "terms": ["def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget"]}
{"chunk_id": "69f2a81a39e962d4e4a977c2", "file_path": "mcp_server_enhanced.py", "start_line": 107, "end_line": 186, "content": "def _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:", "mtime": 1755725603.7595248, "terms": ["def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as"]}
{"chunk_id": "af1a24781b9552b215ed9b85", "file_path": "mcp_server_enhanced.py", "start_line": 133, "end_line": 212, "content": "def _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"", "mtime": 1755725603.7595248, "terms": ["def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache"]}
{"chunk_id": "c35424b37aea9bc7905d47ea", "file_path": "mcp_server_enhanced.py", "start_line": 137, "end_line": 216, "content": "    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':", "mtime": 1755725603.7595248, "terms": ["try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear"]}
{"chunk_id": "97d2eb55b420ae7e96017f88", "file_path": "mcp_server_enhanced.py", "start_line": 163, "end_line": 242, "content": "def _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()", "mtime": 1755725603.7595248, "terms": ["def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool"]}
{"chunk_id": "0c5147e904add8f218522691", "file_path": "mcp_server_enhanced.py", "start_line": 189, "end_line": 268, "content": "def _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:", "mtime": 1755725603.7595248, "terms": ["def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict"]}
{"chunk_id": "26da784ffaf79e796c5c7b5a", "file_path": "mcp_server_enhanced.py", "start_line": 205, "end_line": 284, "content": "            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server", "mtime": 1755725603.7595248, "terms": ["stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server"]}
{"chunk_id": "c842824a4c1b69c9c4413dab", "file_path": "mcp_server_enhanced.py", "start_line": 211, "end_line": 290, "content": "def _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():", "mtime": 1755725603.7595248, "terms": ["def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools"]}
{"chunk_id": "6d75d0b05ec70dfe3fefac93", "file_path": "mcp_server_enhanced.py", "start_line": 243, "end_line": 322, "content": "    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", ", "mtime": 1755725603.7595248, "terms": ["def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms"]}
{"chunk_id": "e53909575b4112a562e92eff", "file_path": "mcp_server_enhanced.py", "start_line": 252, "end_line": 331, "content": "    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]", "mtime": 1755725603.7595248, "terms": ["def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query"]}
{"chunk_id": "cf589303ecf2c663fc5f0d54", "file_path": "mcp_server_enhanced.py", "start_line": 260, "end_line": 339, "content": "    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {", "mtime": 1755725603.7595248, "terms": ["def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties"]}
{"chunk_id": "6a106e3a4b64351fae44126f", "file_path": "mcp_server_enhanced.py", "start_line": 268, "end_line": 347, "content": "    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",", "mtime": 1755725603.7595248, "terms": ["def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats"]}
{"chunk_id": "0c885805d07d19602ac739f3", "file_path": "mcp_server_enhanced.py", "start_line": 273, "end_line": 352, "content": "    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", ", "mtime": 1755725603.7595248, "terms": ["def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management"]}
{"chunk_id": "c66a38357e225d401b5f9b36", "file_path": "mcp_server_enhanced.py", "start_line": 278, "end_line": 357, "content": "    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", \n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},", "mtime": 1755725603.7595248, "terms": ["def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status"]}
{"chunk_id": "998ec497be3e419face9fa35", "file_path": "mcp_server_enhanced.py", "start_line": 341, "end_line": 420, "content": "                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", \n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"cache_type\": {\"type\": \"string\", \"default\": \"all\"}\n                    }\n                }\n            )\n        ]\n\n    @server.call_tool()\n    async def call_tool(name: str, arguments: dict):\n        if name == \"index_path\":\n            result = _handle_index_path(\n                arguments.get(\"path\", \".\"),\n                arguments.get(\"recursive\", True), \n                arguments.get(\"enable_semantic\", True),\n                arguments.get(\"auto_start_watcher\", False),\n                arguments.get(\"exclude_globs\")\n            )\n        elif name == \"search_code\":\n            result = _handle_search_code(\n                arguments[\"query\"],\n                arguments.get(\"limit\", 10),\n                arguments.get(\"semantic_weight\", 0.3),\n                arguments.get(\"use_mmr\", True)\n            )\n        elif name == \"context_pack\":\n            result = _handle_context_pack(\n                arguments[\"query\"],\n                arguments.get(\"token_budget\", 8000),\n                arguments.get(\"max_chunks\", 20),\n                arguments.get(\"strategy\", \"mmr\")\n            )\n        elif name == \"auto_index\":\n            result = _handle_auto_index(\n                arguments.get(\"action\", \"status\"),\n                arguments.get(\"paths\"),\n                arguments.get(\"recursive\", True)\n            )\n        elif name == \"get_stats\":\n            result = _handle_get_stats()\n        elif name == \"cache_management\":\n            result = _handle_cache_management(\n                arguments.get(\"action\", \"status\"),\n                arguments.get(\"cache_type\", \"all\")\n            )\n        else:\n            result = {\"status\": \"error\", \"error\": f\"Tool {name} not found\"}\n        \n        return [TextContent(type=\"text\", text=str(result))]\n\n# ===== INICIALIZAÃ‡ÃƒO DO SERVIDOR =====\n\nif __name__ == \"__main__\":\n    # Log de inicializaÃ§Ã£o\n    sys.stderr.write(\"ðŸ”„ Iniciando servidor MCP melhorado...\\n\")\n    \n    # Mostra status das funcionalidades  \n    if HAS_ENHANCED_FEATURES:\n        sys.stderr.write(\"âœ… Sistema de busca semÃ¢ntica ativado\\n\")\n        sys.stderr.write(\"âœ… Sistema de auto-indexaÃ§Ã£o ativado\\n\")\n    else:\n        sys.stderr.write(\"âš ï¸  [mcp_server_enhanced] Recursos bÃ¡sicos apenas\\n\")\n        sys.stderr.write(\"   ðŸ’¡ Instale sentence-transformers e watchdog para recursos completos\\n\")\n    \n    sys.stderr.write(\"âœ… [mcp_server_enhanced] Servidor MCP melhorado iniciado\\n\")", "mtime": 1755725603.7595248, "terms": ["paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "cache_type", "type", "string", "default", "all", "server", "call_tool", "async", "def", "call_tool", "name", "str", "arguments", "dict", "if", "name", "index_path", "result", "_handle_index_path", "arguments", "get", "path", "arguments", "get", "recursive", "true", "arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "elif", "name", "search_code", "result", "_handle_search_code", "arguments", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack", "result", "_handle_context_pack", "arguments", "query", "arguments", "get", "token_budget", "arguments", "get", "max_chunks", "arguments", "get", "strategy", "mmr", "elif", "name", "auto_index", "result", "_handle_auto_index", "arguments", "get", "action", "status", "arguments", "get", "paths", "arguments", "get", "recursive", "true", "elif", "name", "get_stats", "result", "_handle_get_stats", "elif", "name", "cache_management", "result", "_handle_cache_management", "arguments", "get", "action", "status", "arguments", "get", "cache_type", "all", "else", "result", "status", "error", "error", "tool", "name", "not", "found", "return", "textcontent", "type", "text", "text", "str", "result", "inicializa", "do", "servidor", "if", "__name__", "__main__", "log", "de", "inicializa", "sys", "stderr", "write", "iniciando", "servidor", "mcp", "melhorado", "mostra", "status", "das", "funcionalidades", "if", "has_enhanced_features", "sys", "stderr", "write", "sistema", "de", "busca", "sem", "ntica", "ativado", "sys", "stderr", "write", "sistema", "de", "auto", "indexa", "ativado", "else", "sys", "stderr", "write", "mcp_server_enhanced", "recursos", "sicos", "apenas", "sys", "stderr", "write", "instale", "sentence", "transformers", "watchdog", "para", "recursos", "completos", "sys", "stderr", "write", "mcp_server_enhanced", "servidor", "mcp", "melhorado", "iniciado"]}
{"chunk_id": "c1dec021c0dee2f024ed21c3", "file_path": "mcp_server_enhanced.py", "start_line": 409, "end_line": 433, "content": "    # Log de inicializaÃ§Ã£o\n    sys.stderr.write(\"ðŸ”„ Iniciando servidor MCP melhorado...\\n\")\n    \n    # Mostra status das funcionalidades  \n    if HAS_ENHANCED_FEATURES:\n        sys.stderr.write(\"âœ… Sistema de busca semÃ¢ntica ativado\\n\")\n        sys.stderr.write(\"âœ… Sistema de auto-indexaÃ§Ã£o ativado\\n\")\n    else:\n        sys.stderr.write(\"âš ï¸  [mcp_server_enhanced] Recursos bÃ¡sicos apenas\\n\")\n        sys.stderr.write(\"   ðŸ’¡ Instale sentence-transformers e watchdog para recursos completos\\n\")\n    \n    sys.stderr.write(\"âœ… [mcp_server_enhanced] Servidor MCP melhorado iniciado\\n\")\n    sys.stderr.write(f\"   ðŸ“ Ãndice: {INDEX_DIR}\\n\")\n    sys.stderr.write(f\"   ðŸ“ RepositÃ³rio: {INDEX_ROOT}\\n\")\n    sys.stderr.write(f\"   ðŸ§  Busca semÃ¢ntica: {'DisponÃ­vel' if HAS_ENHANCED_FEATURES else 'IndisponÃ­vel'}\\n\")\n    sys.stderr.write(f\"   ðŸ‘ï¸  Auto-indexaÃ§Ã£o: {'DisponÃ­vel' if HAS_ENHANCED_FEATURES else 'IndisponÃ­vel'}\\n\")\n    \n    # Executa servidor com a API correta\n    if HAS_FASTMCP:\n        # FastMCP nÃ£o usa stdio=True, executa diretamente\n        mcp.run()\n    else:\n        # MCP tradicional usa stdio_server\n        import asyncio\n        asyncio.run(stdio_server(server))", "mtime": 1755725603.7595248, "terms": ["log", "de", "inicializa", "sys", "stderr", "write", "iniciando", "servidor", "mcp", "melhorado", "mostra", "status", "das", "funcionalidades", "if", "has_enhanced_features", "sys", "stderr", "write", "sistema", "de", "busca", "sem", "ntica", "ativado", "sys", "stderr", "write", "sistema", "de", "auto", "indexa", "ativado", "else", "sys", "stderr", "write", "mcp_server_enhanced", "recursos", "sicos", "apenas", "sys", "stderr", "write", "instale", "sentence", "transformers", "watchdog", "para", "recursos", "completos", "sys", "stderr", "write", "mcp_server_enhanced", "servidor", "mcp", "melhorado", "iniciado", "sys", "stderr", "write", "ndice", "index_dir", "sys", "stderr", "write", "reposit", "rio", "index_root", "sys", "stderr", "write", "busca", "sem", "ntica", "dispon", "vel", "if", "has_enhanced_features", "else", "indispon", "vel", "sys", "stderr", "write", "auto", "indexa", "dispon", "vel", "if", "has_enhanced_features", "else", "indispon", "vel", "executa", "servidor", "com", "api", "correta", "if", "has_fastmcp", "fastmcp", "usa", "stdio", "true", "executa", "diretamente", "mcp", "run", "else", "mcp", "tradicional", "usa", "stdio_server", "import", "asyncio", "asyncio", "run", "stdio_server", "server"]}
{"chunk_id": "f16a41869d42dee0a41238ff", "file_path": "__init__.py", "start_line": 1, "end_line": 25, "content": "# MCP System - Model Context Protocol\n\"\"\"\nSistema avanÃ§ado de indexaÃ§Ã£o e busca de cÃ³digo para desenvolvimento assistido por IA.\nReduz drasticamente o consumo de tokens fornecendo apenas contexto relevante.\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Assistant\"\n\n# ImportaÃ§Ãµes principais\nfrom .code_indexer_enhanced import (\n    EnhancedCodeIndexer,\n    BaseCodeIndexer,\n    search_code,\n    build_context_pack,\n    index_repo_paths\n)\n\n__all__ = [\n    \"EnhancedCodeIndexer\",\n    \"BaseCodeIndexer\", \n    \"search_code\",\n    \"build_context_pack\",\n    \"index_repo_paths\"\n]", "mtime": 1755701264.1672254, "terms": ["mcp", "system", "model", "context", "protocol", "sistema", "avan", "ado", "de", "indexa", "busca", "de", "digo", "para", "desenvolvimento", "assistido", "por", "ia", "reduz", "drasticamente", "consumo", "de", "tokens", "fornecendo", "apenas", "contexto", "relevante", "__version__", "__author__", "assistant", "importa", "es", "principais", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "__all__", "enhancedcodeindexer", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths"]}
{"chunk_id": "b10e7095bcdd6413fc033021", "file_path": "code_indexer_enhanced.py", "start_line": 1, "end_line": 80, "content": "# code_indexer_enhanced.py\n# Sistema MCP melhorado com busca hÃ­brida, auto-indexaÃ§Ã£o e cache inteligente\nfrom __future__ import annotations\nimport os, re, json, math, time, hashlib, threading, csv, datetime as dt\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\n\n#logs para mÃ©tricas\n\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: dict):\n    \"\"\"Append de uma linha de mÃ©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avanÃ§ados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from src.embeddings.semantic_search import SemanticSearchEngine\n    from src.utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILITÃRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------", "mtime": 1755730603.8711107, "terms": ["code_indexer_enhanced", "py", "sistema", "mcp", "melhorado", "com", "busca", "brida", "auto", "indexa", "cache", "inteligente", "from", "__future__", "import", "annotations", "import", "os", "re", "json", "math", "time", "hashlib", "threading", "csv", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "logs", "para", "tricas", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "src", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "src", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence"]}
{"chunk_id": "27768a55e38187835aebdb5d", "file_path": "code_indexer_enhanced.py", "start_line": 12, "end_line": 91, "content": "def _log_metrics(row: dict):\n    \"\"\"Append de uma linha de mÃ©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avanÃ§ados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from src.embeddings.semantic_search import SemanticSearchEngine\n    from src.utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILITÃRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():", "mtime": 1755730603.8711107, "terms": ["def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "src", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "src", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists"]}
{"chunk_id": "b8c197b282c1dcaef0dbd45e", "file_path": "code_indexer_enhanced.py", "start_line": 54, "end_line": 133, "content": "def now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)", "mtime": 1755730603.8711107, "terms": ["def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add"]}
{"chunk_id": "f6e75ee5e5224c473bce27d0", "file_path": "code_indexer_enhanced.py", "start_line": 57, "end_line": 136, "content": "def tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)", "mtime": 1755730603.8711107, "terms": ["def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add"]}
{"chunk_id": "145a2fe3bbbd93ff5e39c143", "file_path": "code_indexer_enhanced.py", "start_line": 60, "end_line": 139, "content": "def est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []", "mtime": 1755730603.8711107, "terms": ["def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks"]}
{"chunk_id": "96f659cdf3150fc11d2650a4", "file_path": "code_indexer_enhanced.py", "start_line": 64, "end_line": 143, "content": "def hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])", "mtime": 1755730603.8711107, "terms": ["def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end"]}
{"chunk_id": "cd76301a2d1d666d69ad8acf", "file_path": "code_indexer_enhanced.py", "start_line": 69, "end_line": 148, "content": "class BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:", "mtime": 1755730603.8711107, "terms": ["class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int"]}
{"chunk_id": "1e31e98351da26f510bf9f59", "file_path": "code_indexer_enhanced.py", "start_line": 70, "end_line": 149, "content": "    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)", "mtime": 1755730603.8711107, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path"]}
{"chunk_id": "72c9ccbb20b9efe98abc52d3", "file_path": "code_indexer_enhanced.py", "start_line": 81, "end_line": 160, "content": "    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)", "mtime": 1755730603.8711107, "terms": ["def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key"]}
{"chunk_id": "3ec55d96ba8803522308392c", "file_path": "code_indexer_enhanced.py", "start_line": 89, "end_line": 168, "content": "    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,", "mtime": 1755730603.8711107, "terms": ["def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end"]}
{"chunk_id": "a0a0ab5d242eb524114570ec", "file_path": "code_indexer_enhanced.py", "start_line": 103, "end_line": 182, "content": "    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}", "mtime": 1755730603.8711107, "terms": ["def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen"]}
{"chunk_id": "5e9566f91fcd434f32c38fcd", "file_path": "code_indexer_enhanced.py", "start_line": 113, "end_line": 192, "content": "    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(", "mtime": 1755730603.8711107, "terms": ["def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths"]}
{"chunk_id": "7bbb3ab8879db52db530e00e", "file_path": "code_indexer_enhanced.py", "start_line": 122, "end_line": 201, "content": "    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0", "mtime": 1755730603.8711107, "terms": ["def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed"]}
{"chunk_id": "048827d2aa67a758342b9752", "file_path": "code_indexer_enhanced.py", "start_line": 137, "end_line": 216, "content": "            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:", "mtime": 1755730603.8711107, "terms": ["max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths"]}
{"chunk_id": "7379b29914f6b52b3b1738a9", "file_path": "code_indexer_enhanced.py", "start_line": 148, "end_line": 227, "content": "    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n", "mtime": 1755730603.8711107, "terms": ["def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass"]}
{"chunk_id": "4361dd209b210e9b3dd0afe7", "file_path": "code_indexer_enhanced.py", "start_line": 179, "end_line": 258, "content": "    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue", "mtime": 1755730603.8711107, "terms": ["def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue"]}
{"chunk_id": "654b17b7b22468178a4289e3", "file_path": "code_indexer_enhanced.py", "start_line": 192, "end_line": 271, "content": "def index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n", "mtime": 1755730603.8711107, "terms": ["def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items"]}
{"chunk_id": "8b32e209a42877f2b64bf639", "file_path": "code_indexer_enhanced.py", "start_line": 205, "end_line": 284, "content": "        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None", "mtime": 1755730603.8711107, "terms": ["pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none"]}
{"chunk_id": "3ca8e1543f215f56d6f65a2e", "file_path": "code_indexer_enhanced.py", "start_line": 233, "end_line": 312, "content": "def _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))", "mtime": 1755730603.8711107, "terms": ["def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys"]}
{"chunk_id": "a96159104c4cf27eadacfaad", "file_path": "code_indexer_enhanced.py", "start_line": 252, "end_line": 331, "content": "def _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],", "mtime": 1755730603.8711107, "terms": ["def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line"]}
{"chunk_id": "82f9d27d125a3dfd10b3f1a7", "file_path": "code_indexer_enhanced.py", "start_line": 264, "end_line": 343, "content": "def _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:", "mtime": 1755730603.8711107, "terms": ["def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str"]}
{"chunk_id": "c08888dfc4fbc24d60d4926c", "file_path": "code_indexer_enhanced.py", "start_line": 272, "end_line": 351, "content": "def _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n", "mtime": 1755730603.8711107, "terms": ["def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens"]}
{"chunk_id": "29e91e98a265d43ca4bcdfb3", "file_path": "code_indexer_enhanced.py", "start_line": 273, "end_line": 352, "content": "    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []", "mtime": 1755730603.8711107, "terms": ["if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked"]}
{"chunk_id": "a1441f371df9f3586788d47e", "file_path": "code_indexer_enhanced.py", "start_line": 280, "end_line": 359, "content": "def _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n", "mtime": 1755730603.8711107, "terms": ["def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break"]}
{"chunk_id": "d919483a9871bdca0990c58f", "file_path": "code_indexer_enhanced.py", "start_line": 302, "end_line": 381, "content": "def search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",", "mtime": 1755730603.8711107, "terms": ["def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end"]}
{"chunk_id": "299c53f919d1572988c90da1", "file_path": "code_indexer_enhanced.py", "start_line": 340, "end_line": 419, "content": "def get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:", "mtime": 1755730603.8711107, "terms": ["def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks"]}
{"chunk_id": "244cb5d079537b170ad28735", "file_path": "code_indexer_enhanced.py", "start_line": 341, "end_line": 420, "content": "    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue", "mtime": 1755730603.8711107, "terms": ["return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue"]}
{"chunk_id": "df5e3988cd850cc2b8468f4d", "file_path": "code_indexer_enhanced.py", "start_line": 343, "end_line": 422, "content": "def _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento", "mtime": 1755730603.8711107, "terms": ["def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento"]}
{"chunk_id": "d916b4219aedc2c2b0939c82", "file_path": "code_indexer_enhanced.py", "start_line": 366, "end_line": 445, "content": "def build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por iteraÃ§Ã£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG MÃ‰TRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),", "mtime": 1755730603.8711107, "terms": ["def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds"]}
{"chunk_id": "797056d12a0c533f0c37aa91", "file_path": "code_indexer_enhanced.py", "start_line": 409, "end_line": 488, "content": "    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por iteraÃ§Ã£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG MÃ‰TRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n            \"query\": query[:160],\n            \"chunk_count\": len(pack[\"chunks\"]),\n            \"total_tokens\": pack[\"total_tokens\"],\n            \"budget_tokens\": budget_tokens,\n            \"budget_utilization\": round(pack[\"total_tokens\"] / max(1, budget_tokens), 3),\n            \"latency_ms\": latency_ms,\n        })\n    except Exception:\n        pass  # nÃ£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de cÃ³digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca semÃ¢ntica com embeddings\n    - Auto-indexaÃ§Ã£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None", "mtime": 1755730603.8711107, "terms": ["for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "query", "query", "chunk_count", "len", "pack", "chunks", "total_tokens", "pack", "total_tokens", "budget_tokens", "budget_tokens", "budget_utilization", "round", "pack", "total_tokens", "max", "budget_tokens", "latency_ms", "latency_ms", "except", "exception", "pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none"]}
{"chunk_id": "2653fa1a7bb5333d3d704ccd", "file_path": "code_indexer_enhanced.py", "start_line": 461, "end_line": 540, "content": "class EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de cÃ³digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca semÃ¢ntica com embeddings\n    - Auto-indexaÃ§Ã£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,", "mtime": 1755730603.8711107, "terms": ["class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "6cbbf718fcf269c6e273699a", "file_path": "code_indexer_enhanced.py", "start_line": 470, "end_line": 549, "content": "    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir", "mtime": 1755730603.8711107, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir"]}
{"chunk_id": "acd7e6f7b109a29ae457f918", "file_path": "code_indexer_enhanced.py", "start_line": 477, "end_line": 556, "content": "        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:", "mtime": 1755730603.8711107, "terms": ["inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock"]}
{"chunk_id": "382f74dfa4b6d6a4c824a0a0", "file_path": "code_indexer_enhanced.py", "start_line": 516, "end_line": 595, "content": "    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o", "mtime": 1755730603.8711107, "terms": ["def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr"]}
{"chunk_id": "13d6da67628311c1d48b9788", "file_path": "code_indexer_enhanced.py", "start_line": 537, "end_line": 616, "content": "    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25", "mtime": 1755730603.8711107, "terms": ["def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25"]}
{"chunk_id": "9132a313d4c2bc6da021e0bd", "file_path": "code_indexer_enhanced.py", "start_line": 545, "end_line": 624, "content": "        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca hÃ­brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,", "mtime": 1755730603.8711107, "terms": ["args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results"]}
{"chunk_id": "0afddbb70bc8153c79759101", "file_path": "code_indexer_enhanced.py", "start_line": 582, "end_line": 661, "content": "    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca hÃ­brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âš ï¸  Erro na busca semÃ¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )", "mtime": 1755730603.8711107, "terms": ["def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic"]}
{"chunk_id": "49c7c6b0ad99849652510cd4", "file_path": "code_indexer_enhanced.py", "start_line": 613, "end_line": 692, "content": "            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca hÃ­brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âš ï¸  Erro na busca semÃ¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa funÃ§Ã£o de construÃ§Ã£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela funÃ§Ã£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # ConstrÃ³i pack usando funÃ§Ã£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informaÃ§Ãµes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}", "mtime": 1755730603.8711107, "terms": ["sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str"]}
{"chunk_id": "9afd6b3fe36c5568d4e4e1f1", "file_path": "code_indexer_enhanced.py", "start_line": 637, "end_line": 716, "content": "    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa funÃ§Ã£o de construÃ§Ã£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela funÃ§Ã£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # ConstrÃ³i pack usando funÃ§Ã£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informaÃ§Ãµes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    ", "mtime": 1755730603.8711107, "terms": ["def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false"]}
{"chunk_id": "35752f0da946c62c848d878b", "file_path": "code_indexer_enhanced.py", "start_line": 681, "end_line": 760, "content": "            \n            # Adiciona informaÃ§Ãµes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"", "mtime": 1755730603.8711107, "terms": ["adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr"]}
{"chunk_id": "668375cfff581667e4f9391e", "file_path": "code_indexer_enhanced.py", "start_line": 694, "end_line": 771, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "0cf543ad0e3f823263eb2122", "file_path": "code_indexer_enhanced.py", "start_line": 705, "end_line": 771, "content": "    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "c7af84bf38bad457a493bb32", "file_path": "code_indexer_enhanced.py", "start_line": 717, "end_line": 771, "content": "    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "2af217ebb1c8db14d4f633ab", "file_path": "code_indexer_enhanced.py", "start_line": 731, "end_line": 771, "content": "def enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "eebb0e7bbd2d471620ef3e4e", "file_path": "code_indexer_enhanced.py", "start_line": 746, "end_line": 771, "content": "def enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "c0da20d4b426623494a55d51", "file_path": "code_indexer_enhanced.py", "start_line": 749, "end_line": 771, "content": "    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "18bc80edac607c608c1402e4", "file_path": "code_indexer_enhanced.py", "start_line": 755, "end_line": 771, "content": "def enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "0bba6801cff3c217b480bf79", "file_path": "reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diretÃ³rio de Ã­ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de mÃ©tricas (MCP_METRICS_FILE ou .mcp_index/metrics.csv).\n- (Opcional) Calcula baseline aproximada de tokens do repositÃ³rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# ---- Config mÃ©tricas (mesmo padrÃ£o do context_pack)\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)", "mtime": 1755731278.2792592, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "config", "tricas", "mesmo", "padr", "do", "context_pack", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir"]}
{"chunk_id": "65e30d04e19ef87e8bf8e523", "file_path": "reindex.py", "start_line": 34, "end_line": 113, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1755731278.2792592, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "d8de839123e293bddabe57d6", "file_path": "reindex.py", "start_line": 43, "end_line": 122, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1755731278.2792592, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "5ae6096b36d042f60fa99f45", "file_path": "reindex.py", "start_line": 47, "end_line": 126, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1755731278.2792592, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "b7d68729e9b09df68aa8f961", "file_path": "reindex.py", "start_line": 65, "end_line": 144, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"ðŸ“Š Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1755731278.2792592, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "bfc299e4c1673c806f2690b1", "file_path": "reindex.py", "start_line": 69, "end_line": 148, "content": "\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"ðŸ“Š Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,", "mtime": 1755731278.2792592, "terms": ["importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed"]}
{"chunk_id": "327882a4217d07453fd44454", "file_path": "reindex.py", "start_line": 137, "end_line": 163, "content": "    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # nÃ£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1755731278.2792592, "terms": ["loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "f5c4d977878c14ec095b435d", "file_path": "setup_enhanced_mcp.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSetup automÃ¡tico do sistema MCP melhorado\nConfigura busca semÃ¢ntica, auto-indexaÃ§Ã£o e otimizaÃ§Ãµes\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\ndef print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")", "mtime": 1755701372.735274, "terms": ["usr", "bin", "env", "python3", "setup", "autom", "tico", "do", "sistema", "mcp", "melhorado", "configura", "busca", "sem", "ntica", "auto", "indexa", "otimiza", "es", "import", "os", "import", "sys", "import", "subprocess", "import", "time", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "any", "def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn"]}
{"chunk_id": "2d583ca387f4256c4101b3a8", "file_path": "setup_enhanced_mcp.py", "start_line": 14, "end_line": 93, "content": "def print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")", "mtime": 1755701372.735274, "terms": ["def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt"]}
{"chunk_id": "64ff037144681d7c65f86e41", "file_path": "setup_enhanced_mcp.py", "start_line": 20, "end_line": 99, "content": "def print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            ", "mtime": 1755701372.735274, "terms": ["def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true"]}
{"chunk_id": "2d39b52c8fd0280e613a4454", "file_path": "setup_enhanced_mcp.py", "start_line": 31, "end_line": 110, "content": "def check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n        \n        # InstalaÃ§Ã£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep", "mtime": 1755701372.735274, "terms": ["def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep"]}
{"chunk_id": "514599c8ced1d4f461743cdd", "file_path": "setup_enhanced_mcp.py", "start_line": 69, "end_line": 148, "content": "    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n        \n        # InstalaÃ§Ã£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(f\"{dep} instalado\", \"ok\")\n            else:\n                print_step(f\"Erro ao instalar {dep}: {result.stderr}\", \"warn\")\n                \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro na instalaÃ§Ã£o: {e}\", \"error\")\n        return False\n\ndef setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"", "mtime": 1755701372.735274, "terms": ["except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "dep", "instalado", "ok", "else", "print_step", "erro", "ao", "instalar", "dep", "result", "stderr", "warn", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "instala", "error", "return", "false", "def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json"]}
{"chunk_id": "995f65108b625e37a8ac5052", "file_path": "setup_enhanced_mcp.py", "start_line": 84, "end_line": 163, "content": "def install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n        \n        # InstalaÃ§Ã£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(f\"{dep} instalado\", \"ok\")\n            else:\n                print_step(f\"Erro ao instalar {dep}: {result.stderr}\", \"warn\")\n                \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro na instalaÃ§Ã£o: {e}\", \"error\")\n        return False\n\ndef setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"ConfiguraÃ§Ã£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexaÃ§Ã£o inicial do projeto\"\"\"\n    print_header(\"IndexaÃ§Ã£o Inicial do Projeto\")\n\n    try:", "mtime": 1755701372.735274, "terms": ["def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "dep", "instalado", "ok", "else", "print_step", "erro", "ao", "instalar", "dep", "result", "stderr", "warn", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "instala", "error", "return", "false", "def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try"]}
{"chunk_id": "2e1e0fb2d9f8e82d67781d3a", "file_path": "setup_enhanced_mcp.py", "start_line": 124, "end_line": 203, "content": "def setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"ConfiguraÃ§Ã£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexaÃ§Ã£o inicial do projeto\"\"\"\n    print_header(\"IndexaÃ§Ã£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # NÃ£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"funÃ§Ã£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado nÃ£o disponÃ­vel, usando versÃ£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # IndexaÃ§Ã£o bÃ¡sica usando funÃ§Ã£o integrada\n            print_step(\"Indexando com versÃ£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n", "mtime": 1755701372.735274, "terms": ["def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks"]}
{"chunk_id": "292fc4caad52275461186d32", "file_path": "setup_enhanced_mcp.py", "start_line": 137, "end_line": 216, "content": "                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"ConfiguraÃ§Ã£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexaÃ§Ã£o inicial do projeto\"\"\"\n    print_header(\"IndexaÃ§Ã£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # NÃ£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"funÃ§Ã£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado nÃ£o disponÃ­vel, usando versÃ£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # IndexaÃ§Ã£o bÃ¡sica usando funÃ§Ã£o integrada\n            print_step(\"Indexando com versÃ£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n\n            print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n            return True\n        except Exception as e:\n            print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP estÃ¡ funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    ", "mtime": 1755701372.735274, "terms": ["args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp"]}
{"chunk_id": "0c9119775e982ce45e9d9908", "file_path": "setup_enhanced_mcp.py", "start_line": 159, "end_line": 238, "content": "def initial_indexing() -> bool:\n    \"\"\"Executa indexaÃ§Ã£o inicial do projeto\"\"\"\n    print_header(\"IndexaÃ§Ã£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # NÃ£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"funÃ§Ã£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado nÃ£o disponÃ­vel, usando versÃ£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # IndexaÃ§Ã£o bÃ¡sica usando funÃ§Ã£o integrada\n            print_step(\"Indexando com versÃ£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n\n            print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n            return True\n        except Exception as e:\n            print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP estÃ¡ funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importaÃ§Ã£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools estÃ£o disponÃ­veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools disponÃ­veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  â€¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"", "mtime": 1755701372.735274, "terms": ["def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema"]}
{"chunk_id": "972dc0bde8ae8b3c39d7f083", "file_path": "setup_enhanced_mcp.py", "start_line": 205, "end_line": 284, "content": "            return True\n        except Exception as e:\n            print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexaÃ§Ã£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP estÃ¡ funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importaÃ§Ã£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools estÃ£o disponÃ­veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools disponÃ­veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  â€¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nðŸŽ¯ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nðŸ“‹ COMANDOS PRINCIPAIS:\n\n1ï¸âƒ£ Via MCP Tools (no Claude/cursor):\n   â€¢ index_path: Indexa arquivos/diretÃ³rios\n   â€¢ search_code: Busca hÃ­brida BM25 + semÃ¢ntica\n   â€¢ context_pack: Gera contexto orÃ§amentado\n   â€¢ auto_index: Controla auto-indexaÃ§Ã£o\n   â€¢ get_stats: EstatÃ­sticas do sistema\n   â€¢ cache_management: Gerencia caches\n\n2ï¸âƒ£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3ï¸âƒ£ Auto-indexaÃ§Ã£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudanÃ§as automaticamente\n   ```\n\nðŸ”§ CONFIGURAÃ‡Ã•ES:\n\nâ€¢ Ãndice armazenado em: .mcp_index/\nâ€¢ Busca semÃ¢ntica: Ativada (se sentence-transformers disponÃ­vel)\nâ€¢ Auto-indexaÃ§Ã£o: DisponÃ­vel (se watchdog disponÃ­vel)\nâ€¢ Cache persistente: Sim\nâ€¢ OrÃ§amento de tokens: ConfigurÃ¡vel\n\nâš¡ PRÃ“XIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar cÃ³digo\n3. Configure auto-indexaÃ§Ã£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nðŸ“Š BENEFÃCIOS:\n", "mtime": 1755701372.735274, "terms": ["return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios"]}
{"chunk_id": "8a0b7abadd232bb1004f0793", "file_path": "setup_enhanced_mcp.py", "start_line": 213, "end_line": 292, "content": "def test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP estÃ¡ funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importaÃ§Ã£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools estÃ£o disponÃ­veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools disponÃ­veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  â€¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nðŸŽ¯ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nðŸ“‹ COMANDOS PRINCIPAIS:\n\n1ï¸âƒ£ Via MCP Tools (no Claude/cursor):\n   â€¢ index_path: Indexa arquivos/diretÃ³rios\n   â€¢ search_code: Busca hÃ­brida BM25 + semÃ¢ntica\n   â€¢ context_pack: Gera contexto orÃ§amentado\n   â€¢ auto_index: Controla auto-indexaÃ§Ã£o\n   â€¢ get_stats: EstatÃ­sticas do sistema\n   â€¢ cache_management: Gerencia caches\n\n2ï¸âƒ£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3ï¸âƒ£ Auto-indexaÃ§Ã£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudanÃ§as automaticamente\n   ```\n\nðŸ”§ CONFIGURAÃ‡Ã•ES:\n\nâ€¢ Ãndice armazenado em: .mcp_index/\nâ€¢ Busca semÃ¢ntica: Ativada (se sentence-transformers disponÃ­vel)\nâ€¢ Auto-indexaÃ§Ã£o: DisponÃ­vel (se watchdog disponÃ­vel)\nâ€¢ Cache persistente: Sim\nâ€¢ OrÃ§amento de tokens: ConfigurÃ¡vel\n\nâš¡ PRÃ“XIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar cÃ³digo\n3. Configure auto-indexaÃ§Ã£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nðŸ“Š BENEFÃCIOS:\n\nâœ… 95% menos tokens irrelevantes\nâœ… Busca semÃ¢ntica + lexical hÃ­brida\nâœ… Auto-reindexaÃ§Ã£o em mudanÃ§as\nâœ… Cache inteligente persistente\nâœ… OrÃ§amento de contexto controlado\n\nðŸ†˜ SUPORTE:\n", "mtime": 1755701372.735274, "terms": ["def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte"]}
{"chunk_id": "2171df10feac9d6fa11f98f0", "file_path": "setup_enhanced_mcp.py", "start_line": 237, "end_line": 316, "content": "def show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nðŸŽ¯ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nðŸ“‹ COMANDOS PRINCIPAIS:\n\n1ï¸âƒ£ Via MCP Tools (no Claude/cursor):\n   â€¢ index_path: Indexa arquivos/diretÃ³rios\n   â€¢ search_code: Busca hÃ­brida BM25 + semÃ¢ntica\n   â€¢ context_pack: Gera contexto orÃ§amentado\n   â€¢ auto_index: Controla auto-indexaÃ§Ã£o\n   â€¢ get_stats: EstatÃ­sticas do sistema\n   â€¢ cache_management: Gerencia caches\n\n2ï¸âƒ£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3ï¸âƒ£ Auto-indexaÃ§Ã£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudanÃ§as automaticamente\n   ```\n\nðŸ”§ CONFIGURAÃ‡Ã•ES:\n\nâ€¢ Ãndice armazenado em: .mcp_index/\nâ€¢ Busca semÃ¢ntica: Ativada (se sentence-transformers disponÃ­vel)\nâ€¢ Auto-indexaÃ§Ã£o: DisponÃ­vel (se watchdog disponÃ­vel)\nâ€¢ Cache persistente: Sim\nâ€¢ OrÃ§amento de tokens: ConfigurÃ¡vel\n\nâš¡ PRÃ“XIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar cÃ³digo\n3. Configure auto-indexaÃ§Ã£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nðŸ“Š BENEFÃCIOS:\n\nâœ… 95% menos tokens irrelevantes\nâœ… Busca semÃ¢ntica + lexical hÃ­brida\nâœ… Auto-reindexaÃ§Ã£o em mudanÃ§as\nâœ… Cache inteligente persistente\nâœ… OrÃ§amento de contexto controlado\n\nðŸ†˜ SUPORTE:\n\nâ€¢ Verifique logs: get_stats\nâ€¢ Limpe cache: cache_management {\"action\": \"clear\"}\nâ€¢ Reinicie indexaÃ§Ã£o: Apague .mcp_index/ e reindexe\n\"\"\")\n\ndef main():\n    \"\"\"FunÃ§Ã£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurarÃ¡ busca semÃ¢ntica + auto-indexaÃ§Ã£o\")\n    \n    # 1. Verificar dependÃªncias\n    deps = check_dependencies()\n    \n    # 2. Instalar dependÃªncias se necessÃ¡rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nðŸ’¡ DependÃªncias opcionais em falta: {', '.join(missing)}\")", "mtime": 1755701372.735274, "terms": ["def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte", "verifique", "logs", "get_stats", "limpe", "cache", "cache_management", "action", "clear", "reinicie", "indexa", "apague", "mcp_index", "reindexe", "def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing"]}
{"chunk_id": "c63f56149a642e057c779d49", "file_path": "setup_enhanced_mcp.py", "start_line": 273, "end_line": 349, "content": "â€¢ Cache persistente: Sim\nâ€¢ OrÃ§amento de tokens: ConfigurÃ¡vel\n\nâš¡ PRÃ“XIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar cÃ³digo\n3. Configure auto-indexaÃ§Ã£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nðŸ“Š BENEFÃCIOS:\n\nâœ… 95% menos tokens irrelevantes\nâœ… Busca semÃ¢ntica + lexical hÃ­brida\nâœ… Auto-reindexaÃ§Ã£o em mudanÃ§as\nâœ… Cache inteligente persistente\nâœ… OrÃ§amento de contexto controlado\n\nðŸ†˜ SUPORTE:\n\nâ€¢ Verifique logs: get_stats\nâ€¢ Limpe cache: cache_management {\"action\": \"clear\"}\nâ€¢ Reinicie indexaÃ§Ã£o: Apague .mcp_index/ e reindexe\n\"\"\")\n\ndef main():\n    \"\"\"FunÃ§Ã£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurarÃ¡ busca semÃ¢ntica + auto-indexaÃ§Ã£o\")\n    \n    # 1. Verificar dependÃªncias\n    deps = check_dependencies()\n    \n    # 2. Instalar dependÃªncias se necessÃ¡rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nðŸ’¡ DependÃªncias opcionais em falta: {', '.join(missing)}\")\n        install = input(\"Deseja instalar agora? (s/N): \").lower().strip()\n        if install in ['s', 'sim', 'y', 'yes']:\n            install_dependencies(missing)\n    \n    # 3. Configurar MCP\n    if not setup_mcp_config():\n        print(\"\\nâŒ Falha na configuraÃ§Ã£o do MCP\")\n        return False\n    \n    # 4. IndexaÃ§Ã£o inicial\n    print(\"\\nðŸ’¡ Executando indexaÃ§Ã£o inicial (pode demorar um pouco)...\")\n    if not initial_indexing():\n        print(\"\\nâš ï¸  Falha na indexaÃ§Ã£o inicial, mas sistema pode funcionar\")\n    \n    # 5. Testar servidor\n    if not test_mcp_server():\n        print(\"\\nâš ï¸  Problemas detectados no servidor MCP\")\n    \n    # 6. Mostrar guia de uso\n    show_usage_guide()\n    \n    print(\"\\nðŸŽ‰ Setup concluÃ­do! Reinicie seu editor para usar o sistema MCP melhorado.\")\n    return True\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\nâš ï¸ Setup cancelado pelo usuÃ¡rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\nâŒ Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte", "verifique", "logs", "get_stats", "limpe", "cache", "cache_management", "action", "clear", "reinicie", "indexa", "apague", "mcp_index", "reindexe", "def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing", "install", "input", "deseja", "instalar", "agora", "lower", "strip", "if", "install", "in", "sim", "yes", "install_dependencies", "missing", "configurar", "mcp", "if", "not", "setup_mcp_config", "print", "falha", "na", "configura", "do", "mcp", "return", "false", "indexa", "inicial", "print", "executando", "indexa", "inicial", "pode", "demorar", "um", "pouco", "if", "not", "initial_indexing", "print", "falha", "na", "indexa", "inicial", "mas", "sistema", "pode", "funcionar", "testar", "servidor", "if", "not", "test_mcp_server", "print", "problemas", "detectados", "no", "servidor", "mcp", "mostrar", "guia", "de", "uso", "show_usage_guide", "print", "setup", "conclu", "do", "reinicie", "seu", "editor", "para", "usar", "sistema", "mcp", "melhorado", "return", "true", "if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "04ef4728811811ee7a0284e5", "file_path": "setup_enhanced_mcp.py", "start_line": 298, "end_line": 349, "content": "def main():\n    \"\"\"FunÃ§Ã£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurarÃ¡ busca semÃ¢ntica + auto-indexaÃ§Ã£o\")\n    \n    # 1. Verificar dependÃªncias\n    deps = check_dependencies()\n    \n    # 2. Instalar dependÃªncias se necessÃ¡rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nðŸ’¡ DependÃªncias opcionais em falta: {', '.join(missing)}\")\n        install = input(\"Deseja instalar agora? (s/N): \").lower().strip()\n        if install in ['s', 'sim', 'y', 'yes']:\n            install_dependencies(missing)\n    \n    # 3. Configurar MCP\n    if not setup_mcp_config():\n        print(\"\\nâŒ Falha na configuraÃ§Ã£o do MCP\")\n        return False\n    \n    # 4. IndexaÃ§Ã£o inicial\n    print(\"\\nðŸ’¡ Executando indexaÃ§Ã£o inicial (pode demorar um pouco)...\")\n    if not initial_indexing():\n        print(\"\\nâš ï¸  Falha na indexaÃ§Ã£o inicial, mas sistema pode funcionar\")\n    \n    # 5. Testar servidor\n    if not test_mcp_server():\n        print(\"\\nâš ï¸  Problemas detectados no servidor MCP\")\n    \n    # 6. Mostrar guia de uso\n    show_usage_guide()\n    \n    print(\"\\nðŸŽ‰ Setup concluÃ­do! Reinicie seu editor para usar o sistema MCP melhorado.\")\n    return True\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\nâš ï¸ Setup cancelado pelo usuÃ¡rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\nâŒ Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing", "install", "input", "deseja", "instalar", "agora", "lower", "strip", "if", "install", "in", "sim", "yes", "install_dependencies", "missing", "configurar", "mcp", "if", "not", "setup_mcp_config", "print", "falha", "na", "configura", "do", "mcp", "return", "false", "indexa", "inicial", "print", "executando", "indexa", "inicial", "pode", "demorar", "um", "pouco", "if", "not", "initial_indexing", "print", "falha", "na", "indexa", "inicial", "mas", "sistema", "pode", "funcionar", "testar", "servidor", "if", "not", "test_mcp_server", "print", "problemas", "detectados", "no", "servidor", "mcp", "mostrar", "guia", "de", "uso", "show_usage_guide", "print", "setup", "conclu", "do", "reinicie", "seu", "editor", "para", "usar", "sistema", "mcp", "melhorado", "return", "true", "if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "680ca7c74f7822fbe34f55e3", "file_path": "setup_enhanced_mcp.py", "start_line": 341, "end_line": 349, "content": "if __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\nâš ï¸ Setup cancelado pelo usuÃ¡rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\nâŒ Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "946797749e83a7383de7e803", "file_path": "embeddings/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Embeddings Module\n\"\"\"\nMÃ³dulo de embeddings semÃ¢nticos para busca avanÃ§ada de cÃ³digo.\n\"\"\"\n\nfrom .semantic_search import *", "mtime": 1755701269.9533663, "terms": ["mcp", "system", "embeddings", "module", "dulo", "de", "embeddings", "sem", "nticos", "para", "busca", "avan", "ada", "de", "digo", "from", "semantic_search", "import"]}
{"chunk_id": "f278ef05522b8657f521598c", "file_path": "embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca semÃ¢ntica usando embeddings locais\nIntegraÃ§Ã£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel", "mtime": 1755697115.1540978, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel"]}
{"chunk_id": "b33e9c866a4325e4972e400b", "file_path": "embeddings/semantic_search.py", "start_line": 24, "end_line": 103, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou", "mtime": 1755697115.1540978, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou"]}
{"chunk_id": "390832dbcf1cbbba6b557366", "file_path": "embeddings/semantic_search.py", "start_line": 32, "end_line": 111, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")", "mtime": 1755697115.1540978, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id"]}
{"chunk_id": "b8c8887984dde3c04857215a", "file_path": "embeddings/semantic_search.py", "start_line": 38, "end_line": 117, "content": "    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]", "mtime": 1755697115.1540978, "terms": ["def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated"]}
{"chunk_id": "14042ffeba35f7c8336cdcf3", "file_path": "embeddings/semantic_search.py", "start_line": 54, "end_line": 133, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding", "mtime": 1755697115.1540978, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding"]}
{"chunk_id": "2b7114f80fb9604f6f9bf4c8", "file_path": "embeddings/semantic_search.py", "start_line": 66, "end_line": 145, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1755697115.1540978, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "88c4fc03b20171daf3298d55", "file_path": "embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:", "mtime": 1755697115.1540978, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args"]}
{"chunk_id": "de8579eee3fadc223fedb0c4", "file_path": "embeddings/semantic_search.py", "start_line": 70, "end_line": 149, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta", "mtime": 1755697115.1540978, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "2c2e60e4379a78b0c7e92e1e", "file_path": "embeddings/semantic_search.py", "start_line": 74, "end_line": 153, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:", "mtime": 1755697115.1540978, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "d5417cb34fa610b6f5431f88", "file_path": "embeddings/semantic_search.py", "start_line": 78, "end_line": 157, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []", "mtime": 1755697115.1540978, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return"]}
{"chunk_id": "fa4f48cfa341aa4c4641ae14", "file_path": "embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()", "mtime": 1755697115.1540978, "terms": ["except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model"]}
{"chunk_id": "6e7033b34fa098b4283eeabb", "file_path": "embeddings/semantic_search.py", "start_line": 143, "end_line": 222, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o", "mtime": 1755697115.1540978, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele"]}
{"chunk_id": "3d7822c05f7559ee97512555", "file_path": "embeddings/semantic_search.py", "start_line": 182, "end_line": 261, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1755697115.1540978, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "756f028a2bc54bd7895c421b", "file_path": "embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))", "mtime": 1755697115.1540978, "terms": ["results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json"]}
{"chunk_id": "07fe09eea88b5a13b413f45e", "file_path": "embeddings/semantic_search.py", "start_line": 267, "end_line": 304, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "8bb83be47838f80ccd1adf20", "file_path": "embeddings/semantic_search.py", "start_line": 273, "end_line": 304, "content": "            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "0280ab6fa07d9e30c9481a80", "file_path": "embeddings/semantic_search.py", "start_line": 281, "end_line": 304, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "028c18221ff1be8b4fbe7719", "file_path": "embeddings/semantic_search.py", "start_line": 296, "end_line": 304, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "fbdb9b0c1f4af8520bffb15c", "file_path": "cache/search_cache.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nMÃ³dulo para gerenciamento de cache de resultados de busca\n\"\"\"\nimport json\nimport hashlib\nimport time\nfrom typing import Any, Dict, Optional\n# Removendo imports nÃ£o utilizados que podem causar problemas\n\nclass SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"", "mtime": 1755690276.2485237, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gerenciamento", "de", "cache", "de", "resultados", "de", "busca", "import", "json", "import", "hashlib", "import", "time", "from", "typing", "import", "any", "dict", "optional", "removendo", "imports", "utilizados", "que", "podem", "causar", "problemas", "class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none"]}
{"chunk_id": "ca5360dc59a6469218343d38", "file_path": "cache/search_cache.py", "start_line": 11, "end_line": 90, "content": "class SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio", "mtime": 1755690276.2485237, "terms": ["class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio"]}
{"chunk_id": "35764bba5e8e74814b43b73f", "file_path": "cache/search_cache.py", "start_line": 14, "end_line": 93, "content": "    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:", "mtime": 1755690276.2485237, "terms": ["def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order"]}
{"chunk_id": "0bd9abd79b3070ece8471666", "file_path": "cache/search_cache.py", "start_line": 27, "end_line": 106, "content": "    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n", "mtime": 1755690276.2485237, "terms": ["def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "df919cc333a84981be2d9730", "file_path": "cache/search_cache.py", "start_line": 48, "end_line": 127, "content": "    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)", "mtime": 1755690276.2485237, "terms": ["def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key"]}
{"chunk_id": "a0dbc1de914801d595c0213e", "file_path": "cache/search_cache.py", "start_line": 69, "end_line": 148, "content": "            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(", "mtime": 1755690276.2485237, "terms": ["del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum"]}
{"chunk_id": "cf539096748fdafe5d215d8d", "file_path": "cache/search_cache.py", "start_line": 79, "end_line": 158, "content": "    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }", "mtime": 1755690276.2485237, "terms": ["def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds"]}
{"chunk_id": "4e9a2127a100d7e5cad1493d", "file_path": "cache/search_cache.py", "start_line": 107, "end_line": 162, "content": "    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "391196799063b14b58506b68", "file_path": "cache/search_cache.py", "start_line": 135, "end_line": 162, "content": "    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "c3bfd9110d6f7e9a55029513", "file_path": "cache/search_cache.py", "start_line": 137, "end_line": 162, "content": "        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "6b1c638eb644117f64d6e630", "file_path": "cache/search_cache.py", "start_line": 140, "end_line": 162, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "1972efb438b746e9d7fed6f9", "file_path": "cache/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Cache Module\n\"\"\"\nSistema de cache para embeddings e Ã­ndices BM25.\n\"\"\"\n\nfrom .search_cache import *", "mtime": 1755701282.161089, "terms": ["mcp", "system", "cache", "module", "sistema", "de", "cache", "para", "embeddings", "ndices", "bm25", "from", "search_cache", "import"]}
{"chunk_id": "99799c722fddee579e00f3aa", "file_path": "utils/__init__.py", "start_line": 1, "end_line": 7, "content": "# MCP System - Utils Module\n\"\"\"\nUtilitÃ¡rios do sistema MCP: embeddings, file watcher, etc.\n\"\"\"\n\nfrom .embeddings import *\nfrom .file_watcher import *", "mtime": 1755701275.657908, "terms": ["mcp", "system", "utils", "module", "utilit", "rios", "do", "sistema", "mcp", "embeddings", "file", "watcher", "etc", "from", "embeddings", "import", "from", "file_watcher", "import"]}
{"chunk_id": "9872699dc5eeca6eabb7622b", "file_path": "utils/embeddings.py", "start_line": 1, "end_line": 70, "content": "#!/usr/bin/env python3\n\"\"\"\nMÃ³dulo para geraÃ§Ã£o de embeddings usando Sentence Transformers\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Union\n\n\nclass EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gera", "de", "embeddings", "usando", "sentence", "transformers", "from", "sentence_transformers", "import", "sentencetransformer", "import", "numpy", "as", "np", "from", "typing", "import", "union", "class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "4b5bbedd0c1045be3de37c60", "file_path": "utils/embeddings.py", "start_line": 10, "end_line": 70, "content": "class EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "2003079116df9e8672c147d0", "file_path": "utils/embeddings.py", "start_line": 13, "end_line": 70, "content": "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "b960a1a51a7a855e3bd1e40a", "file_path": "utils/embeddings.py", "start_line": 24, "end_line": 70, "content": "    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "b4966b83236635bec7fdc9d3", "file_path": "utils/embeddings.py", "start_line": 36, "end_line": 70, "content": "    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "83881cb3a503c39e2e983f27", "file_path": "utils/embeddings.py", "start_line": 57, "end_line": 70, "content": "    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "0c8de0dce3e8419a412cf6cf", "file_path": "utils/embeddings.py", "start_line": 69, "end_line": 70, "content": "# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "42c55ee19958c9b9e0024656", "file_path": "utils/file_watcher.py", "start_line": 1, "end_line": 80, "content": "# src/utils/file_watcher.py\n\"\"\"\nSistema de monitoramento de arquivos para auto-indexaÃ§Ã£o\nDetecta mudanÃ§as e reindexar automaticamente arquivos modificados\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport time\nimport threading\nfrom pathlib import Path\nfrom typing import Set, Callable, Dict, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport hashlib\n\nimport sys\n\ntry:\n    from watchdog.observers import Observer\n    from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent\n    HAS_WATCHDOG = True\nexcept ImportError:\n    HAS_WATCHDOG = False\n    Observer = None\n    FileSystemEventHandler = None\n\n@dataclass\nclass IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False", "mtime": 1755697163.1334832, "terms": ["src", "utils", "file_watcher", "py", "sistema", "de", "monitoramento", "de", "arquivos", "para", "auto", "indexa", "detecta", "mudan", "as", "reindexar", "automaticamente", "arquivos", "modificados", "from", "__future__", "import", "annotations", "import", "os", "import", "time", "import", "threading", "from", "pathlib", "import", "path", "from", "typing", "import", "set", "callable", "dict", "optional", "from", "dataclasses", "import", "dataclass", "from", "concurrent", "futures", "import", "threadpoolexecutor", "import", "hashlib", "import", "sys", "try", "from", "watchdog", "observers", "import", "observer", "from", "watchdog", "events", "import", "filesystemeventhandler", "filemodifiedevent", "filecreatedevent", "filedeletedevent", "has_watchdog", "true", "except", "importerror", "has_watchdog", "false", "observer", "none", "filesystemeventhandler", "none", "dataclass", "class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false"]}
{"chunk_id": "aa9909d08cf52990eff03a5d", "file_path": "utils/file_watcher.py", "start_line": 29, "end_line": 108, "content": "class IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce", "mtime": 1755697163.1334832, "terms": ["class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce"]}
{"chunk_id": "0be2022196a58f0c5fd04023", "file_path": "utils/file_watcher.py", "start_line": 34, "end_line": 113, "content": "class FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:", "mtime": 1755697163.1334832, "terms": ["class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer"]}
{"chunk_id": "12e75180c957b7c2f5fef8ba", "file_path": "utils/file_watcher.py", "start_line": 40, "end_line": 119, "content": "    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks"]}
{"chunk_id": "51297785d4fa096d265474c8", "file_path": "utils/file_watcher.py", "start_line": 69, "end_line": 148, "content": "        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return", "mtime": 1755697163.1334832, "terms": ["if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return"]}
{"chunk_id": "44d4bcbb12417f0801f46869", "file_path": "utils/file_watcher.py", "start_line": 73, "end_line": 152, "content": "    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return", "mtime": 1755697163.1334832, "terms": ["def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return"]}
{"chunk_id": "3c5c11dd453fbff3b69edc0c", "file_path": "utils/file_watcher.py", "start_line": 93, "end_line": 172, "content": "    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n", "mtime": 1755697163.1334832, "terms": ["def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr"]}
{"chunk_id": "7144cd5e1bcf09e39d37a056", "file_path": "utils/file_watcher.py", "start_line": 111, "end_line": 190, "content": "    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')", "mtime": 1755697163.1334832, "terms": ["def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted"]}
{"chunk_id": "2a1c48109869ed62e5286d5d", "file_path": "utils/file_watcher.py", "start_line": 123, "end_line": 202, "content": "    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)", "mtime": 1755697163.1334832, "terms": ["def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr"]}
{"chunk_id": "32e77e46ee5d8fb55df10d4a", "file_path": "utils/file_watcher.py", "start_line": 137, "end_line": 216, "content": "            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados", "mtime": 1755697163.1334832, "terms": ["self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados"]}
{"chunk_id": "eb54ae8f8bc786fe26e0fb66", "file_path": "utils/file_watcher.py", "start_line": 145, "end_line": 224, "content": "    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)", "mtime": 1755697163.1334832, "terms": ["def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr"]}
{"chunk_id": "ad33f6150f3a41fcab30624c", "file_path": "utils/file_watcher.py", "start_line": 173, "end_line": 252, "content": "class WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:", "mtime": 1755697163.1334832, "terms": ["class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as"]}
{"chunk_id": "fc625ba8485aafd283f1b5d0", "file_path": "utils/file_watcher.py", "start_line": 176, "end_line": 255, "content": "    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict"]}
{"chunk_id": "2a69640d63e13e95c8af1b11", "file_path": "utils/file_watcher.py", "start_line": 180, "end_line": 259, "content": "    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,", "mtime": 1755697163.1334832, "terms": ["def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running"]}
{"chunk_id": "15f7d36faaaa26ac890ac2e9", "file_path": "utils/file_watcher.py", "start_line": 184, "end_line": 263, "content": "    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats", "mtime": 1755697163.1334832, "terms": ["def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats"]}
{"chunk_id": "62099b9d42213e18d8e32c15", "file_path": "utils/file_watcher.py", "start_line": 188, "end_line": 267, "content": "    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"", "mtime": 1755697163.1334832, "terms": ["def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher"]}
{"chunk_id": "153f060e00ab6a57c3bb7d51", "file_path": "utils/file_watcher.py", "start_line": 192, "end_line": 271, "content": "class FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    ", "mtime": 1755697163.1334832, "terms": ["class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as"]}
{"chunk_id": "098715c8cfe1d01f4af1d259", "file_path": "utils/file_watcher.py", "start_line": 195, "end_line": 274, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,", "mtime": 1755697163.1334832, "terms": ["def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none"]}
{"chunk_id": "713452a4d6b8feedfe7e83ee", "file_path": "utils/file_watcher.py", "start_line": 205, "end_line": 284, "content": "        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        ", "mtime": 1755697163.1334832, "terms": ["try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt"]}
{"chunk_id": "db4045659718608d0e04bfe4", "file_path": "utils/file_watcher.py", "start_line": 227, "end_line": 306, "content": "    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []", "mtime": 1755697163.1334832, "terms": ["def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files"]}
{"chunk_id": "11d8eb8c257da37186cd9428", "file_path": "utils/file_watcher.py", "start_line": 244, "end_line": 323, "content": "    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou", "mtime": 1755697163.1334832, "terms": ["def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou"]}
{"chunk_id": "779e690520b4aa3b67612843", "file_path": "utils/file_watcher.py", "start_line": 255, "end_line": 334, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        ", "mtime": 1755697163.1334832, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files"]}
{"chunk_id": "76c28e45e37dab615efe4502", "file_path": "utils/file_watcher.py", "start_line": 266, "end_line": 345, "content": "class SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:", "mtime": 1755697163.1334832, "terms": ["class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files"]}
{"chunk_id": "977993463a7f3436de756107", "file_path": "utils/file_watcher.py", "start_line": 272, "end_line": 351, "content": "    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files"]}
{"chunk_id": "1f4950c8c4d6e34c6cc4e3ee", "file_path": "utils/file_watcher.py", "start_line": 273, "end_line": 352, "content": "                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)", "mtime": 1755697163.1334832, "terms": ["watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files"]}
{"chunk_id": "2004bd7223f24cd8ee14745b", "file_path": "utils/file_watcher.py", "start_line": 296, "end_line": 375, "content": "    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        ", "mtime": 1755697163.1334832, "terms": ["def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start"]}
{"chunk_id": "9dea304d2a2f7b62b299c9bf", "file_path": "utils/file_watcher.py", "start_line": 304, "end_line": 383, "content": "    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():", "mtime": 1755697163.1334832, "terms": ["def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive"]}
{"chunk_id": "e13adf0ce8ad0d3942f67b1d", "file_path": "utils/file_watcher.py", "start_line": 337, "end_line": 406, "content": "    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "f9cf4c241ad14d69c9e4093a", "file_path": "utils/file_watcher.py", "start_line": 341, "end_line": 406, "content": "                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "2ceb50c653dcead02e012138", "file_path": "utils/file_watcher.py", "start_line": 363, "end_line": 406, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "7c997427a8ce953e10f449dd", "file_path": "utils/file_watcher.py", "start_line": 380, "end_line": 406, "content": "    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "4d811d48fc15a3e6884a9518", "file_path": "utils/file_watcher.py", "start_line": 387, "end_line": 406, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "100c5c904cdb420609c52c91", "file_path": "utils/file_watcher.py", "start_line": 398, "end_line": 406, "content": "def create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "2627c0f97de40a354e7b7a7a", "file_path": "scripts/summarize_metrics.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nResumo de mÃ©tricas do MCP (context_pack) a partir de .mcp_index/metrics.csv\n\nCampos esperados no CSV:\n  ts, query, chunk_count, total_tokens, budget_tokens, budget_utilization, latency_ms\n\nUso bÃ¡sico:\n  python summarize_metrics.py\n  python summarize_metrics.py --file .mcp_index/metrics.csv\n  python summarize_metrics.py --since 7\n  python summarize_metrics.py --filter \"minha funcao\"\n  python summarize_metrics.py --json\n\nSaÃ­da:\n- Resumo geral (perÃ­odo, N linhas, tokens mÃ©dios, p95, etc.)\n- Tabela diÃ¡ria com avg/median/p95 de tokens e latÃªncia\n\"\"\"\n\nimport os, sys, csv, argparse, datetime as dt, statistics as st, json\nfrom math import floor\nfrom typing import List, Dict, Any\n\ndef p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente Ãºltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' contÃ©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao invÃ©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV estÃ¡ vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n", "mtime": 1755730890.8006165, "terms": ["usr", "bin", "env", "python3", "resumo", "de", "tricas", "do", "mcp", "context_pack", "partir", "de", "mcp_index", "metrics", "csv", "campos", "esperados", "no", "csv", "ts", "query", "chunk_count", "total_tokens", "budget_tokens", "budget_utilization", "latency_ms", "uso", "sico", "python", "summarize_metrics", "py", "python", "summarize_metrics", "py", "file", "mcp_index", "metrics", "csv", "python", "summarize_metrics", "py", "since", "python", "summarize_metrics", "py", "filter", "minha", "funcao", "python", "summarize_metrics", "py", "json", "sa", "da", "resumo", "geral", "per", "odo", "linhas", "tokens", "dios", "p95", "etc", "tabela", "di", "ria", "com", "avg", "median", "p95", "de", "tokens", "lat", "ncia", "import", "os", "sys", "csv", "argparse", "datetime", "as", "dt", "statistics", "as", "st", "json", "from", "math", "import", "floor", "from", "typing", "import", "list", "dict", "any", "def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily"]}
{"chunk_id": "ab112a745e12dab0b395b30b", "file_path": "scripts/summarize_metrics.py", "start_line": 24, "end_line": 103, "content": "def p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente Ãºltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' contÃ©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao invÃ©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV estÃ¡ vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []", "mtime": 1755730890.8006165, "terms": ["def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat"]}
{"chunk_id": "f4bb10a1f776d0e67f78ae28", "file_path": "scripts/summarize_metrics.py", "start_line": 30, "end_line": 109, "content": "def coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente Ãºltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' contÃ©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao invÃ©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV estÃ¡ vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]", "mtime": 1755730890.8006165, "terms": ["def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day"]}
{"chunk_id": "df6df2fbf2ff7d6ad7fc0099", "file_path": "scripts/summarize_metrics.py", "start_line": 39, "end_line": 118, "content": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente Ãºltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' contÃ©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao invÃ©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV estÃ¡ vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass", "mtime": 1755730890.8006165, "terms": ["def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass"]}
{"chunk_id": "6d8efb53500fbf78a214911e", "file_path": "scripts/summarize_metrics.py", "start_line": 51, "end_line": 130, "content": "def load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV estÃ¡ vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n", "mtime": 1755730890.8006165, "terms": ["def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day"]}
{"chunk_id": "c164296d635332b1b2d5639b", "file_path": "scripts/summarize_metrics.py", "start_line": 62, "end_line": 141, "content": "def within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se nÃ£o der pra parsear, nÃ£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},", "mtime": 1755730890.8006165, "terms": ["def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end"]}
{"chunk_id": "3d007456816f195c051dc8e1", "file_path": "scripts/summarize_metrics.py", "start_line": 69, "end_line": 148, "content": "\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,", "mtime": 1755730890.8006165, "terms": ["def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else"]}
{"chunk_id": "e57ac7c53869f5b22c401d80", "file_path": "scripts/summarize_metrics.py", "start_line": 70, "end_line": 149, "content": "def summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # funÃ§Ãµes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,", "mtime": 1755730890.8006165, "terms": ["def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else"]}
{"chunk_id": "df4bc647c8cace9a5517460e", "file_path": "scripts/summarize_metrics.py", "start_line": 89, "end_line": 168, "content": "    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # diÃ¡rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,\n        },\n        \"latency_ms\": {\n            \"avg\": round(sum(all_lat)/len(all_lat), 2) if all_lat else 0.0,\n            \"med\": round(st.median(all_lat), 2) if all_lat else 0.0,\n            \"p95\": round(p95(all_lat), 2) if all_lat else 0.0,\n            \"min\": min(all_lat) if all_lat else 0,\n            \"max\": max(all_lat) if all_lat else 0,\n        },\n        \"chunk_count_avg\": round(sum(all_chunks)/len(all_chunks), 2) if all_chunks else 0.0,\n    }\n\n    return {\"overall\": overall, \"daily\": daily}\n\ndef print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:", "mtime": 1755730890.8006165, "terms": ["def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else", "latency_ms", "avg", "round", "sum", "all_lat", "len", "all_lat", "if", "all_lat", "else", "med", "round", "st", "median", "all_lat", "if", "all_lat", "else", "p95", "round", "p95", "all_lat", "if", "all_lat", "else", "min", "min", "all_lat", "if", "all_lat", "else", "max", "max", "all_lat", "if", "all_lat", "else", "chunk_count_avg", "round", "sum", "all_chunks", "len", "all_chunks", "if", "all_chunks", "else", "return", "overall", "overall", "daily", "daily", "def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall"]}
{"chunk_id": "af0db25f4897325b04aed5c8", "file_path": "scripts/summarize_metrics.py", "start_line": 137, "end_line": 196, "content": "            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,\n        },\n        \"latency_ms\": {\n            \"avg\": round(sum(all_lat)/len(all_lat), 2) if all_lat else 0.0,\n            \"med\": round(st.median(all_lat), 2) if all_lat else 0.0,\n            \"p95\": round(p95(all_lat), 2) if all_lat else 0.0,\n            \"min\": min(all_lat) if all_lat else 0,\n            \"max\": max(all_lat) if all_lat else 0,\n        },\n        \"chunk_count_avg\": round(sum(all_chunks)/len(all_chunks), 2) if all_chunks else 0.0,\n    }\n\n    return {\"overall\": overall, \"daily\": daily}\n\ndef print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:\n        print(\"sem dados apÃ³s filtros.\")\n        return\n\n    per = overall[\"period\"]\n    print(f\"PerÃ­odo: {per.get('start','?')} â†’ {per.get('end','?')}\")\n    print(f\"N linhas: {overall['n']}\")\n    print(f\"Tokens  | avg: {overall['tokens']['avg']}, med: {overall['tokens']['med']}, p95: {overall['tokens']['p95']}, sum: {overall['tokens']['sum']}\")\n    print(f\"Lat(ms) | avg: {overall['latency_ms']['avg']}, med: {overall['latency_ms']['med']}, p95: {overall['latency_ms']['p95']}\")\n    print(f\"Chunk count mÃ©dio: {overall['chunk_count_avg']}\")\n    print(\"\\n=== DAILY ===\")\n    print(f\"{'day':<12} {'n':>4}  {'tok_avg':>8} {'tok_med':>8} {'tok_p95':>8}   {'lat_avg':>8} {'lat_p95':>8}   {'chunks_avg':>10} {'budget_u_avg':>12}\")\n    for d in daily:\n        print(f\"{d['day']:<12} {d['n']:>4}  \"\n              f\"{d['tokens']['avg']:>8} {d['tokens']['med']:>8} {d['tokens']['p95']:>8}   \"\n              f\"{d['latency_ms']['avg']:>8} {d['latency_ms']['p95']:>8}   \"\n              f\"{d['chunk_count_avg']:>10} {d['budget_util_avg']:>12}\")\n\ndef main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else", "latency_ms", "avg", "round", "sum", "all_lat", "len", "all_lat", "if", "all_lat", "else", "med", "round", "st", "median", "all_lat", "if", "all_lat", "else", "p95", "round", "p95", "all_lat", "if", "all_lat", "else", "min", "min", "all_lat", "if", "all_lat", "else", "max", "max", "all_lat", "if", "all_lat", "else", "chunk_count_avg", "round", "sum", "all_chunks", "len", "all_chunks", "if", "all_chunks", "else", "return", "overall", "overall", "daily", "daily", "def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall", "print", "sem", "dados", "ap", "filtros", "return", "per", "overall", "period", "print", "per", "odo", "per", "get", "start", "per", "get", "end", "print", "linhas", "overall", "print", "tokens", "avg", "overall", "tokens", "avg", "med", "overall", "tokens", "med", "p95", "overall", "tokens", "p95", "sum", "overall", "tokens", "sum", "print", "lat", "ms", "avg", "overall", "latency_ms", "avg", "med", "overall", "latency_ms", "med", "p95", "overall", "latency_ms", "p95", "print", "chunk", "count", "dio", "overall", "chunk_count_avg", "print", "daily", "print", "day", "tok_avg", "tok_med", "tok_p95", "lat_avg", "lat_p95", "chunks_avg", "budget_u_avg", "for", "in", "daily", "print", "day", "tokens", "avg", "tokens", "med", "tokens", "p95", "latency_ms", "avg", "latency_ms", "p95", "chunk_count_avg", "budget_util_avg", "def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "ce1b8335ec9e5c9ab0fc23e1", "file_path": "scripts/summarize_metrics.py", "start_line": 163, "end_line": 196, "content": "def print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:\n        print(\"sem dados apÃ³s filtros.\")\n        return\n\n    per = overall[\"period\"]\n    print(f\"PerÃ­odo: {per.get('start','?')} â†’ {per.get('end','?')}\")\n    print(f\"N linhas: {overall['n']}\")\n    print(f\"Tokens  | avg: {overall['tokens']['avg']}, med: {overall['tokens']['med']}, p95: {overall['tokens']['p95']}, sum: {overall['tokens']['sum']}\")\n    print(f\"Lat(ms) | avg: {overall['latency_ms']['avg']}, med: {overall['latency_ms']['med']}, p95: {overall['latency_ms']['p95']}\")\n    print(f\"Chunk count mÃ©dio: {overall['chunk_count_avg']}\")\n    print(\"\\n=== DAILY ===\")\n    print(f\"{'day':<12} {'n':>4}  {'tok_avg':>8} {'tok_med':>8} {'tok_p95':>8}   {'lat_avg':>8} {'lat_p95':>8}   {'chunks_avg':>10} {'budget_u_avg':>12}\")\n    for d in daily:\n        print(f\"{d['day']:<12} {d['n']:>4}  \"\n              f\"{d['tokens']['avg']:>8} {d['tokens']['med']:>8} {d['tokens']['p95']:>8}   \"\n              f\"{d['latency_ms']['avg']:>8} {d['latency_ms']['p95']:>8}   \"\n              f\"{d['chunk_count_avg']:>10} {d['budget_util_avg']:>12}\")\n\ndef main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall", "print", "sem", "dados", "ap", "filtros", "return", "per", "overall", "period", "print", "per", "odo", "per", "get", "start", "per", "get", "end", "print", "linhas", "overall", "print", "tokens", "avg", "overall", "tokens", "avg", "med", "overall", "tokens", "med", "p95", "overall", "tokens", "p95", "sum", "overall", "tokens", "sum", "print", "lat", "ms", "avg", "overall", "latency_ms", "avg", "med", "overall", "latency_ms", "med", "p95", "overall", "latency_ms", "p95", "print", "chunk", "count", "dio", "overall", "chunk_count_avg", "print", "daily", "print", "day", "tok_avg", "tok_med", "tok_p95", "lat_avg", "lat_p95", "chunks_avg", "budget_u_avg", "for", "in", "daily", "print", "day", "tokens", "avg", "tokens", "med", "tokens", "p95", "latency_ms", "avg", "latency_ms", "p95", "chunk_count_avg", "budget_util_avg", "def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "5c25648a28b0bafcbc0cc442", "file_path": "scripts/summarize_metrics.py", "start_line": 186, "end_line": 196, "content": "def main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "edcf9674cd750796bdb752c5", "file_path": "scripts/mcp_client_stats.py", "start_line": 1, "end_line": 49, "content": "#!/usr/bin/env python3\n\"\"\"\nCliente MCP para obter estatÃ­sticas do sistema\n\"\"\"\n\nimport json\nimport asyncio\nimport os\nimport time\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def get_stats():\n    \"\"\"ObtÃ©m estatÃ­sticas do servidor MCP\"\"\"\n    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n    server_path = os.path.join(base_dir, 'mcp_server_enhanced.py')\n\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[\"-u\", server_path],\n        env={\"INDEX_DIR\": \".mcp_index\", \"INDEX_ROOT\": base_dir}\n    )\n\n    max_retries = 10\n    retry_delay = 1  # segundos\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            for attempt in range(max_retries):\n                try:\n                    tools = await session.list_tools()\n                    print(\"Ferramentas disponÃ­veis:\")\n                    for tool in tools:\n                        print(f\"  - {tool.name}: {tool.description}\")\n\n                    print(\"\\nExecutando comando get_stats...\")\n                    result = await session.call_tool(\"get_stats\", {})\n                    print(\"Resultado:\")\n                    print(json.dumps(result, indent=2, ensure_ascii=False))\n                    break\n                except Exception as e:\n                    print(f\"Tentativa {attempt+1} falhou: {e}\")\n                    if attempt == max_retries - 1:\n                        print(\"Falha ao executar comando apÃ³s vÃ¡rias tentativas.\")\n                        break\n                    await asyncio.sleep(retry_delay)\n\nif __name__ == \"__main__\":\n    asyncio.run(get_stats())", "mtime": 1755727193.3613484, "terms": ["usr", "bin", "env", "python3", "cliente", "mcp", "para", "obter", "estat", "sticas", "do", "sistema", "import", "json", "import", "asyncio", "import", "os", "import", "time", "from", "mcp", "import", "clientsession", "stdioserverparameters", "from", "mcp", "client", "stdio", "import", "stdio_client", "async", "def", "get_stats", "obt", "estat", "sticas", "do", "servidor", "mcp", "base_dir", "os", "path", "abspath", "os", "path", "join", "os", "path", "dirname", "__file__", "server_path", "os", "path", "join", "base_dir", "mcp_server_enhanced", "py", "server_params", "stdioserverparameters", "command", "python", "args", "server_path", "env", "index_dir", "mcp_index", "index_root", "base_dir", "max_retries", "retry_delay", "segundos", "async", "with", "stdio_client", "server_params", "as", "read", "write", "async", "with", "clientsession", "read", "write", "as", "session", "for", "attempt", "in", "range", "max_retries", "try", "tools", "await", "session", "list_tools", "print", "ferramentas", "dispon", "veis", "for", "tool", "in", "tools", "print", "tool", "name", "tool", "description", "print", "nexecutando", "comando", "get_stats", "result", "await", "session", "call_tool", "get_stats", "print", "resultado", "print", "json", "dumps", "result", "indent", "ensure_ascii", "false", "break", "except", "exception", "as", "print", "tentativa", "attempt", "falhou", "if", "attempt", "max_retries", "print", "falha", "ao", "executar", "comando", "ap", "rias", "tentativas", "break", "await", "asyncio", "sleep", "retry_delay", "if", "__name__", "__main__", "asyncio", "run", "get_stats"]}
{"chunk_id": "0470f4a230ae57ec18418e49", "file_path": "scripts/get_stats.py", "start_line": 1, "end_line": 34, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para obter estatÃ­sticas do sistema MCP\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\n\ndef get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estatÃ­sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisiÃ§Ã£o para o servidor MCP...\")\n    print(f\"RequisiÃ§Ã£o: {request_str}\")\n    \n    # Aqui vocÃª precisaria se conectar ao servidor MCP\n    # Esta Ã© uma implementaÃ§Ã£o simplificada\n    print(\"\\nPara executar este comando, vocÃª pode:\")\n    print(\"1. Usar a interface do VS Code com a extensÃ£o MCP\")\n    print(\"2. Enviar a requisiÃ§Ã£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["usr", "bin", "env", "python3", "script", "para", "obter", "estat", "sticas", "do", "sistema", "mcp", "import", "json", "import", "subprocess", "import", "sys", "def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "2488ba58a4cb868d786ef089", "file_path": "scripts/get_stats.py", "start_line": 10, "end_line": 34, "content": "def get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estatÃ­sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisiÃ§Ã£o para o servidor MCP...\")\n    print(f\"RequisiÃ§Ã£o: {request_str}\")\n    \n    # Aqui vocÃª precisaria se conectar ao servidor MCP\n    # Esta Ã© uma implementaÃ§Ã£o simplificada\n    print(\"\\nPara executar este comando, vocÃª pode:\")\n    print(\"1. Usar a interface do VS Code com a extensÃ£o MCP\")\n    print(\"2. Enviar a requisiÃ§Ã£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "27744efa5de470a8f5dfd7c0", "file_path": "reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diretÃ³rio de Ã­ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de mÃ©tricas (MCP_METRICS_FILE ou .mcp_index/metrics.csv).\n- (Opcional) Calcula baseline aproximada de tokens do repositÃ³rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# ---- Config mÃ©tricas (mesmo padrÃ£o do context_pack)\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)", "mtime": 1755731436.0803573, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "config", "tricas", "mesmo", "padr", "do", "context_pack", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir"]}
{"chunk_id": "e7c774b75b4e9f5911cce40a", "file_path": "reindex.py", "start_line": 34, "end_line": 113, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1755731436.0803573, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "9ac068e6f70e05580089c4e7", "file_path": "reindex.py", "start_line": 43, "end_line": 122, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1755731436.0803573, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "39f9fe55db0355f67dfe83d9", "file_path": "reindex.py", "start_line": 47, "end_line": 126, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1755731436.0803573, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "b3aece1ad5078bf1465c4c3e", "file_path": "reindex.py", "start_line": 65, "end_line": 144, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"ðŸ“Š Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1755731436.0803573, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "38ca7d55ab9ac0ff23c6946b", "file_path": "reindex.py", "start_line": 69, "end_line": 148, "content": "\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"ðŸ“Š Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,", "mtime": 1755731436.0803573, "terms": ["importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed"]}
{"chunk_id": "aad8e65810028ee77501b919", "file_path": "reindex.py", "start_line": 137, "end_line": 163, "content": "    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # nÃ£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1755731436.0803573, "terms": ["loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "53867eaffed5fefed0517c2e", "file_path": "main.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nCLI principal para anÃ¡lise de extratos bancÃ¡rios.\n\"\"\"\nimport sys\nfrom pathlib import Path\nimport click\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\n\n# Adiciona o diretÃ³rio raiz ao path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom src.application.use_cases import ExtractAnalyzer\nfrom src.domain.exceptions import DomainException\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nconsole = Console()\n\n\n@click.group()\ndef cli():\n    \"\"\"Sistema de AnÃ¡lise de Extratos BancÃ¡rios\"\"\"\n    pass\n\n\n@cli.command()\n@click.argument('file_path', type=click.Path(exists=True))\n@click.option('--output', '-o', help='Caminho para salvar o relatÃ³rio')\n@click.option('--format', '-f', type=click.Choice(['text', 'markdown']), default='text', help='Formato do relatÃ³rio')\ndef analyze(file_path, output, format):\n    \"\"\"Analisa um extrato bancÃ¡rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]âœ“ AnÃ¡lise concluÃ­da![/bold green]\\n\\n\"\n            f\"ðŸ“Š Total de transaÃ§Ãµes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"ðŸ’° Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"ðŸ’¸ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"ðŸ“ˆ Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"âš ï¸  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"ðŸ’¡ Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relatÃ³rio completo\n        if output:\n            console.print(f\"[green]RelatÃ³rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]RelatÃ³rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:", "mtime": 1756147266.072644, "terms": ["usr", "bin", "env", "python3", "cli", "principal", "para", "an", "lise", "de", "extratos", "banc", "rios", "import", "sys", "from", "pathlib", "import", "path", "import", "click", "from", "rich", "console", "import", "console", "from", "rich", "table", "import", "table", "from", "rich", "panel", "import", "panel", "adiciona", "diret", "rio", "raiz", "ao", "path", "sys", "path", "insert", "str", "path", "__file__", "parent", "parent", "from", "src", "application", "use_cases", "import", "extractanalyzer", "from", "src", "domain", "exceptions", "import", "domainexception", "from", "src", "utils", "currency_utils", "import", "currencyutils", "console", "console", "click", "group", "def", "cli", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "pass", "cli", "command", "click", "argument", "file_path", "type", "click", "path", "exists", "true", "click", "option", "output", "help", "caminho", "para", "salvar", "relat", "rio", "click", "option", "format", "type", "click", "choice", "text", "markdown", "default", "text", "help", "formato", "do", "relat", "rio", "def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as"]}
{"chunk_id": "0bad8fef21a512c3f8890f26", "file_path": "main.py", "start_line": 24, "end_line": 103, "content": "def cli():\n    \"\"\"Sistema de AnÃ¡lise de Extratos BancÃ¡rios\"\"\"\n    pass\n\n\n@cli.command()\n@click.argument('file_path', type=click.Path(exists=True))\n@click.option('--output', '-o', help='Caminho para salvar o relatÃ³rio')\n@click.option('--format', '-f', type=click.Choice(['text', 'markdown']), default='text', help='Formato do relatÃ³rio')\ndef analyze(file_path, output, format):\n    \"\"\"Analisa um extrato bancÃ¡rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]âœ“ AnÃ¡lise concluÃ­da![/bold green]\\n\\n\"\n            f\"ðŸ“Š Total de transaÃ§Ãµes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"ðŸ’° Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"ðŸ’¸ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"ðŸ“ˆ Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"âš ï¸  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"ðŸ’¡ Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relatÃ³rio completo\n        if output:\n            console.print(f\"[green]RelatÃ³rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]RelatÃ³rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de domÃ­nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instruÃ§Ãµes de exemplo.\"\"\"\n    instructions = \"\"\"# InstruÃ§Ãµes para uso do Sistema de AnÃ¡lise de Extratos BancÃ¡rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:", "mtime": 1756147266.072644, "terms": ["def", "cli", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "pass", "cli", "command", "click", "argument", "file_path", "type", "click", "path", "exists", "true", "click", "option", "output", "help", "caminho", "para", "salvar", "relat", "rio", "click", "option", "format", "type", "click", "choice", "text", "markdown", "default", "text", "help", "formato", "do", "relat", "rio", "def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas"]}
{"chunk_id": "8491bc1b2b7691aa3aa4e9b9", "file_path": "main.py", "start_line": 33, "end_line": 112, "content": "def analyze(file_path, output, format):\n    \"\"\"Analisa um extrato bancÃ¡rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]âœ“ AnÃ¡lise concluÃ­da![/bold green]\\n\\n\"\n            f\"ðŸ“Š Total de transaÃ§Ãµes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"ðŸ’° Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"ðŸ’¸ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"ðŸ“ˆ Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"âš ï¸  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"ðŸ’¡ Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relatÃ³rio completo\n        if output:\n            console.print(f\"[green]RelatÃ³rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]RelatÃ³rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de domÃ­nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instruÃ§Ãµes de exemplo.\"\"\"\n    instructions = \"\"\"# InstruÃ§Ãµes para uso do Sistema de AnÃ¡lise de Extratos BancÃ¡rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas ObrigatÃ³rias:\n- Data da transaÃ§Ã£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- DescriÃ§Ã£o da transaÃ§Ã£o\n- Valor da transaÃ§Ã£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo apÃ³s a transaÃ§Ã£o\n- NÃºmero da conta", "mtime": 1756147266.072644, "terms": ["def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta"]}
{"chunk_id": "971ab0d6d8d0380179867bec", "file_path": "main.py", "start_line": 69, "end_line": 148, "content": "            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relatÃ³rio completo\n        if output:\n            console.print(f\"[green]RelatÃ³rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]RelatÃ³rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de domÃ­nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instruÃ§Ãµes de exemplo.\"\"\"\n    instructions = \"\"\"# InstruÃ§Ãµes para uso do Sistema de AnÃ¡lise de Extratos BancÃ¡rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas ObrigatÃ³rias:\n- Data da transaÃ§Ã£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- DescriÃ§Ã£o da transaÃ§Ã£o\n- Valor da transaÃ§Ã£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo apÃ³s a transaÃ§Ã£o\n- NÃºmero da conta\n- Saldo inicial/final\n\n### Nomes de Colunas Aceitos:\n\n#### Para Data:\n- data\n- date\n- data transacao\n- transaction date\n\n#### Para DescriÃ§Ã£o:\n- descricao\n- description\n- descriÃ§Ã£o\n\n#### Para Valor:\n- valor\n- amount\n- value\n- montante\n\n#### Para Saldo:\n- saldo\n- balance\n- saldo apÃ³s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,SalÃ¡rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:", "mtime": 1756147266.072644, "terms": ["for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta", "saldo", "inicial", "final", "nomes", "de", "colunas", "aceitos", "para", "data", "data", "date", "data", "transacao", "transaction", "date", "para", "descri", "descricao", "description", "descri", "para", "valor", "valor", "amount", "value", "montante", "para", "saldo", "saldo", "balance", "saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas"]}
{"chunk_id": "439c634265dc1b6aff52cfd2", "file_path": "main.py", "start_line": 90, "end_line": 169, "content": "def sample(output_path):\n    \"\"\"Cria um arquivo de instruÃ§Ãµes de exemplo.\"\"\"\n    instructions = \"\"\"# InstruÃ§Ãµes para uso do Sistema de AnÃ¡lise de Extratos BancÃ¡rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas ObrigatÃ³rias:\n- Data da transaÃ§Ã£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- DescriÃ§Ã£o da transaÃ§Ã£o\n- Valor da transaÃ§Ã£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo apÃ³s a transaÃ§Ã£o\n- NÃºmero da conta\n- Saldo inicial/final\n\n### Nomes de Colunas Aceitos:\n\n#### Para Data:\n- data\n- date\n- data transacao\n- transaction date\n\n#### Para DescriÃ§Ã£o:\n- descricao\n- description\n- descriÃ§Ã£o\n\n#### Para Valor:\n- valor\n- amount\n- value\n- montante\n\n#### Para Saldo:\n- saldo\n- balance\n- saldo apÃ³s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,SalÃ¡rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:\n\nO sistema detecta automaticamente a moeda do extrato:\n- EUR (Euro) - PadrÃ£o\n- USD (DÃ³lar Americano)\n- BRL (Real Brasileiro)\n- GBP (Libra Esterlina)\n- JPY (Iene JaponÃªs)\n- CHF (Franco SuÃ­Ã§o)\n- CAD (DÃ³lar Canadense)\n- AUD (DÃ³lar Australiano)\n\n## Executando a AnÃ¡lise:\n\nPara analisar um extrato, use o comando:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.pdf\npython main.py analyze caminho/para/seu/extrato.xlsx\npython main.py analyze caminho/para/seu/extrato.csv\n```\n", "mtime": 1756147266.072644, "terms": ["def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta", "saldo", "inicial", "final", "nomes", "de", "colunas", "aceitos", "para", "data", "data", "date", "data", "transacao", "transaction", "date", "para", "descri", "descricao", "description", "descri", "para", "valor", "valor", "amount", "value", "montante", "para", "saldo", "saldo", "balance", "saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas", "sistema", "detecta", "automaticamente", "moeda", "do", "extrato", "eur", "euro", "padr", "usd", "lar", "americano", "brl", "real", "brasileiro", "gbp", "libra", "esterlina", "jpy", "iene", "japon", "chf", "franco", "su", "cad", "lar", "canadense", "aud", "lar", "australiano", "executando", "an", "lise", "para", "analisar", "um", "extrato", "use", "comando", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "pdf", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "xlsx", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv"]}
{"chunk_id": "b4d598870f59f1608de3732a", "file_path": "main.py", "start_line": 137, "end_line": 201, "content": "- saldo apÃ³s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,SalÃ¡rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:\n\nO sistema detecta automaticamente a moeda do extrato:\n- EUR (Euro) - PadrÃ£o\n- USD (DÃ³lar Americano)\n- BRL (Real Brasileiro)\n- GBP (Libra Esterlina)\n- JPY (Iene JaponÃªs)\n- CHF (Franco SuÃ­Ã§o)\n- CAD (DÃ³lar Canadense)\n- AUD (DÃ³lar Australiano)\n\n## Executando a AnÃ¡lise:\n\nPara analisar um extrato, use o comando:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.pdf\npython main.py analyze caminho/para/seu/extrato.xlsx\npython main.py analyze caminho/para/seu/extrato.csv\n```\n\nPara salvar o relatÃ³rio em um arquivo:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.csv --output relatorio.txt\n```\n\nPara gerar um relatÃ³rio em formato Markdown:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.csv --format markdown --output relatorio.md\n```\n\"\"\"\n    \n    try:\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(instructions)\n        console.print(f\"[green]âœ“[/green] Arquivo de instruÃ§Ãµes criado em: {output_path}\")\n    except Exception as e:\n        console.print(f\"[red]âœ—[/red] Erro ao criar arquivo de instruÃ§Ãµes: {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\ndef version():\n    \"\"\"Mostra a versÃ£o do sistema.\"\"\"\n    console.print(\"[bold blue]Sistema de AnÃ¡lise de Extratos BancÃ¡rios[/bold blue]\")\n    console.print(\"VersÃ£o: 1.0.0\")\n    console.print(\"Formatos suportados: PDF, Excel, CSV\")\n\n\nif __name__ == '__main__':\n    cli()", "mtime": 1756147266.072644, "terms": ["saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas", "sistema", "detecta", "automaticamente", "moeda", "do", "extrato", "eur", "euro", "padr", "usd", "lar", "americano", "brl", "real", "brasileiro", "gbp", "libra", "esterlina", "jpy", "iene", "japon", "chf", "franco", "su", "cad", "lar", "canadense", "aud", "lar", "australiano", "executando", "an", "lise", "para", "analisar", "um", "extrato", "use", "comando", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "pdf", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "xlsx", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "para", "salvar", "relat", "rio", "em", "um", "arquivo", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "output", "relatorio", "txt", "para", "gerar", "um", "relat", "rio", "em", "formato", "markdown", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "format", "markdown", "output", "relatorio", "md", "try", "with", "open", "output_path", "encoding", "utf", "as", "write", "instructions", "console", "print", "green", "green", "arquivo", "de", "instru", "es", "criado", "em", "output_path", "except", "exception", "as", "console", "print", "red", "red", "erro", "ao", "criar", "arquivo", "de", "instru", "es", "str", "sys", "exit", "cli", "command", "def", "version", "mostra", "vers", "do", "sistema", "console", "print", "bold", "blue", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "bold", "blue", "console", "print", "vers", "console", "print", "formatos", "suportados", "pdf", "excel", "csv", "if", "__name__", "__main__", "cli"]}
{"chunk_id": "66f3f21ef64861b50fccee07", "file_path": "main.py", "start_line": 193, "end_line": 201, "content": "def version():\n    \"\"\"Mostra a versÃ£o do sistema.\"\"\"\n    console.print(\"[bold blue]Sistema de AnÃ¡lise de Extratos BancÃ¡rios[/bold blue]\")\n    console.print(\"VersÃ£o: 1.0.0\")\n    console.print(\"Formatos suportados: PDF, Excel, CSV\")\n\n\nif __name__ == '__main__':\n    cli()", "mtime": 1756147266.072644, "terms": ["def", "version", "mostra", "vers", "do", "sistema", "console", "print", "bold", "blue", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "bold", "blue", "console", "print", "vers", "console", "print", "formatos", "suportados", "pdf", "excel", "csv", "if", "__name__", "__main__", "cli"]}
{"chunk_id": "cff1c15fae1dee325b517947", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 1, "end_line": 80, "content": "# mcp_server_enhanced.py\n\"\"\"\nServidor MCP melhorado com busca semÃ¢ntica e auto-indexaÃ§Ã£o\nUsando FastMCP para API simplificada com decorators\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, List\nimport pathlib\nimport threading\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from mcp.server.fastmcp import FastMCP\n    HAS_MCP = True\n    HAS_FASTMCP = True\nexcept ImportError:\n    try:\n        # Fallback para versÃµes mais antigas\n        from mcp.server import Server\n        from mcp import types\n        HAS_MCP = True\n        HAS_FASTMCP = False\n    except ImportError:\n        sys.stderr.write(\"[mcp_server_enhanced] ERROR: MCP SDK nÃ£o encontrado. Instale `mcp`.\\n\")\n        raise\n\n# Importa funcionalidades melhoradas\ntry:\n    from .code_indexer_enhanced import (\n        EnhancedCodeIndexer,\n        enhanced_search_code,\n        enhanced_build_context_pack,\n        enhanced_index_repo_paths,\n        BaseCodeIndexer,\n        search_code,\n        build_context_pack,\n        index_repo_paths\n    )\n    HAS_ENHANCED = True\n    sys.stderr.write(\"[mcp_server_enhanced] âœ… Funcionalidades melhoradas carregadas\\n\")\nexcept ImportError as e:\n    sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Funcionalidades melhoradas nÃ£o disponÃ­veis: {e}\\n\")\n    sys.stderr.write(\"[mcp_server_enhanced] ðŸ”„ Usando versÃ£o base integrada\\n\")\n\n    # Fallback para versÃ£o base integrada\n    try:\n        from .code_indexer_enhanced import (\n            BaseCodeIndexer,\n            search_code,\n            build_context_pack,\n            index_repo_paths\n        )\n        HAS_ENHANCED = False\n    except ImportError as e2:\n        sys.stderr.write(f\"[mcp_server_enhanced] âŒ Erro crÃ­tico: {e2}\\n\")\n        raise\n\nHAS_ENHANCED_FEATURES = HAS_ENHANCED\n\n# Config / instÃ¢ncias - agora usando caminhos relativos Ã  pasta mcp_system\nINDEX_DIR = os.environ.get(\"INDEX_DIR\", str(CURRENT_DIR / \".mcp_index\"))\nINDEX_ROOT = os.environ.get(\"INDEX_ROOT\", str(CURRENT_DIR.parent))\n\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXAÃ‡ÃƒO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default", "mtime": 1756159097.7969208, "terms": ["mcp_server_enhanced", "py", "servidor", "mcp", "melhorado", "com", "busca", "sem", "ntica", "auto", "indexa", "usando", "fastmcp", "para", "api", "simplificada", "com", "decorators", "import", "os", "import", "sys", "from", "typing", "import", "any", "dict", "list", "import", "pathlib", "import", "threading", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "mcp", "server", "fastmcp", "import", "fastmcp", "has_mcp", "true", "has_fastmcp", "true", "except", "importerror", "try", "fallback", "para", "vers", "es", "mais", "antigas", "from", "mcp", "server", "import", "server", "from", "mcp", "import", "types", "has_mcp", "true", "has_fastmcp", "false", "except", "importerror", "sys", "stderr", "write", "mcp_server_enhanced", "error", "mcp", "sdk", "encontrado", "instale", "mcp", "raise", "importa", "funcionalidades", "melhoradas", "try", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "true", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "carregadas", "except", "importerror", "as", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "dispon", "veis", "sys", "stderr", "write", "mcp_server_enhanced", "usando", "vers", "base", "integrada", "fallback", "para", "vers", "base", "integrada", "try", "from", "code_indexer_enhanced", "import", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "false", "except", "importerror", "as", "e2", "sys", "stderr", "write", "mcp_server_enhanced", "erro", "cr", "tico", "e2", "raise", "has_enhanced_features", "has_enhanced", "config", "inst", "ncias", "agora", "usando", "caminhos", "relativos", "pasta", "mcp_system", "index_dir", "os", "environ", "get", "index_dir", "str", "current_dir", "mcp_index", "index_root", "os", "environ", "get", "index_root", "str", "current_dir", "parent", "if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default"]}
{"chunk_id": "4f6180504fb9c2de1639d3b1", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 69, "end_line": 148, "content": "    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXAÃ‡ÃƒO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=abs_paths, recursive=AUTO_INDEX_RECURSIVE)\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n", "mtime": 1756159097.7969208, "terms": ["_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "abs_paths", "recursive", "auto_index_recursive", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive"]}
{"chunk_id": "643f4d42b21f77d3d5a2305d", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 78, "end_line": 157, "content": "def _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=abs_paths, recursive=AUTO_INDEX_RECURSIVE)\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")", "mtime": 1756159097.7969208, "terms": ["def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "abs_paths", "recursive", "auto_index_recursive", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr"]}
{"chunk_id": "6a2360a89ed8c8e538d4d897", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 90, "end_line": 169, "content": "def _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=abs_paths, recursive=AUTO_INDEX_RECURSIVE)\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)", "mtime": 1756159097.7969208, "terms": ["def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "abs_paths", "recursive", "auto_index_recursive", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit"]}
{"chunk_id": "f0a95e3b02d608a4c667bb6d", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 121, "end_line": 200, "content": "def _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        ", "mtime": 1756159097.7969208, "terms": ["def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget"]}
{"chunk_id": "07c83a43b0c094cb9a3a56c1", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 137, "end_line": 216, "content": "            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):", "mtime": 1756159097.7969208, "terms": ["if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing"]}
{"chunk_id": "f09ff0cfafcbedbec25257dc", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 155, "end_line": 234, "content": "def _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:", "mtime": 1756159097.7969208, "terms": ["def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as"]}
{"chunk_id": "466e4983d39cde517608f780", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 181, "end_line": 260, "content": "def _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"", "mtime": 1756159097.7969208, "terms": ["def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache"]}
{"chunk_id": "271f96ba5432a2c9dcf2fa4d", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 205, "end_line": 284, "content": "        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:", "mtime": 1756159097.7969208, "terms": ["except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp"]}
{"chunk_id": "04c33d4ef6e0435087daa313", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 211, "end_line": 290, "content": "def _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()", "mtime": 1756159097.7969208, "terms": ["def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start"]}
{"chunk_id": "c6fbeb8c6b5956c98e8ed21e", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 237, "end_line": 316, "content": "def _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,", "mtime": 1756159097.7969208, "terms": ["def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int"]}
{"chunk_id": "352f5e6e1e16494bdacafe73", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 259, "end_line": 338, "content": "def _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)", "mtime": 1756159097.7969208, "terms": ["def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true"]}
{"chunk_id": "ab8b72f45e6d52cc046aa2e8", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 273, "end_line": 352, "content": "        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:", "mtime": 1756159097.7969208, "terms": ["elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as"]}
{"chunk_id": "fbe586db94e05f582ea82d81", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 297, "end_line": 376, "content": "    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",", "mtime": 1756159097.7969208, "terms": ["def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object"]}
{"chunk_id": "5d4b65a78582a122f671c776", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 306, "end_line": 385, "content": "    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),", "mtime": 1756159097.7969208, "terms": ["def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query"]}
{"chunk_id": "ade5fa3b42a83a606f29662b", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 314, "end_line": 393, "content": "    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},", "mtime": 1756159097.7969208, "terms": ["def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default"]}
{"chunk_id": "316222be36f8fb210abe0fb9", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 322, "end_line": 401, "content": "    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",", "mtime": 1756159097.7969208, "terms": ["def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index"]}
{"chunk_id": "143802830b371315a6c08bd0", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 327, "end_line": 406, "content": "    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},", "mtime": 1756159097.7969208, "terms": ["def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status"]}
{"chunk_id": "e6f19187418d6213e54a6dcd", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 332, "end_line": 411, "content": "    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),", "mtime": 1756159097.7969208, "terms": ["def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true"]}
{"chunk_id": "136d1b06aeb152f5a7d239f7", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 341, "end_line": 420, "content": "\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                }\n            ),\n            Tool(", "mtime": 1756159097.7969208, "terms": ["else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool"]}
{"chunk_id": "89b06d4a124bc5a0cc2f84a0", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 409, "end_line": 488, "content": "                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                }\n            ),\n            Tool(\n                name=\"cache_management\",\n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"cache_type\": {\"type\": \"string\", \"default\": \"all\"}\n                    }\n                }\n            )\n        ]\n\n    @server.call_tool()\n    async def call_tool(name: str, arguments: Dict[str, Any] = None):\n        arguments = arguments or {}\n        try:\n            if name == \"index_path\":\n                return _handle_index_path(\n                    arguments.get(\"path\", \".\"),\n                    arguments.get(\"recursive\", True),\n                    arguments.get(\"enable_semantic\", True),\n                    arguments.get(\"auto_start_watcher\", False),\n                    arguments.get(\"exclude_globs\", None)\n                )\n            elif name == \"search_code\":\n                return _handle_search_code(\n                    arguments.get(\"query\"),\n                    arguments.get(\"limit\", 10),\n                    arguments.get(\"semantic_weight\", 0.3),\n                    arguments.get(\"use_mmr\", True)\n                )\n            elif name == \"context_pack\":\n                return _handle_context_pack(\n                    arguments.get(\"query\"),\n                    arguments.get(\"token_budget\", 8000),\n                    arguments.get(\"max_chunks\", 20),\n                    arguments.get(\"strategy\", \"mmr\")\n                )\n            elif name == \"auto_index\":\n                return _handle_auto_index(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"paths\", None),\n                    arguments.get(\"recursive\", True)\n                )\n            elif name == \"get_stats\":\n                return _handle_get_stats()\n            elif name == \"cache_management\":\n                return _handle_cache_management(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"cache_type\", \"all\")\n                )\n            else:\n                return {\"status\": \"error\", \"error\": f\"Ferramenta desconhecida: {name}\"}\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n\n    # Iniciar servidor\n    async def main():\n        # Removido: jÃ¡ iniciamos a thread antes\n        # threading.Thread(target=_initial_index, daemon=True).start()\n        async with stdio_server() as (read_stream, write_stream):\n            await server.run(read_stream, write_stream, server_name=\"code-indexer-enhanced\")\n\n    if __name__ == \"__main__\":\n        import asyncio\n        asyncio.run(main())\n\nif HAS_FASTMCP and __name__ == \"__main__\":", "mtime": 1756159097.7969208, "terms": ["tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "cache_type", "type", "string", "default", "all", "server", "call_tool", "async", "def", "call_tool", "name", "str", "arguments", "dict", "str", "any", "none", "arguments", "arguments", "or", "try", "if", "name", "index_path", "return", "_handle_index_path", "arguments", "get", "path", "arguments", "get", "recursive", "true", "arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "none", "elif", "name", "search_code", "return", "_handle_search_code", "arguments", "get", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack", "return", "_handle_context_pack", "arguments", "get", "query", "arguments", "get", "token_budget", "arguments", "get", "max_chunks", "arguments", "get", "strategy", "mmr", "elif", "name", "auto_index", "return", "_handle_auto_index", "arguments", "get", "action", "status", "arguments", "get", "paths", "none", "arguments", "get", "recursive", "true", "elif", "name", "get_stats", "return", "_handle_get_stats", "elif", "name", "cache_management", "return", "_handle_cache_management", "arguments", "get", "action", "status", "arguments", "get", "cache_type", "all", "else", "return", "status", "error", "error", "ferramenta", "desconhecida", "name", "except", "exception", "as", "return", "status", "error", "error", "str", "iniciar", "servidor", "async", "def", "main", "removido", "iniciamos", "thread", "antes", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "async", "with", "stdio_server", "as", "read_stream", "write_stream", "await", "server", "run", "read_stream", "write_stream", "server_name", "code", "indexer", "enhanced", "if", "__name__", "__main__", "import", "asyncio", "asyncio", "run", "main", "if", "has_fastmcp", "and", "__name__", "__main__"]}
{"chunk_id": "953d268b5e3b0d537d1a75fe", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 477, "end_line": 490, "content": "    # Iniciar servidor\n    async def main():\n        # Removido: jÃ¡ iniciamos a thread antes\n        # threading.Thread(target=_initial_index, daemon=True).start()\n        async with stdio_server() as (read_stream, write_stream):\n            await server.run(read_stream, write_stream, server_name=\"code-indexer-enhanced\")\n\n    if __name__ == \"__main__\":\n        import asyncio\n        asyncio.run(main())\n\nif HAS_FASTMCP and __name__ == \"__main__\":\n    # Executa o servidor FastMCP quando disponÃ­vel\n    mcp.run()", "mtime": 1756159097.7969208, "terms": ["iniciar", "servidor", "async", "def", "main", "removido", "iniciamos", "thread", "antes", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "async", "with", "stdio_server", "as", "read_stream", "write_stream", "await", "server", "run", "read_stream", "write_stream", "server_name", "code", "indexer", "enhanced", "if", "__name__", "__main__", "import", "asyncio", "asyncio", "run", "main", "if", "has_fastmcp", "and", "__name__", "__main__", "executa", "servidor", "fastmcp", "quando", "dispon", "vel", "mcp", "run"]}
{"chunk_id": "cf68b6e8100962416706bc9e", "file_path": "mcp_system/__init__.py", "start_line": 1, "end_line": 45, "content": "# __init__.py\n\"\"\"\nMCP System - Model Context Protocol\nSistema avanÃ§ado de indexaÃ§Ã£o e busca de cÃ³digo para desenvolvimento assistido por IA.\n\"\"\"\n\n# VersÃ£o do pacote\n__version__ = \"1.0.0\"\n\n# ImportaÃ§Ãµes principais para facilitar o uso do pacote\nfrom .code_indexer_enhanced import (\n    BaseCodeIndexer,\n    EnhancedCodeIndexer,\n    search_code,\n    build_context_pack,\n    index_repo_paths,\n    enhanced_search_code,\n    enhanced_build_context_pack,\n    enhanced_index_repo_paths\n)\n\n# Verificar se recursos avanÃ§ados estÃ£o disponÃ­veis\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\n    HAS_ENHANCED_FEATURES = True\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n\n# Expor funcionalidades principais\n__all__ = [\n    \"BaseCodeIndexer\",\n    \"EnhancedCodeIndexer\",\n    \"search_code\",\n    \"build_context_pack\",\n    \"index_repo_paths\",\n    \"enhanced_search_code\",\n    \"enhanced_build_context_pack\",\n    \"enhanced_index_repo_paths\",\n    \"HAS_ENHANCED_FEATURES\"\n]\n\n# Se recursos avanÃ§ados estiverem disponÃ­veis, exportÃ¡-los tambÃ©m\nif HAS_ENHANCED_FEATURES:\n    __all__.extend([\"SemanticSearchEngine\", \"create_file_watcher\"])", "mtime": 1756157484.5288744, "terms": ["__init__", "py", "mcp", "system", "model", "context", "protocol", "sistema", "avan", "ado", "de", "indexa", "busca", "de", "digo", "para", "desenvolvimento", "assistido", "por", "ia", "vers", "do", "pacote", "__version__", "importa", "es", "principais", "para", "facilitar", "uso", "do", "pacote", "from", "code_indexer_enhanced", "import", "basecodeindexer", "enhancedcodeindexer", "search_code", "build_context_pack", "index_repo_paths", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "verificar", "se", "recursos", "avan", "ados", "est", "dispon", "veis", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "has_enhanced_features", "true", "except", "importerror", "has_enhanced_features", "false", "expor", "funcionalidades", "principais", "__all__", "basecodeindexer", "enhancedcodeindexer", "search_code", "build_context_pack", "index_repo_paths", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "has_enhanced_features", "se", "recursos", "avan", "ados", "estiverem", "dispon", "veis", "export", "los", "tamb", "if", "has_enhanced_features", "__all__", "extend", "semanticsearchengine", "create_file_watcher"]}
{"chunk_id": "b710065ff2635afe529c4ddf", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 1, "end_line": 80, "content": "# code_indexer_enhanced.py\n# Sistema MCP melhorado com busca hÃ­brida, auto-indexaÃ§Ã£o e cache inteligente\nfrom __future__ import annotations\nimport os, re, json, math, time, hashlib, threading, csv, datetime as dt\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nimport pathlib\n\n#logs para mÃ©tricas\n\n# DiretÃ³rio atual deste mÃ³dulo para resoluÃ§Ãµes relativas ao pacote mcp_system\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", str(CURRENT_DIR / \".mcp_index/metrics.csv\"))\n\ndef _log_metrics(row: dict):\n    \"\"\"Append de uma linha de mÃ©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avanÃ§ados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILITÃRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count", "mtime": 1756157712.217539, "terms": ["code_indexer_enhanced", "py", "sistema", "mcp", "melhorado", "com", "busca", "brida", "auto", "indexa", "cache", "inteligente", "from", "__future__", "import", "annotations", "import", "os", "re", "json", "math", "time", "hashlib", "threading", "csv", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "import", "pathlib", "logs", "para", "tricas", "diret", "rio", "atual", "deste", "dulo", "para", "resolu", "es", "relativas", "ao", "pacote", "mcp_system", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "str", "current_dir", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count"]}
{"chunk_id": "2e6b68d5f24a88108cdf11ac", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 16, "end_line": 95, "content": "def _log_metrics(row: dict):\n    \"\"\"Append de uma linha de mÃ©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avanÃ§ados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILITÃRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():", "mtime": 1756157712.217539, "terms": ["def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists"]}
{"chunk_id": "f8eb81a34702c0d6bc739c04", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 58, "end_line": 137, "content": "def now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n", "mtime": 1756157712.217539, "terms": ["def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none"]}
{"chunk_id": "f0a074a04a5c40052d1070c3", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 61, "end_line": 140, "content": "def tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.", "mtime": 1756157712.217539, "terms": ["def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap"]}
{"chunk_id": "0f776e5293cf59789792f595", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 64, "end_line": 143, "content": "def est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()", "mtime": 1756157712.217539, "terms": ["def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines"]}
{"chunk_id": "326ae6069fe7ab52e66e94ed", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 68, "end_line": 147, "content": "def hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):", "mtime": 1756157712.217539, "terms": ["def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class"]}
{"chunk_id": "92edb46ae3b2a2ad0b8f7c16", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 69, "end_line": 148, "content": "    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)", "mtime": 1756157712.217539, "terms": ["return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add"]}
{"chunk_id": "e360fd95c4d1b98daf16d7b4", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 73, "end_line": 152, "content": "class BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)", "mtime": 1756157712.217539, "terms": ["class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add"]}
{"chunk_id": "86f66b6594c3b0af372c4a2a", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 74, "end_line": 153, "content": "    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap", "mtime": 1756157712.217539, "terms": ["def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap"]}
{"chunk_id": "2734be3f7d4d9242156607c1", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 85, "end_line": 164, "content": "    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:", "mtime": 1756157712.217539, "terms": ["def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int"]}
{"chunk_id": "847b2af87d8e4041fd63f44e", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 93, "end_line": 172, "content": "    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:", "mtime": 1756157712.217539, "terms": ["def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception"]}
{"chunk_id": "dcaf010f31cadfa64c7d64e4", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 107, "end_line": 186, "content": "    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- mÃ©tricas e estatÃ­sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,", "mtime": 1756157712.217539, "terms": ["def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid"]}
{"chunk_id": "c89911a0ed15c3748edbf184", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 117, "end_line": 196, "content": "    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)", "mtime": 1756157712.217539, "terms": ["def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks"]}
{"chunk_id": "951dda2c98b21fc2e773ec25", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 129, "end_line": 208, "content": "    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted", "mtime": 1756157712.217539, "terms": ["def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted"]}
{"chunk_id": "c889361f451599173cddb9ba", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 137, "end_line": 216, "content": "\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,", "mtime": 1756157712.217539, "terms": ["def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true"]}
{"chunk_id": "08b2cacbe3b77764398e10d8", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 138, "end_line": 217, "content": "    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,", "mtime": 1756157712.217539, "terms": ["def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "fb126a09908cdf9eb3b606d9", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 164, "end_line": 243, "content": "    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue", "mtime": 1756157712.217539, "terms": ["def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue"]}
{"chunk_id": "2307d449d7b381183b775918", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 200, "end_line": 279, "content": "    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)", "mtime": 1756157712.217539, "terms": ["def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid"]}
{"chunk_id": "024641090b20d351834e0223", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 205, "end_line": 284, "content": "                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUNÃ‡Ã•ES DE INDEXAÃ‡ÃƒO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)", "mtime": 1756157712.217539, "terms": ["seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days"]}
{"chunk_id": "99aeda8fa635403a086d0cf6", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 213, "end_line": 292, "content": "def index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # AtenÃ§Ã£o: rglob/glob invertidos â€” para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUNÃ‡Ã•ES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}", "mtime": 1756157712.217539, "terms": ["def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores"]}
{"chunk_id": "6729153160ccdf96129e0496", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 256, "end_line": 335, "content": "def _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))", "mtime": 1756157712.217539, "terms": ["def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys"]}
{"chunk_id": "a37e6e5919205cbcf97cbd18", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 273, "end_line": 352, "content": "    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,", "mtime": 1756157712.217539, "terms": ["return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid"]}
{"chunk_id": "c1437f37df02d7b591800e23", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 275, "end_line": 354, "content": "def _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],", "mtime": 1756157712.217539, "terms": ["def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line"]}
{"chunk_id": "297b23b0f37ac64485045b53", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 287, "end_line": 366, "content": "def _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:", "mtime": 1756157712.217539, "terms": ["def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str"]}
{"chunk_id": "b7f5127edc84d4e044a54a6e", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 295, "end_line": 374, "content": "def _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n", "mtime": 1756157712.217539, "terms": ["def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens"]}
{"chunk_id": "9791ec00b6d31436dd12f005", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 303, "end_line": 382, "content": "def _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n", "mtime": 1756157712.217539, "terms": ["def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break"]}
{"chunk_id": "423f31b18d0013532d97863a", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 325, "end_line": 404, "content": "def search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combinaÃ§Ã£o simples (80â€“90% lexical, 10â€“20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",", "mtime": 1756157712.217539, "terms": ["def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end"]}
{"chunk_id": "1aaf29a6f588ee61100ba3f4", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 341, "end_line": 420, "content": "    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUNÃ‡Ã•ES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,", "mtime": 1756157712.217539, "terms": ["pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer"]}
{"chunk_id": "70d98893cefaaae0a927bc61", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 363, "end_line": 442, "content": "def get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:", "mtime": 1756157712.217539, "terms": ["def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks"]}
{"chunk_id": "31410c97439542296ca0f193", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 366, "end_line": 445, "content": "def _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento", "mtime": 1756157712.217539, "terms": ["def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento"]}
{"chunk_id": "daa86acea3240df0d0a928b9", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 389, "end_line": 468, "content": "def build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para injeÃ§Ã£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do orÃ§amento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por iteraÃ§Ã£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG MÃ‰TRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),", "mtime": 1756157712.217539, "terms": ["def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds"]}
{"chunk_id": "d02fa64d3f75e725076b8399", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 409, "end_line": 488, "content": "        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir latÃªncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (jÃ¡ inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se nÃ£o cabe e jÃ¡ temos algo no pack, tenta o prÃ³ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no orÃ§amento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por iteraÃ§Ã£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG MÃ‰TRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n            \"query\": query[:160],\n            \"chunk_count\": len(pack[\"chunks\"]),\n            \"total_tokens\": pack[\"total_tokens\"],\n            \"budget_tokens\": budget_tokens,\n            \"budget_utilization\": round(pack[\"total_tokens\"] / max(1, budget_tokens), 3),\n            \"latency_ms\": latency_ms,\n        })\n    except Exception:\n        pass  # nÃ£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de cÃ³digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca semÃ¢ntica com embeddings", "mtime": 1756157712.217539, "terms": ["t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "query", "query", "chunk_count", "len", "pack", "chunks", "total_tokens", "pack", "total_tokens", "budget_tokens", "budget_tokens", "budget_utilization", "round", "pack", "total_tokens", "max", "budget_tokens", "latency_ms", "latency_ms", "except", "exception", "pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings"]}
{"chunk_id": "ac8f36ca8bd7aa0599c45c54", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 477, "end_line": 556, "content": "        pass  # nÃ£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de cÃ³digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca semÃ¢ntica com embeddings\n    - Auto-indexaÃ§Ã£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys", "mtime": 1756157712.217539, "terms": ["pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys"]}
{"chunk_id": "6c357045c21a185f3b886abc", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 484, "end_line": 563, "content": "class EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de cÃ³digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca semÃ¢ntica com embeddings\n    - Auto-indexaÃ§Ã£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,", "mtime": 1756157712.217539, "terms": ["class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "b34f567874589c7436394d14", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 493, "end_line": 572, "content": "    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # ConfiguraÃ§Ãµes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar busca semÃ¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem serÃ¡ enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao inicializar auto-indexaÃ§Ã£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir", "mtime": 1756157712.217539, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir"]}
{"chunk_id": "164abf4e1322e8a6a6c57095", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 539, "end_line": 618, "content": "    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexaÃ§Ã£o automÃ¡tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o", "mtime": 1756157712.217539, "terms": ["def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr"]}
{"chunk_id": "4fef2764cf59ea7e6b7a454a", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 545, "end_line": 624, "content": "\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para nÃ£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"ðŸ”„ Auto-reindexaÃ§Ã£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na auto-indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"", "mtime": 1756157712.217539, "terms": ["reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia"]}
{"chunk_id": "1ec0c158f721a5d86ede9883", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 560, "end_line": 639, "content": "    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: PadrÃµes de arquivos para incluir\n            exclude_globs: PadrÃµes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            DicionÃ¡rio com estatÃ­sticas da indexaÃ§Ã£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"ðŸ“ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa funÃ§Ã£o de indexaÃ§Ã£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"âœ… IndexaÃ§Ã£o concluÃ­da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âŒ Erro na indexaÃ§Ã£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25", "mtime": 1756157712.217539, "terms": ["def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25"]}
{"chunk_id": "c951cb6fffc3b7718a51a3e4", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 605, "end_line": 684, "content": "    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca hÃ­brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âš ï¸  Erro na busca semÃ¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )", "mtime": 1756157712.217539, "terms": ["def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic"]}
{"chunk_id": "d398f18708afb535ede522a5", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 613, "end_line": 692, "content": "        Busca hÃ­brida combinando BM25 e busca semÃ¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: NÃºmero mÃ¡ximo de resultados\n            use_semantic: Se True, usa busca hÃ­brida. Se None, usa configuraÃ§Ã£o padrÃ£o\n            semantic_weight: Peso da similaridade semÃ¢ntica (sobrescreve padrÃ£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relevÃ¢ncia\n        \"\"\"\n        # Define se usar busca semÃ¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa funÃ§Ã£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca semÃ¢ntica nÃ£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca hÃ­brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âš ï¸  Erro na busca semÃ¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa funÃ§Ã£o de construÃ§Ã£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela funÃ§Ã£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],", "mtime": 1756157712.217539, "terms": ["busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id"]}
{"chunk_id": "662c6a0af22360c01e37e457", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 660, "end_line": 739, "content": "    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        ConstrÃ³i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: OrÃ§amento mÃ¡ximo de tokens\n            max_chunks: NÃºmero mÃ¡ximo de chunks\n            strategy: EstratÃ©gia de seleÃ§Ã£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca semÃ¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa funÃ§Ã£o de construÃ§Ã£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela funÃ§Ã£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # ConstrÃ³i pack usando funÃ§Ã£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informaÃ§Ãµes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    ", "mtime": 1756157712.217539, "terms": ["def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false"]}
{"chunk_id": "47e7b9f31c7037aecd3f075b", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 681, "end_line": 760, "content": "            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor seleÃ§Ã£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa funÃ§Ã£o de construÃ§Ã£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela funÃ§Ã£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # ConstrÃ³i pack usando funÃ§Ã£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informaÃ§Ãµes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor", "mtime": 1756157712.217539, "terms": ["query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor"]}
{"chunk_id": "b864bb94267c6e626906c872", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 717, "end_line": 796, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O parÃ¢metro enable_semantic nÃ£o Ã© usado diretamente na indexaÃ§Ã£o,\n    # mas Ã© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,", "mtime": 1756157712.217539, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks"]}
{"chunk_id": "e38d1acf82b221b2875fa13d", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 728, "end_line": 801, "content": "    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexaÃ§Ã£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao iniciar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O parÃ¢metro enable_semantic nÃ£o Ã© usado diretamente na indexaÃ§Ã£o,\n    # mas Ã© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "027dd24d991cade03a4b9bfa", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 740, "end_line": 801, "content": "    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O parÃ¢metro enable_semantic nÃ£o Ã© usado diretamente na indexaÃ§Ã£o,\n    # mas Ã© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "ff1df7f9fd359ae30159436f", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 749, "end_line": 801, "content": "            sys.stderr.write(f\"âŒ Erro ao parar auto-indexaÃ§Ã£o: {e}\\n\")\n            return False\n\n# ========== FUNÃ‡Ã•ES PÃšBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O parÃ¢metro enable_semantic nÃ£o Ã© usado diretamente na indexaÃ§Ã£o,\n    # mas Ã© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "b970d5b43b42ad97dd2e6003", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 754, "end_line": 801, "content": "def enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O parÃ¢metro enable_semantic nÃ£o Ã© usado diretamente na indexaÃ§Ã£o,\n    # mas Ã© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "21ebb990dae8f9512ffe5213", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 772, "end_line": 801, "content": "def enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os parÃ¢metros semantic_weight e use_mmr sÃ£o mantidos para compatibilidade\n    # mas podem nÃ£o ser usados dependendo da implementaÃ§Ã£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "d13300788a2c19ccc69d5aa2", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 785, "end_line": 801, "content": "def enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com cÃ³digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756157712.217539, "terms": ["def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "dcb14bca6afd685432a0b28c", "file_path": "mcp_system/reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diretÃ³rio de Ã­ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de mÃ©tricas (MCP_METRICS_FILE ou .mcp_index/metrics.csv).\n- (Opcional) Calcula baseline aproximada de tokens do repositÃ³rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\n# ---- Config mÃ©tricas (mesmo padrÃ£o do context_pack) - agora usando caminho relativo Ã  pasta mcp_system\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", str(CURRENT_DIR / \".mcp_index/metrics.csv\"))\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n", "mtime": 1756154900.4583526, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "config", "tricas", "mesmo", "padr", "do", "context_pack", "agora", "usando", "caminho", "relativo", "pasta", "mcp_system", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "str", "current_dir", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise"]}
{"chunk_id": "e4441d04b78ca25ef7128f2b", "file_path": "mcp_system/reindex.py", "start_line": 38, "end_line": 117, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1756154900.4583526, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "c7227327ac70b1ff6ba7f8cd", "file_path": "mcp_system/reindex.py", "start_line": 47, "end_line": 126, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heurÃ­stica compatÃ­vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1756154900.4583526, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "c7a952603750e274e1dc052c", "file_path": "mcp_system/reindex.py", "start_line": 51, "end_line": 130, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"DiretÃ³rio de Ã­ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o Ã­ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclusÃ£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclusÃ£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"ApÃ³s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos saÃ­da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1756154900.4583526, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "ff543b5860258d312549397e", "file_path": "mcp_system/reindex.py", "start_line": 69, "end_line": 148, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] NÃ£o foi possÃ­vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"ðŸ”„ Limpando Ã­ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diretÃ³rio de Ã­ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # LÃª o Ã­ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; senÃ£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"ðŸ“Š Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1756154900.4583526, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "cfcabe30dd9d4432efc0c6bd", "file_path": "mcp_system/reindex.py", "start_line": 137, "end_line": 167, "content": "\n    if not args.quiet:\n        print(f\"âœ… Reindex concluÃ­da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de mÃ©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # nÃ£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154900.4583526, "terms": ["if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "3f993a8a8e009db391fa2800", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSetup automÃ¡tico do sistema MCP melhorado\nConfigura busca semÃ¢ntica, auto-indexaÃ§Ã£o e otimizaÃ§Ãµes\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ndef print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True", "mtime": 1756154931.1757026, "terms": ["usr", "bin", "env", "python3", "setup", "autom", "tico", "do", "sistema", "mcp", "melhorado", "configura", "busca", "sem", "ntica", "auto", "indexa", "otimiza", "es", "import", "os", "import", "sys", "import", "subprocess", "import", "time", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "any", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true"]}
{"chunk_id": "6444ec23468ea9e790a0f00a", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 18, "end_line": 97, "content": "def print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"ðŸš€ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"", "mtime": 1756154931.1757026, "terms": ["def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt"]}
{"chunk_id": "a9a46bf055a6209d3cafaa8f", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 24, "end_line": 103, "content": "def print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"âœ… {step}\")\n    elif status == \"warn\":\n        print(f\"âš ï¸  {step}\")\n    elif status == \"error\":\n        print(f\"âŒ {step}\")\n    else:\n        print(f\"ðŸ”„ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            ", "mtime": 1756154931.1757026, "terms": ["def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true"]}
{"chunk_id": "ac77571b66ca63e9fe91742a", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 35, "end_line": 114, "content": "def check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica dependÃªncias necessÃ¡rias\"\"\"\n    print_header(\"Verificando DependÃªncias\")\n    \n    deps = {}\n    \n    # Python bÃ¡sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca semÃ¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")", "mtime": 1756154931.1757026, "terms": ["def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers"]}
{"chunk_id": "f805a71e8cb390e18a189424", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 69, "end_line": 148, "content": "    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexaÃ§Ã£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")\n            if \"watchdog\" in missing_deps:\n                packages.append(\"watchdog>=3.0.0\")\n            if \"numpy\" in missing_deps:\n                packages.append(\"numpy>=1.21.0\")\n            if \"scikit_learn\" in missing_deps:\n                packages.append(\"scikit-learn>=1.0.0\")\n                \n            if packages:\n                print_step(\"Instalando pacotes individualmente\")\n                result = subprocess.run([\n                    sys.executable, \"-m\", \"pip\", \"install\"] + packages,\n                    capture_output=True, text=True\n                )\n                \n                if result.returncode == 0:\n                    print_step(\"DependÃªncias instaladas\", \"ok\")\n                    return True\n                else:\n                    print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n                    return False\n            else:\n                print_step(\"Nenhum pacote para instalar\", \"ok\")\n                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar dependÃªncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diretÃ³rio de Ã­ndice\"\"\"\n    print_header(\"Configurando DiretÃ³rio de Ãndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"", "mtime": 1756154931.1757026, "terms": ["try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers", "if", "watchdog", "in", "missing_deps", "packages", "append", "watchdog", "if", "numpy", "in", "missing_deps", "packages", "append", "numpy", "if", "scikit_learn", "in", "missing_deps", "packages", "append", "scikit", "learn", "if", "packages", "print_step", "instalando", "pacotes", "individualmente", "result", "subprocess", "run", "sys", "executable", "pip", "install", "packages", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "print_step", "nenhum", "pacote", "para", "instalar", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index"]}
{"chunk_id": "d7986f82d676909d15dec12c", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 88, "end_line": 167, "content": "def install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala dependÃªncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando DependÃªncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"DependÃªncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")\n            if \"watchdog\" in missing_deps:\n                packages.append(\"watchdog>=3.0.0\")\n            if \"numpy\" in missing_deps:\n                packages.append(\"numpy>=1.21.0\")\n            if \"scikit_learn\" in missing_deps:\n                packages.append(\"scikit-learn>=1.0.0\")\n                \n            if packages:\n                print_step(\"Instalando pacotes individualmente\")\n                result = subprocess.run([\n                    sys.executable, \"-m\", \"pip\", \"install\"] + packages,\n                    capture_output=True, text=True\n                )\n                \n                if result.returncode == 0:\n                    print_step(\"DependÃªncias instaladas\", \"ok\")\n                    return True\n                else:\n                    print_step(f\"Erro na instalaÃ§Ã£o: {result.stderr}\", \"error\")\n                    return False\n            else:\n                print_step(\"Nenhum pacote para instalar\", \"ok\")\n                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar dependÃªncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diretÃ³rio de Ã­ndice\"\"\"\n    print_header(\"Configurando DiretÃ³rio de Ãndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"DiretÃ³rio de Ã­ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diretÃ³rio de Ã­ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade bÃ¡sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade BÃ¡sica\")\n    \n    try:\n        # Testar importaÃ§Ã£o dos mÃ³dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"ImportaÃ§Ã£o do indexador bÃ¡sico\", \"ok\")\n        \n        # Testar criaÃ§Ã£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))", "mtime": 1756154931.1757026, "terms": ["def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers", "if", "watchdog", "in", "missing_deps", "packages", "append", "watchdog", "if", "numpy", "in", "missing_deps", "packages", "append", "numpy", "if", "scikit_learn", "in", "missing_deps", "packages", "append", "scikit", "learn", "if", "packages", "print_step", "instalando", "pacotes", "individualmente", "result", "subprocess", "run", "sys", "executable", "pip", "install", "packages", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "print_step", "nenhum", "pacote", "para", "instalar", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir"]}
{"chunk_id": "2f1304046725b97db9ac6153", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 137, "end_line": 216, "content": "                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar dependÃªncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diretÃ³rio de Ã­ndice\"\"\"\n    print_header(\"Configurando DiretÃ³rio de Ãndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"DiretÃ³rio de Ã­ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diretÃ³rio de Ã­ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade bÃ¡sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade BÃ¡sica\")\n    \n    try:\n        # Testar importaÃ§Ã£o dos mÃ³dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"ImportaÃ§Ã£o do indexador bÃ¡sico\", \"ok\")\n        \n        # Testar criaÃ§Ã£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"CriaÃ§Ã£o do indexador\", \"ok\")\n        \n        # Testar mÃ©todos bÃ¡sicos\n        stats = indexer.get_stats()\n        print_step(\"ObtenÃ§Ã£o de estatÃ­sticas\", \"ok\")\n        \n        print_step(\"Testes bÃ¡sicos concluÃ­dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes bÃ¡sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"FunÃ§Ã£o principal de setup\"\"\"\n    print_header(\"Setup AutomÃ¡tico do Sistema MCP\")\n    \n    # Verificar dependÃªncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDependÃªncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as dependÃªncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instalaÃ§Ã£o de dependÃªncias. Saindo.\")\n                return 1\n        else:\n            print(\"InstalaÃ§Ã£o de dependÃªncias cancelada.\")\n    else:\n        print(\"\\nâœ… Todas as dependÃªncias estÃ£o instaladas!\")\n    \n    # Configurar diretÃ³rio de Ã­ndice\n    if not setup_index_directory():\n        print(\"Falha na configuraÃ§Ã£o do diretÃ³rio de Ã­ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade bÃ¡sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes bÃ¡sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup ConcluÃ­do com Sucesso!\")\n    print(\"O sistema MCP estÃ¡ pronto para uso.\")\n    print(f\"O diretÃ³rio de Ã­ndice estÃ¡ localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n", "mtime": 1756154931.1757026, "terms": ["return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return"]}
{"chunk_id": "ec4afc2833c17ea315ca824d", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 143, "end_line": 218, "content": "def setup_index_directory():\n    \"\"\"Configura o diretÃ³rio de Ã­ndice\"\"\"\n    print_header(\"Configurando DiretÃ³rio de Ãndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"DiretÃ³rio de Ã­ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diretÃ³rio de Ã­ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade bÃ¡sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade BÃ¡sica\")\n    \n    try:\n        # Testar importaÃ§Ã£o dos mÃ³dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"ImportaÃ§Ã£o do indexador bÃ¡sico\", \"ok\")\n        \n        # Testar criaÃ§Ã£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"CriaÃ§Ã£o do indexador\", \"ok\")\n        \n        # Testar mÃ©todos bÃ¡sicos\n        stats = indexer.get_stats()\n        print_step(\"ObtenÃ§Ã£o de estatÃ­sticas\", \"ok\")\n        \n        print_step(\"Testes bÃ¡sicos concluÃ­dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes bÃ¡sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"FunÃ§Ã£o principal de setup\"\"\"\n    print_header(\"Setup AutomÃ¡tico do Sistema MCP\")\n    \n    # Verificar dependÃªncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDependÃªncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as dependÃªncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instalaÃ§Ã£o de dependÃªncias. Saindo.\")\n                return 1\n        else:\n            print(\"InstalaÃ§Ã£o de dependÃªncias cancelada.\")\n    else:\n        print(\"\\nâœ… Todas as dependÃªncias estÃ£o instaladas!\")\n    \n    # Configurar diretÃ³rio de Ã­ndice\n    if not setup_index_directory():\n        print(\"Falha na configuraÃ§Ã£o do diretÃ³rio de Ã­ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade bÃ¡sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes bÃ¡sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup ConcluÃ­do com Sucesso!\")\n    print(\"O sistema MCP estÃ¡ pronto para uso.\")\n    print(f\"O diretÃ³rio de Ã­ndice estÃ¡ localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "399a8d5f2197b1123cd7f778", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 156, "end_line": 218, "content": "def test_basic_functionality():\n    \"\"\"Testa funcionalidade bÃ¡sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade BÃ¡sica\")\n    \n    try:\n        # Testar importaÃ§Ã£o dos mÃ³dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"ImportaÃ§Ã£o do indexador bÃ¡sico\", \"ok\")\n        \n        # Testar criaÃ§Ã£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"CriaÃ§Ã£o do indexador\", \"ok\")\n        \n        # Testar mÃ©todos bÃ¡sicos\n        stats = indexer.get_stats()\n        print_step(\"ObtenÃ§Ã£o de estatÃ­sticas\", \"ok\")\n        \n        print_step(\"Testes bÃ¡sicos concluÃ­dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes bÃ¡sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"FunÃ§Ã£o principal de setup\"\"\"\n    print_header(\"Setup AutomÃ¡tico do Sistema MCP\")\n    \n    # Verificar dependÃªncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDependÃªncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as dependÃªncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instalaÃ§Ã£o de dependÃªncias. Saindo.\")\n                return 1\n        else:\n            print(\"InstalaÃ§Ã£o de dependÃªncias cancelada.\")\n    else:\n        print(\"\\nâœ… Todas as dependÃªncias estÃ£o instaladas!\")\n    \n    # Configurar diretÃ³rio de Ã­ndice\n    if not setup_index_directory():\n        print(\"Falha na configuraÃ§Ã£o do diretÃ³rio de Ã­ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade bÃ¡sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes bÃ¡sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup ConcluÃ­do com Sucesso!\")\n    print(\"O sistema MCP estÃ¡ pronto para uso.\")\n    print(f\"O diretÃ³rio de Ã­ndice estÃ¡ localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "9a283c92b0a2d85c790ce7c7", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 181, "end_line": 218, "content": "def main():\n    \"\"\"FunÃ§Ã£o principal de setup\"\"\"\n    print_header(\"Setup AutomÃ¡tico do Sistema MCP\")\n    \n    # Verificar dependÃªncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDependÃªncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as dependÃªncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instalaÃ§Ã£o de dependÃªncias. Saindo.\")\n                return 1\n        else:\n            print(\"InstalaÃ§Ã£o de dependÃªncias cancelada.\")\n    else:\n        print(\"\\nâœ… Todas as dependÃªncias estÃ£o instaladas!\")\n    \n    # Configurar diretÃ³rio de Ã­ndice\n    if not setup_index_directory():\n        print(\"Falha na configuraÃ§Ã£o do diretÃ³rio de Ã­ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade bÃ¡sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes bÃ¡sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup ConcluÃ­do com Sucesso!\")\n    print(\"O sistema MCP estÃ¡ pronto para uso.\")\n    print(f\"O diretÃ³rio de Ã­ndice estÃ¡ localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "1c5e862a4630a5adb92a3f7d", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 205, "end_line": 218, "content": "    \n    # Testar funcionalidade bÃ¡sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes bÃ¡sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup ConcluÃ­do com Sucesso!\")\n    print(\"O sistema MCP estÃ¡ pronto para uso.\")\n    print(f\"O diretÃ³rio de Ã­ndice estÃ¡ localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "018e6cb2e7752aae00f8f7e0", "file_path": "tests/test_currency_utils.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSuite de testes para o mÃ³dulo de utilitÃ¡rios de moeda.\n\"\"\"\n\nimport sys\nimport os\nimport pytest\nfrom decimal import Decimal\nimport pandas as pd\n\n# Adiciona o diretÃ³rio raiz ao path para imports\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\ndef test_currency_detection_from_text():\n    \"\"\"Testa a detecÃ§Ã£o de moeda a partir de texto.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa detecÃ§Ã£o de Euro\n    text_eur = \"MovimentaÃ§Ãµes em Euros (â‚¬) para o perÃ­odo\"\n    assert CurrencyUtils.detect_currency_from_text(text_eur) == \"EUR\"\n    \n    # Testa detecÃ§Ã£o de Real - corrigido para usar padrÃ£o que funciona com a implementaÃ§Ã£o\n    text_brl = \"R$ 1.234,56 - Extrato em Reais\"\n    assert CurrencyUtils.detect_currency_from_text(text_brl) == \"BRL\"\n    \n    # Testa detecÃ§Ã£o de DÃ³lar\n    text_usd = \"Account Statement in US Dollars $1,234.56\"\n    assert CurrencyUtils.detect_currency_from_text(text_usd) == \"USD\"\n    \n    # Testa fallback para EUR\n    text_unknown = \"Random text without currency symbols\"\n    assert CurrencyUtils.detect_currency_from_text(text_unknown) == \"EUR\"\n\ndef test_currency_symbol_retrieval():\n    \"\"\"Testa a obtenÃ§Ã£o de sÃ­mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa sÃ­mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"â‚¬\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"Â£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formataÃ§Ã£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formataÃ§Ã£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"â‚¬ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"â‚¬ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em DÃ³lar (estilo americano) - corrigido para o formato real da implementaÃ§Ã£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formataÃ§Ã£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"â‚¬ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores", "mtime": 1756145852.325752, "terms": ["usr", "bin", "env", "python3", "suite", "de", "testes", "para", "dulo", "de", "utilit", "rios", "de", "moeda", "import", "sys", "import", "os", "import", "pytest", "from", "decimal", "import", "decimal", "import", "pandas", "as", "pd", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "imports", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "def", "test_currency_detection_from_text", "testa", "detec", "de", "moeda", "partir", "de", "texto", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "detec", "de", "euro", "text_eur", "movimenta", "es", "em", "euros", "para", "per", "odo", "assert", "currencyutils", "detect_currency_from_text", "text_eur", "eur", "testa", "detec", "de", "real", "corrigido", "para", "usar", "padr", "que", "funciona", "com", "implementa", "text_brl", "extrato", "em", "reais", "assert", "currencyutils", "detect_currency_from_text", "text_brl", "brl", "testa", "detec", "de", "lar", "text_usd", "account", "statement", "in", "us", "dollars", "assert", "currencyutils", "detect_currency_from_text", "text_usd", "usd", "testa", "fallback", "para", "eur", "text_unknown", "random", "text", "without", "currency", "symbols", "assert", "currencyutils", "detect_currency_from_text", "text_unknown", "eur", "def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores"]}
{"chunk_id": "5674ac45320904ca7313c3ef", "file_path": "tests/test_currency_utils.py", "start_line": 16, "end_line": 95, "content": "def test_currency_detection_from_text():\n    \"\"\"Testa a detecÃ§Ã£o de moeda a partir de texto.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa detecÃ§Ã£o de Euro\n    text_eur = \"MovimentaÃ§Ãµes em Euros (â‚¬) para o perÃ­odo\"\n    assert CurrencyUtils.detect_currency_from_text(text_eur) == \"EUR\"\n    \n    # Testa detecÃ§Ã£o de Real - corrigido para usar padrÃ£o que funciona com a implementaÃ§Ã£o\n    text_brl = \"R$ 1.234,56 - Extrato em Reais\"\n    assert CurrencyUtils.detect_currency_from_text(text_brl) == \"BRL\"\n    \n    # Testa detecÃ§Ã£o de DÃ³lar\n    text_usd = \"Account Statement in US Dollars $1,234.56\"\n    assert CurrencyUtils.detect_currency_from_text(text_usd) == \"USD\"\n    \n    # Testa fallback para EUR\n    text_unknown = \"Random text without currency symbols\"\n    assert CurrencyUtils.detect_currency_from_text(text_unknown) == \"EUR\"\n\ndef test_currency_symbol_retrieval():\n    \"\"\"Testa a obtenÃ§Ã£o de sÃ­mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa sÃ­mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"â‚¬\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"Â£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formataÃ§Ã£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formataÃ§Ã£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"â‚¬ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"â‚¬ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em DÃ³lar (estilo americano) - corrigido para o formato real da implementaÃ§Ã£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formataÃ§Ã£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"â‚¬ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo sÃ­mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com sÃ­mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes", "mtime": 1756145852.325752, "terms": ["def", "test_currency_detection_from_text", "testa", "detec", "de", "moeda", "partir", "de", "texto", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "detec", "de", "euro", "text_eur", "movimenta", "es", "em", "euros", "para", "per", "odo", "assert", "currencyutils", "detect_currency_from_text", "text_eur", "eur", "testa", "detec", "de", "real", "corrigido", "para", "usar", "padr", "que", "funciona", "com", "implementa", "text_brl", "extrato", "em", "reais", "assert", "currencyutils", "detect_currency_from_text", "text_brl", "brl", "testa", "detec", "de", "lar", "text_usd", "account", "statement", "in", "us", "dollars", "assert", "currencyutils", "detect_currency_from_text", "text_usd", "usd", "testa", "fallback", "para", "eur", "text_unknown", "random", "text", "without", "currency", "symbols", "assert", "currencyutils", "detect_currency_from_text", "text_unknown", "eur", "def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes"]}
{"chunk_id": "b64cffad2da353661d1148a1", "file_path": "tests/test_currency_utils.py", "start_line": 36, "end_line": 96, "content": "def test_currency_symbol_retrieval():\n    \"\"\"Testa a obtenÃ§Ã£o de sÃ­mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa sÃ­mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"â‚¬\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"Â£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formataÃ§Ã£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formataÃ§Ã£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"â‚¬ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"â‚¬ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em DÃ³lar (estilo americano) - corrigido para o formato real da implementaÃ§Ã£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formataÃ§Ã£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"â‚¬ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo sÃ­mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com sÃ­mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "77c702a4656a69c66e34f137", "file_path": "tests/test_currency_utils.py", "start_line": 49, "end_line": 96, "content": "def test_currency_formatting():\n    \"\"\"Testa a formataÃ§Ã£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formataÃ§Ã£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"â‚¬ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"â‚¬ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formataÃ§Ã£o em DÃ³lar (estilo americano) - corrigido para o formato real da implementaÃ§Ã£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formataÃ§Ã£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"â‚¬ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo sÃ­mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com sÃ­mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "4cba7a0e9e26ed1052783374", "file_path": "tests/test_currency_utils.py", "start_line": 68, "end_line": 96, "content": "def test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo sÃ­mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com sÃ­mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "90dcca73a44b24ccd7436c25", "file_path": "tests/test_currency_utils.py", "start_line": 69, "end_line": 96, "content": "    \"\"\"Testa a extraÃ§Ã£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monetÃ¡rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['â‚¬ -85,30', 'â‚¬ 2.500,00']  # Valores com sÃ­mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo sÃ­mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['Pagamento conta luz', 'DepÃ³sito salÃ¡rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com sÃ­mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "95dcde83e904f9c9e58813a1", "file_path": "tests/test_csv_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nTestes para o leitor de extratos CSV.\n\"\"\"\nimport sys\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nimport pytest\nimport pandas as pd\n\n# Adiciona o diretÃ³rio raiz ao path para importaÃ§Ãµes\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\nfrom src.infrastructure.readers.csv_reader import CSVStatementReader\nfrom src.domain.models import Transaction, TransactionType\n\n\ndef test_csv_reader_can_read():\n    \"\"\"Testa se o CSV reader pode identificar arquivos CSV.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Deve retornar True para arquivos CSV\n    assert reader.can_read(Path(\"test.csv\")) == True\n    assert reader.can_read(Path(\"test.CSV\")) == True\n    \n    # Deve retornar False para outros formatos\n    assert reader.can_read(Path(\"test.pdf\")) == False\n    assert reader.can_read(Path(\"test.xlsx\")) == False\n    \n    # Testa com string tambÃ©m\n    assert reader.can_read(\"test.csv\") == True\n\n\ndef test_csv_reader_extract_transactions():\n    \"\"\"Testa a extraÃ§Ã£o de transaÃ§Ãµes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transaÃ§Ã£o (receita)\n    assert transactions[0].description == 'SalÃ¡rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transaÃ§Ã£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transaÃ§Ã£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n", "mtime": 1756147893.729488, "terms": ["testes", "para", "leitor", "de", "extratos", "csv", "import", "sys", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "import", "pytest", "import", "pandas", "as", "pd", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "importa", "es", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "def", "test_csv_reader_can_read", "testa", "se", "csv", "reader", "pode", "identificar", "arquivos", "csv", "reader", "csvstatementreader", "deve", "retornar", "true", "para", "arquivos", "csv", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "csv", "true", "deve", "retornar", "false", "para", "outros", "formatos", "assert", "reader", "can_read", "path", "test", "pdf", "false", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "testa", "com", "string", "tamb", "assert", "reader", "can_read", "test", "csv", "true", "def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime"]}
{"chunk_id": "58652c5eb183f0a1de1a843d", "file_path": "tests/test_csv_reader.py", "start_line": 20, "end_line": 99, "content": "def test_csv_reader_can_read():\n    \"\"\"Testa se o CSV reader pode identificar arquivos CSV.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Deve retornar True para arquivos CSV\n    assert reader.can_read(Path(\"test.csv\")) == True\n    assert reader.can_read(Path(\"test.CSV\")) == True\n    \n    # Deve retornar False para outros formatos\n    assert reader.can_read(Path(\"test.pdf\")) == False\n    assert reader.can_read(Path(\"test.xlsx\")) == False\n    \n    # Testa com string tambÃ©m\n    assert reader.can_read(\"test.csv\") == True\n\n\ndef test_csv_reader_extract_transactions():\n    \"\"\"Testa a extraÃ§Ã£o de transaÃ§Ãµes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transaÃ§Ã£o (receita)\n    assert transactions[0].description == 'SalÃ¡rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transaÃ§Ã£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transaÃ§Ã£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monetÃ¡rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com parÃªnteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_can_read", "testa", "se", "csv", "reader", "pode", "identificar", "arquivos", "csv", "reader", "csvstatementreader", "deve", "retornar", "true", "para", "arquivos", "csv", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "csv", "true", "deve", "retornar", "false", "para", "outros", "formatos", "assert", "reader", "can_read", "path", "test", "pdf", "false", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "testa", "com", "string", "tamb", "assert", "reader", "can_read", "test", "csv", "true", "def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit"]}
{"chunk_id": "de10b938e85a3c7633646728", "file_path": "tests/test_csv_reader.py", "start_line": 36, "end_line": 115, "content": "def test_csv_reader_extract_transactions():\n    \"\"\"Testa a extraÃ§Ã£o de transaÃ§Ãµes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transaÃ§Ã£o (receita)\n    assert transactions[0].description == 'SalÃ¡rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transaÃ§Ã£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transaÃ§Ã£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monetÃ¡rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com parÃªnteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com vÃ­rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores vÃ¡lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com vÃ­rgula", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula"]}
{"chunk_id": "ece790707a034ab8bb607554", "file_path": "tests/test_csv_reader.py", "start_line": 68, "end_line": 147, "content": "def test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monetÃ¡rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com parÃªnteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com vÃ­rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores vÃ¡lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com vÃ­rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores invÃ¡lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    ", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri"]}
{"chunk_id": "a769ed1a55d2886508beb7d8", "file_path": "tests/test_csv_reader.py", "start_line": 69, "end_line": 148, "content": "    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monetÃ¡rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com parÃªnteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com vÃ­rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores vÃ¡lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com vÃ­rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores invÃ¡lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    \n    # Deve retornar None para colunas inexistentes", "mtime": 1756147893.729488, "terms": ["testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes"]}
{"chunk_id": "5881ef2dced199e32cbbf02f", "file_path": "tests/test_csv_reader.py", "start_line": 82, "end_line": 161, "content": "def test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monetÃ¡rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com parÃªnteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com vÃ­rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores vÃ¡lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com vÃ­rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores invÃ¡lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame mÃ­nimo sem informaÃ§Ã£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV temporÃ¡rio", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio"]}
{"chunk_id": "605e503c575b681184f97416", "file_path": "tests/test_csv_reader.py", "start_line": 107, "end_line": 176, "content": "def test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores vÃ¡lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com vÃ­rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores invÃ¡lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame mÃ­nimo sem informaÃ§Ã£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV temporÃ¡rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo temporÃ¡rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "212518b5b09514dd1ad7f2bc", "file_path": "tests/test_csv_reader.py", "start_line": 128, "end_line": 176, "content": "def test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame mÃ­nimo sem informaÃ§Ã£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV temporÃ¡rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo temporÃ¡rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "d0ae111bd7ce5dbe999403ab", "file_path": "tests/test_csv_reader.py", "start_line": 137, "end_line": 176, "content": "    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'DescriÃ§Ã£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descriÃ§Ã£o', 'description']) == 'DescriÃ§Ã£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame mÃ­nimo sem informaÃ§Ã£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV temporÃ¡rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo temporÃ¡rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "2c908d230eae25a22aea2b18", "file_path": "tests/test_csv_reader.py", "start_line": 152, "end_line": 176, "content": "def test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame mÃ­nimo sem informaÃ§Ã£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['SalÃ¡rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV temporÃ¡rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo temporÃ¡rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "44d5ab6f1d23e1dd00bd559c", "file_path": "tests/test_suite.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSuite de testes bÃ¡sica para o projeto de anÃ¡lise de extratos bancÃ¡rios\n\"\"\"\n\nimport sys\nimport os\nimport pytest\nfrom decimal import Decimal\nfrom datetime import datetime\nimport uuid\n\n# Adiciona o diretÃ³rio raiz ao path para imports\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\ndef test_imports():\n    \"\"\"Testa se os principais mÃ³dulos podem ser importados\"\"\"\n    try:\n        import main\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar main: {e}\")\n    \n    try:\n        from src.domain import models\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar src.domain.models: {e}\")\n    \n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar PDFStatementReader: {e}\")\n\ndef test_model_creation():\n    \"\"\"Testa a criaÃ§Ã£o de modelos bÃ¡sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa criaÃ§Ã£o de uma transaÃ§Ã£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa criaÃ§Ã£o de um extrato bancÃ¡rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa criaÃ§Ã£o de um resultado de anÃ¡lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],", "mtime": 1756145955.0716913, "terms": ["usr", "bin", "env", "python3", "suite", "de", "testes", "sica", "para", "projeto", "de", "an", "lise", "de", "extratos", "banc", "rios", "import", "sys", "import", "os", "import", "pytest", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "import", "uuid", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "imports", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "def", "test_imports", "testa", "se", "os", "principais", "dulos", "podem", "ser", "importados", "try", "import", "main", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "main", "try", "from", "src", "domain", "import", "models", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "src", "domain", "models", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "pdfstatementreader", "def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts"]}
{"chunk_id": "31cf527edbbec1bb1d0d8861", "file_path": "tests/test_suite.py", "start_line": 17, "end_line": 91, "content": "def test_imports():\n    \"\"\"Testa se os principais mÃ³dulos podem ser importados\"\"\"\n    try:\n        import main\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar main: {e}\")\n    \n    try:\n        from src.domain import models\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar src.domain.models: {e}\")\n    \n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar PDFStatementReader: {e}\")\n\ndef test_model_creation():\n    \"\"\"Testa a criaÃ§Ã£o de modelos bÃ¡sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa criaÃ§Ã£o de uma transaÃ§Ã£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa criaÃ§Ã£o de um extrato bancÃ¡rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa criaÃ§Ã£o de um resultado de anÃ¡lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["def", "test_imports", "testa", "se", "os", "principais", "dulos", "podem", "ser", "importados", "try", "import", "main", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "main", "try", "from", "src", "domain", "import", "models", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "src", "domain", "models", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "pdfstatementreader", "def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "46460358db719b0aa06fc5f4", "file_path": "tests/test_suite.py", "start_line": 37, "end_line": 91, "content": "def test_model_creation():\n    \"\"\"Testa a criaÃ§Ã£o de modelos bÃ¡sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa criaÃ§Ã£o de uma transaÃ§Ã£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa criaÃ§Ã£o de um extrato bancÃ¡rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa criaÃ§Ã£o de um resultado de anÃ¡lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "1dcfdbe11d3d9beea7c0e57d", "file_path": "tests/test_suite.py", "start_line": 69, "end_line": 91, "content": "        \n        # Testa criaÃ§Ã£o de um resultado de anÃ¡lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "09b3445e571a4749a735fda7", "file_path": "tests/test_comprehensive_suite.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nTestes abrangentes para o sistema de anÃ¡lise de extratos.\n\"\"\"\nimport sys\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nimport pytest\nimport uuid\n\n# Adiciona o diretÃ³rio raiz ao path para importaÃ§Ãµes\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\n# Testes de modelos de domÃ­nio\ndef test_domain_models():\n    \"\"\"Testa a criaÃ§Ã£o de modelos de domÃ­nio.\"\"\"\n    from src.domain.models import Transaction, BankStatement, AnalysisResult, TransactionType, TransactionCategory\n    \n    # Testa criaÃ§Ã£o de transaÃ§Ã£o\n    transaction = Transaction(\n        id=str(uuid.uuid4()),\n        date=datetime.now(),\n        description=\"Test transaction\",\n        amount=Decimal(\"100.50\"),\n        type=TransactionType.CREDIT,\n        category=TransactionCategory.SALARIO\n    )\n    \n    assert transaction.description == \"Test transaction\"\n    assert transaction.amount == Decimal(\"100.50\")\n    assert transaction.type == TransactionType.CREDIT\n    assert transaction.category == TransactionCategory.SALARIO\n    assert transaction.is_income == True\n    assert transaction.is_expense == False\n    \n    # Testa criaÃ§Ã£o de extrato bancÃ¡rio\n    statement = BankStatement(\n        id=str(uuid.uuid4()),\n        bank_name=\"Banco Teste\",\n        account_number=\"12345-6\",\n        period_start=datetime.now(),\n        period_end=datetime.now(),\n        initial_balance=Decimal(\"1000.00\"),\n        final_balance=Decimal(\"1500.00\"),\n        currency=\"EUR\",\n        transactions=[transaction]\n    )\n    \n    assert statement.bank_name == \"Banco Teste\"\n    assert statement.account_number == \"12345-6\"\n    assert statement.currency == \"EUR\"\n    assert len(statement.transactions) == 1\n    \n    # Testa cÃ¡lculo de totais\n    assert statement.total_income == Decimal(\"100.50\")\n    assert statement.total_expenses == Decimal(\"0.00\")\n\ndef test_pdf_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detecÃ§Ã£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest", "mtime": 1756147904.95932, "terms": ["testes", "abrangentes", "para", "sistema", "de", "an", "lise", "de", "extratos", "import", "sys", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "import", "pytest", "import", "uuid", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "importa", "es", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "testes", "de", "modelos", "de", "dom", "nio", "def", "test_domain_models", "testa", "cria", "de", "modelos", "de", "dom", "nio", "from", "src", "domain", "models", "import", "transaction", "bankstatement", "analysisresult", "transactiontype", "transactioncategory", "testa", "cria", "de", "transa", "transaction", "transaction", "id", "str", "uuid", "uuid4", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "category", "transactioncategory", "salario", "assert", "transaction", "description", "test", "transaction", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "credit", "assert", "transaction", "category", "transactioncategory", "salario", "assert", "transaction", "is_income", "true", "assert", "transaction", "is_expense", "false", "testa", "cria", "de", "extrato", "banc", "rio", "statement", "bankstatement", "id", "str", "uuid", "uuid4", "bank_name", "banco", "teste", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "currency", "eur", "transactions", "transaction", "assert", "statement", "bank_name", "banco", "teste", "assert", "statement", "account_number", "assert", "statement", "currency", "eur", "assert", "len", "statement", "transactions", "testa", "lculo", "de", "totais", "assert", "statement", "total_income", "decimal", "assert", "statement", "total_expenses", "decimal", "def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest"]}
{"chunk_id": "9a20eaaccb7ee734d7c3bbf5", "file_path": "tests/test_comprehensive_suite.py", "start_line": 16, "end_line": 95, "content": "def test_domain_models():\n    \"\"\"Testa a criaÃ§Ã£o de modelos de domÃ­nio.\"\"\"\n    from src.domain.models import Transaction, BankStatement, AnalysisResult, TransactionType, TransactionCategory\n    \n    # Testa criaÃ§Ã£o de transaÃ§Ã£o\n    transaction = Transaction(\n        id=str(uuid.uuid4()),\n        date=datetime.now(),\n        description=\"Test transaction\",\n        amount=Decimal(\"100.50\"),\n        type=TransactionType.CREDIT,\n        category=TransactionCategory.SALARIO\n    )\n    \n    assert transaction.description == \"Test transaction\"\n    assert transaction.amount == Decimal(\"100.50\")\n    assert transaction.type == TransactionType.CREDIT\n    assert transaction.category == TransactionCategory.SALARIO\n    assert transaction.is_income == True\n    assert transaction.is_expense == False\n    \n    # Testa criaÃ§Ã£o de extrato bancÃ¡rio\n    statement = BankStatement(\n        id=str(uuid.uuid4()),\n        bank_name=\"Banco Teste\",\n        account_number=\"12345-6\",\n        period_start=datetime.now(),\n        period_end=datetime.now(),\n        initial_balance=Decimal(\"1000.00\"),\n        final_balance=Decimal(\"1500.00\"),\n        currency=\"EUR\",\n        transactions=[transaction]\n    )\n    \n    assert statement.bank_name == \"Banco Teste\"\n    assert statement.account_number == \"12345-6\"\n    assert statement.currency == \"EUR\"\n    assert len(statement.transactions) == 1\n    \n    # Testa cÃ¡lculo de totais\n    assert statement.total_income == Decimal(\"100.50\")\n    assert statement.total_expenses == Decimal(\"0.00\")\n\ndef test_pdf_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detecÃ§Ã£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabeÃ§alhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:", "mtime": 1756147904.95932, "terms": ["def", "test_domain_models", "testa", "cria", "de", "modelos", "de", "dom", "nio", "from", "src", "domain", "models", "import", "transaction", "bankstatement", "analysisresult", "transactiontype", "transactioncategory", "testa", "cria", "de", "transa", "transaction", "transaction", "id", "str", "uuid", "uuid4", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "category", "transactioncategory", "salario", "assert", "transaction", "description", "test", "transaction", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "credit", "assert", "transaction", "category", "transactioncategory", "salario", "assert", "transaction", "is_income", "true", "assert", "transaction", "is_expense", "false", "testa", "cria", "de", "extrato", "banc", "rio", "statement", "bankstatement", "id", "str", "uuid", "uuid4", "bank_name", "banco", "teste", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "currency", "eur", "transactions", "transaction", "assert", "statement", "bank_name", "banco", "teste", "assert", "statement", "account_number", "assert", "statement", "currency", "eur", "assert", "len", "statement", "transactions", "testa", "lculo", "de", "totais", "assert", "statement", "total_income", "decimal", "assert", "statement", "total_expenses", "decimal", "def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror"]}
{"chunk_id": "f9c27f3c9de02eb7f345e3c5", "file_path": "tests/test_comprehensive_suite.py", "start_line": 59, "end_line": 138, "content": "def test_pdf_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detecÃ§Ã£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabeÃ§alhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(", "mtime": 1756147904.95932, "terms": ["def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction"]}
{"chunk_id": "0ef1c0606a6c3e333fac91d1", "file_path": "tests/test_comprehensive_suite.py", "start_line": 68, "end_line": 147, "content": "def test_excel_reader_currency_detection():\n    \"\"\"Testa a detecÃ§Ã£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabeÃ§alhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        ", "mtime": 1756147904.95932, "terms": ["def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement"]}
{"chunk_id": "ddaa0d350016ec99bf8d9f5b", "file_path": "tests/test_comprehensive_suite.py", "start_line": 69, "end_line": 148, "content": "    \"\"\"Testa a detecÃ§Ã£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabeÃ§alhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada", "mtime": 1756147904.95932, "terms": ["testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada"]}
{"chunk_id": "3aa60c4837681b9f1d74a237", "file_path": "tests/test_comprehensive_suite.py", "start_line": 82, "end_line": 161, "content": "def test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabeÃ§alhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'DescriÃ§Ã£o': ['SalÃ¡rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padrÃ£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")", "mtime": 1756147904.95932, "terms": ["def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator"]}
{"chunk_id": "0c8adb05cf0b75604f1398ae", "file_path": "tests/test_comprehensive_suite.py", "start_line": 104, "end_line": 180, "content": "def test_csv_reader_import():\n    \"\"\"Testa a importaÃ§Ã£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "130700e6b10ecdca30745e43", "file_path": "tests/test_comprehensive_suite.py", "start_line": 117, "end_line": 180, "content": "def test_categorizer_import():\n    \"\"\"Testa a importaÃ§Ã£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "cfc28fb494c7a57a87902b80", "file_path": "tests/test_comprehensive_suite.py", "start_line": 126, "end_line": 180, "content": "def test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "b0aae422c1ab3aad501a6533", "file_path": "tests/test_comprehensive_suite.py", "start_line": 137, "end_line": 180, "content": "            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "82dec4995d132c1275175fcc", "file_path": "tests/test_comprehensive_suite.py", "start_line": 154, "end_line": 180, "content": "def test_report_generator_import():\n    \"\"\"Testa a importaÃ§Ã£o do gerador de relatÃ³rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "4123e1acebdb3dc2cd1bd4f8", "file_path": "tests/test_comprehensive_suite.py", "start_line": 163, "end_line": 180, "content": "def test_use_case_import():\n    \"\"\"Testa a importaÃ§Ã£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "52f48a24027019e0b6494a5c", "file_path": "tests/test_comprehensive_suite.py", "start_line": 171, "end_line": 180, "content": "def test_main_import():\n    \"\"\"Testa a importaÃ§Ã£o do mÃ³dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"NÃ£o foi possÃ­vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "95bb5318c9cc3fc9387dc673", "file_path": "scripts/examine_excel.py", "start_line": 1, "end_line": 42, "content": "#!/usr/bin/env python3\n\"\"\"\nScript to examine the structure of the sample Excel file.\n\"\"\"\nimport pandas as pd\nfrom pathlib import Path\n\ndef examine_excel_file(file_path):\n    \"\"\"Examine the structure of an Excel file.\"\"\"\n    print(f\"Examining file: {file_path}\")\n    \n    # Read all sheets\n    excel_file = pd.ExcelFile(file_path)\n    print(f\"Sheet names: {excel_file.sheet_names}\")\n    \n    # Read each sheet\n    for sheet_name in excel_file.sheet_names:\n        print(f\"\\n--- Sheet: {sheet_name} ---\")\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        print(f\"Shape: {df.shape}\")\n        print(\"Columns:\")\n        for i, col in enumerate(df.columns):\n            print(f\"  {i+1}. {col}\")\n        print(\"\\nFirst 20 rows:\")\n        print(df.head(20))\n        print(\"\\nData types:\")\n        print(df.dtypes)\n        \n        # Check for non-empty rows\n        print(\"\\nNon-empty rows:\")\n        non_empty = df.dropna(how='all')\n        print(f\"Total non-empty rows: {len(non_empty)}\")\n        print(\"First 10 non-empty rows:\")\n        print(non_empty.head(10))\n\nif __name__ == \"__main__\":\n    # Path to the sample Excel file\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    if excel_path.exists():\n        examine_excel_file(excel_path)\n    else:\n        print(f\"File not found: {excel_path}\")", "mtime": 1755769048.2636738, "terms": ["usr", "bin", "env", "python3", "script", "to", "examine", "the", "structure", "of", "the", "sample", "excel", "file", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "def", "examine_excel_file", "file_path", "examine", "the", "structure", "of", "an", "excel", "file", "print", "examining", "file", "file_path", "read", "all", "sheets", "excel_file", "pd", "excelfile", "file_path", "print", "sheet", "names", "excel_file", "sheet_names", "read", "each", "sheet", "for", "sheet_name", "in", "excel_file", "sheet_names", "print", "sheet", "sheet_name", "df", "pd", "read_excel", "file_path", "sheet_name", "sheet_name", "print", "shape", "df", "shape", "print", "columns", "for", "col", "in", "enumerate", "df", "columns", "print", "col", "print", "nfirst", "rows", "print", "df", "head", "print", "ndata", "types", "print", "df", "dtypes", "check", "for", "non", "empty", "rows", "print", "nnon", "empty", "rows", "non_empty", "df", "dropna", "how", "all", "print", "total", "non", "empty", "rows", "len", "non_empty", "print", "first", "non", "empty", "rows", "print", "non_empty", "head", "if", "__name__", "__main__", "path", "to", "the", "sample", "excel", "file", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "excel_path", "exists", "examine_excel_file", "excel_path", "else", "print", "file", "not", "found", "excel_path"]}
{"chunk_id": "f44126c63e8c62c51ffe866e", "file_path": "scripts/examine_excel.py", "start_line": 8, "end_line": 42, "content": "def examine_excel_file(file_path):\n    \"\"\"Examine the structure of an Excel file.\"\"\"\n    print(f\"Examining file: {file_path}\")\n    \n    # Read all sheets\n    excel_file = pd.ExcelFile(file_path)\n    print(f\"Sheet names: {excel_file.sheet_names}\")\n    \n    # Read each sheet\n    for sheet_name in excel_file.sheet_names:\n        print(f\"\\n--- Sheet: {sheet_name} ---\")\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        print(f\"Shape: {df.shape}\")\n        print(\"Columns:\")\n        for i, col in enumerate(df.columns):\n            print(f\"  {i+1}. {col}\")\n        print(\"\\nFirst 20 rows:\")\n        print(df.head(20))\n        print(\"\\nData types:\")\n        print(df.dtypes)\n        \n        # Check for non-empty rows\n        print(\"\\nNon-empty rows:\")\n        non_empty = df.dropna(how='all')\n        print(f\"Total non-empty rows: {len(non_empty)}\")\n        print(\"First 10 non-empty rows:\")\n        print(non_empty.head(10))\n\nif __name__ == \"__main__\":\n    # Path to the sample Excel file\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    if excel_path.exists():\n        examine_excel_file(excel_path)\n    else:\n        print(f\"File not found: {excel_path}\")", "mtime": 1755769048.2636738, "terms": ["def", "examine_excel_file", "file_path", "examine", "the", "structure", "of", "an", "excel", "file", "print", "examining", "file", "file_path", "read", "all", "sheets", "excel_file", "pd", "excelfile", "file_path", "print", "sheet", "names", "excel_file", "sheet_names", "read", "each", "sheet", "for", "sheet_name", "in", "excel_file", "sheet_names", "print", "sheet", "sheet_name", "df", "pd", "read_excel", "file_path", "sheet_name", "sheet_name", "print", "shape", "df", "shape", "print", "columns", "for", "col", "in", "enumerate", "df", "columns", "print", "col", "print", "nfirst", "rows", "print", "df", "head", "print", "ndata", "types", "print", "df", "dtypes", "check", "for", "non", "empty", "rows", "print", "nnon", "empty", "rows", "non_empty", "df", "dropna", "how", "all", "print", "total", "non", "empty", "rows", "len", "non_empty", "print", "first", "non", "empty", "rows", "print", "non_empty", "head", "if", "__name__", "__main__", "path", "to", "the", "sample", "excel", "file", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "excel_path", "exists", "examine_excel_file", "excel_path", "else", "print", "file", "not", "found", "excel_path"]}
{"chunk_id": "1eb22b284c906636e2bc435d", "file_path": "scripts/test_excel_reader.py", "start_line": 1, "end_line": 62, "content": "#!/usr/bin/env python3\n\"\"\"\nScript de teste para o leitor de Excel.\n\"\"\"\nimport sys\nfrom pathlib import Path\n\n# Adiciona o diretÃ³rio raiz ao path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom src.infrastructure.readers.excel_reader import ExcelStatementReader\nfrom src.domain.models import TransactionType\n\ndef test_excel_reader():\n    \"\"\"Testa o leitor de Excel com o arquivo de exemplo.\"\"\"\n    reader = ExcelStatementReader()\n    \n    # Caminho para o arquivo de exemplo\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    \n    if not excel_path.exists():\n        print(f\"Arquivo nÃ£o encontrado: {excel_path}\")\n        return\n    \n    print(f\"Testando leitor de Excel com: {excel_path}\")\n    \n    # Verifica se pode ler o arquivo\n    if not reader.can_read(excel_path):\n        print(\"O leitor nÃ£o reconhece este arquivo como Excel\")\n        return\n    \n    try:\n        # LÃª o extrato\n        statement = reader.read(excel_path)\n        \n        print(f\"\\nExtrato lido com sucesso!\")\n        print(f\"Banco: {statement.bank_name}\")\n        print(f\"Conta: {statement.account_number}\")\n        print(f\"PerÃ­odo: {statement.period_start} a {statement.period_end}\")\n        print(f\"Saldo inicial: â‚¬ {statement.initial_balance}\")\n        print(f\"Saldo final: â‚¬ {statement.final_balance}\")\n        print(f\"Total de transaÃ§Ãµes: {len(statement.transactions)}\")\n        \n        print(f\"\\nPrimeiras 5 transaÃ§Ãµes:\")\n        for i, transaction in enumerate(statement.transactions[:5]):\n            print(f\"  {i+1}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}â‚¬ {transaction.amount}\")\n        \n        print(f\"\\nÃšltimas 5 transaÃ§Ãµes:\")\n        for i, transaction in enumerate(statement.transactions[-5:], len(statement.transactions)-4):\n            print(f\"  {i}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}â‚¬ {transaction.amount}\")\n                  \n    except Exception as e:\n        print(f\"Erro ao ler o extrato: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    test_excel_reader()", "mtime": 1755769411.9854424, "terms": ["usr", "bin", "env", "python3", "script", "de", "teste", "para", "leitor", "de", "excel", "import", "sys", "from", "pathlib", "import", "path", "adiciona", "diret", "rio", "raiz", "ao", "path", "sys", "path", "insert", "str", "path", "__file__", "parent", "parent", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "domain", "models", "import", "transactiontype", "def", "test_excel_reader", "testa", "leitor", "de", "excel", "com", "arquivo", "de", "exemplo", "reader", "excelstatementreader", "caminho", "para", "arquivo", "de", "exemplo", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "not", "excel_path", "exists", "print", "arquivo", "encontrado", "excel_path", "return", "print", "testando", "leitor", "de", "excel", "com", "excel_path", "verifica", "se", "pode", "ler", "arquivo", "if", "not", "reader", "can_read", "excel_path", "print", "leitor", "reconhece", "este", "arquivo", "como", "excel", "return", "try", "extrato", "statement", "reader", "read", "excel_path", "print", "nextrato", "lido", "com", "sucesso", "print", "banco", "statement", "bank_name", "print", "conta", "statement", "account_number", "print", "per", "odo", "statement", "period_start", "statement", "period_end", "print", "saldo", "inicial", "statement", "initial_balance", "print", "saldo", "final", "statement", "final_balance", "print", "total", "de", "transa", "es", "len", "statement", "transactions", "print", "nprimeiras", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "print", "ltimas", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "len", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "except", "exception", "as", "print", "erro", "ao", "ler", "extrato", "import", "traceback", "traceback", "print_exc", "if", "__name__", "__main__", "test_excel_reader"]}
{"chunk_id": "bba78b772bbd8feb95847921", "file_path": "scripts/test_excel_reader.py", "start_line": 14, "end_line": 62, "content": "def test_excel_reader():\n    \"\"\"Testa o leitor de Excel com o arquivo de exemplo.\"\"\"\n    reader = ExcelStatementReader()\n    \n    # Caminho para o arquivo de exemplo\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    \n    if not excel_path.exists():\n        print(f\"Arquivo nÃ£o encontrado: {excel_path}\")\n        return\n    \n    print(f\"Testando leitor de Excel com: {excel_path}\")\n    \n    # Verifica se pode ler o arquivo\n    if not reader.can_read(excel_path):\n        print(\"O leitor nÃ£o reconhece este arquivo como Excel\")\n        return\n    \n    try:\n        # LÃª o extrato\n        statement = reader.read(excel_path)\n        \n        print(f\"\\nExtrato lido com sucesso!\")\n        print(f\"Banco: {statement.bank_name}\")\n        print(f\"Conta: {statement.account_number}\")\n        print(f\"PerÃ­odo: {statement.period_start} a {statement.period_end}\")\n        print(f\"Saldo inicial: â‚¬ {statement.initial_balance}\")\n        print(f\"Saldo final: â‚¬ {statement.final_balance}\")\n        print(f\"Total de transaÃ§Ãµes: {len(statement.transactions)}\")\n        \n        print(f\"\\nPrimeiras 5 transaÃ§Ãµes:\")\n        for i, transaction in enumerate(statement.transactions[:5]):\n            print(f\"  {i+1}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}â‚¬ {transaction.amount}\")\n        \n        print(f\"\\nÃšltimas 5 transaÃ§Ãµes:\")\n        for i, transaction in enumerate(statement.transactions[-5:], len(statement.transactions)-4):\n            print(f\"  {i}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}â‚¬ {transaction.amount}\")\n                  \n    except Exception as e:\n        print(f\"Erro ao ler o extrato: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    test_excel_reader()", "mtime": 1755769411.9854424, "terms": ["def", "test_excel_reader", "testa", "leitor", "de", "excel", "com", "arquivo", "de", "exemplo", "reader", "excelstatementreader", "caminho", "para", "arquivo", "de", "exemplo", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "not", "excel_path", "exists", "print", "arquivo", "encontrado", "excel_path", "return", "print", "testando", "leitor", "de", "excel", "com", "excel_path", "verifica", "se", "pode", "ler", "arquivo", "if", "not", "reader", "can_read", "excel_path", "print", "leitor", "reconhece", "este", "arquivo", "como", "excel", "return", "try", "extrato", "statement", "reader", "read", "excel_path", "print", "nextrato", "lido", "com", "sucesso", "print", "banco", "statement", "bank_name", "print", "conta", "statement", "account_number", "print", "per", "odo", "statement", "period_start", "statement", "period_end", "print", "saldo", "inicial", "statement", "initial_balance", "print", "saldo", "final", "statement", "final_balance", "print", "total", "de", "transa", "es", "len", "statement", "transactions", "print", "nprimeiras", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "print", "ltimas", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "len", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "except", "exception", "as", "print", "erro", "ao", "ler", "extrato", "import", "traceback", "traceback", "print_exc", "if", "__name__", "__main__", "test_excel_reader"]}
{"chunk_id": "d177fd23d02a98ea16659b99", "file_path": "scripts/create_sample_pdf.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para criar um PDF de extrato bancÃ¡rio de exemplo para testes.\n\"\"\"\n\nfrom reportlab.lib.pagesizes import A4\nfrom reportlab.lib import colors\nfrom reportlab.lib.units import mm\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.enums import TA_CENTER, TA_RIGHT\nimport os\nfrom datetime import datetime, timedelta\n\ndef create_sample_statement():\n    \"\"\"Cria um extrato bancÃ¡rio de exemplo em PDF.\"\"\"\n    \n    # Criar diretÃ³rio se nÃ£o existir\n    os.makedirs(\"data/samples\", exist_ok=True)\n    \n    # Nome do arquivo\n    filename = \"data/samples/extrato_exemplo.pdf\"\n    \n    # Criar documento\n    doc = SimpleDocTemplate(filename, pagesize=A4)\n    story = []\n    \n    # Estilos\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(\n        'CustomTitle',\n        parent=styles['Heading1'],\n        fontSize=16,\n        textColor=colors.HexColor('#003366'),\n        alignment=TA_CENTER\n    )\n    \n    # CabeÃ§alho\n    story.append(Paragraph(\"BANCO EXEMPLO S.A.\", title_style))\n    story.append(Spacer(1, 12))\n    story.append(Paragraph(\"Extrato de Conta Corrente\", styles['Heading2']))\n    story.append(Spacer(1, 12))\n    \n    # InformaÃ§Ãµes da conta\n    info_data = [\n        [\"AgÃªncia:\", \"1234\", \"Conta:\", \"56789-0\"],\n        [\"Cliente:\", \"JoÃ£o da Silva\", \"CPF:\", \"123.456.789-00\"],\n        [\"PerÃ­odo:\", \"01/08/2025 a 31/08/2025\", \"\", \"\"]\n    ]\n    \n    info_table = Table(info_data, colWidths=[60, 120, 60, 120])\n    info_table.setStyle(TableStyle([\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),\n    ]))\n    \n    story.append(info_table)\n    story.append(Spacer(1, 20))\n    \n    # Saldo anterior\n    story.append(Paragraph(\"Saldo Anterior (31/07/2025): â‚¬ 2.500,00\", styles['Normal']))\n    story.append(Spacer(1, 12))\n\n    # TransaÃ§Ãµes\n    transactions = [\n        [\"Data\", \"DescriÃ§Ã£o\", \"Valor\", \"Saldo\"],\n        [\"01/08\", \"SALARIO EMPRESA XYZ\", \"â‚¬ 5.000,00 C\", \"â‚¬ 7.500,00\"],\n        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"â‚¬ 150,00 D\", \"â‚¬ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"â‚¬ 287,45 D\", \"â‚¬ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"â‚¬ 89,90 D\", \"â‚¬ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"â‚¬ 23,50 D\", \"â‚¬ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"â‚¬ 156,00 D\", \"â‚¬ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"â‚¬ 234,78 D\", \"â‚¬ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"â‚¬ 39,90 D\", \"â‚¬ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"â‚¬ 500,00 C\", \"â‚¬ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"â‚¬ 200,00 D\", \"â‚¬ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"â‚¬ 120,00 D\", \"â‚¬ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"â‚¬ 80,00 C\", \"â‚¬ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"â‚¬ 450,00 D\", \"â‚¬ 6.328,47\"],", "mtime": 1755109561.8157306, "terms": ["usr", "bin", "env", "python3", "script", "para", "criar", "um", "pdf", "de", "extrato", "banc", "rio", "de", "exemplo", "para", "testes", "from", "reportlab", "lib", "pagesizes", "import", "a4", "from", "reportlab", "lib", "import", "colors", "from", "reportlab", "lib", "units", "import", "mm", "from", "reportlab", "platypus", "import", "simpledoctemplate", "table", "tablestyle", "paragraph", "spacer", "from", "reportlab", "lib", "styles", "import", "getsamplestylesheet", "paragraphstyle", "from", "reportlab", "lib", "enums", "import", "ta_center", "ta_right", "import", "os", "from", "datetime", "import", "datetime", "timedelta", "def", "create_sample_statement", "cria", "um", "extrato", "banc", "rio", "de", "exemplo", "em", "pdf", "criar", "diret", "rio", "se", "existir", "os", "makedirs", "data", "samples", "exist_ok", "true", "nome", "do", "arquivo", "filename", "data", "samples", "extrato_exemplo", "pdf", "criar", "documento", "doc", "simpledoctemplate", "filename", "pagesize", "a4", "story", "estilos", "styles", "getsamplestylesheet", "title_style", "paragraphstyle", "customtitle", "parent", "styles", "heading1", "fontsize", "textcolor", "colors", "hexcolor", "alignment", "ta_center", "cabe", "alho", "story", "append", "paragraph", "banco", "exemplo", "title_style", "story", "append", "spacer", "story", "append", "paragraph", "extrato", "de", "conta", "corrente", "styles", "heading2", "story", "append", "spacer", "informa", "es", "da", "conta", "info_data", "ag", "ncia", "conta", "cliente", "jo", "da", "silva", "cpf", "per", "odo", "info_table", "table", "info_data", "colwidths", "info_table", "setstyle", "tablestyle", "fontsize", "fontname", "helvetica", "bold", "fontname", "helvetica", "bold", "story", "append", "info_table", "story", "append", "spacer", "saldo", "anterior", "story", "append", "paragraph", "saldo", "anterior", "styles", "normal", "story", "append", "spacer", "transa", "es", "transactions", "data", "descri", "valor", "saldo", "salario", "empresa", "xyz", "pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center"]}
{"chunk_id": "76922712166a514f264e02fe", "file_path": "scripts/create_sample_pdf.py", "start_line": 15, "end_line": 94, "content": "def create_sample_statement():\n    \"\"\"Cria um extrato bancÃ¡rio de exemplo em PDF.\"\"\"\n    \n    # Criar diretÃ³rio se nÃ£o existir\n    os.makedirs(\"data/samples\", exist_ok=True)\n    \n    # Nome do arquivo\n    filename = \"data/samples/extrato_exemplo.pdf\"\n    \n    # Criar documento\n    doc = SimpleDocTemplate(filename, pagesize=A4)\n    story = []\n    \n    # Estilos\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(\n        'CustomTitle',\n        parent=styles['Heading1'],\n        fontSize=16,\n        textColor=colors.HexColor('#003366'),\n        alignment=TA_CENTER\n    )\n    \n    # CabeÃ§alho\n    story.append(Paragraph(\"BANCO EXEMPLO S.A.\", title_style))\n    story.append(Spacer(1, 12))\n    story.append(Paragraph(\"Extrato de Conta Corrente\", styles['Heading2']))\n    story.append(Spacer(1, 12))\n    \n    # InformaÃ§Ãµes da conta\n    info_data = [\n        [\"AgÃªncia:\", \"1234\", \"Conta:\", \"56789-0\"],\n        [\"Cliente:\", \"JoÃ£o da Silva\", \"CPF:\", \"123.456.789-00\"],\n        [\"PerÃ­odo:\", \"01/08/2025 a 31/08/2025\", \"\", \"\"]\n    ]\n    \n    info_table = Table(info_data, colWidths=[60, 120, 60, 120])\n    info_table.setStyle(TableStyle([\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),\n    ]))\n    \n    story.append(info_table)\n    story.append(Spacer(1, 20))\n    \n    # Saldo anterior\n    story.append(Paragraph(\"Saldo Anterior (31/07/2025): â‚¬ 2.500,00\", styles['Normal']))\n    story.append(Spacer(1, 12))\n\n    # TransaÃ§Ãµes\n    transactions = [\n        [\"Data\", \"DescriÃ§Ã£o\", \"Valor\", \"Saldo\"],\n        [\"01/08\", \"SALARIO EMPRESA XYZ\", \"â‚¬ 5.000,00 C\", \"â‚¬ 7.500,00\"],\n        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"â‚¬ 150,00 D\", \"â‚¬ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"â‚¬ 287,45 D\", \"â‚¬ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"â‚¬ 89,90 D\", \"â‚¬ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"â‚¬ 23,50 D\", \"â‚¬ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"â‚¬ 156,00 D\", \"â‚¬ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"â‚¬ 234,78 D\", \"â‚¬ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"â‚¬ 39,90 D\", \"â‚¬ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"â‚¬ 500,00 C\", \"â‚¬ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"â‚¬ 200,00 D\", \"â‚¬ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"â‚¬ 120,00 D\", \"â‚¬ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"â‚¬ 80,00 C\", \"â‚¬ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"â‚¬ 450,00 D\", \"â‚¬ 6.328,47\"],\n        [\"28/08\", \"IFOOD\", \"â‚¬ 67,80 D\", \"â‚¬ 6.260,67\"],\n        [\"30/08\", \"RENDIMENTO POUPANCA\", \"â‚¬ 15,43 C\", \"â‚¬ 6.276,10\"],\n    ]\n\n    # Criar tabela de transaÃ§Ãµes\n    trans_table = Table(transactions, colWidths=[50, 200, 80, 80])\n    trans_table.setStyle(TableStyle([\n        # CabeÃ§alho\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 10),\n        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n", "mtime": 1755109561.8157306, "terms": ["def", "create_sample_statement", "cria", "um", "extrato", "banc", "rio", "de", "exemplo", "em", "pdf", "criar", "diret", "rio", "se", "existir", "os", "makedirs", "data", "samples", "exist_ok", "true", "nome", "do", "arquivo", "filename", "data", "samples", "extrato_exemplo", "pdf", "criar", "documento", "doc", "simpledoctemplate", "filename", "pagesize", "a4", "story", "estilos", "styles", "getsamplestylesheet", "title_style", "paragraphstyle", "customtitle", "parent", "styles", "heading1", "fontsize", "textcolor", "colors", "hexcolor", "alignment", "ta_center", "cabe", "alho", "story", "append", "paragraph", "banco", "exemplo", "title_style", "story", "append", "spacer", "story", "append", "paragraph", "extrato", "de", "conta", "corrente", "styles", "heading2", "story", "append", "spacer", "informa", "es", "da", "conta", "info_data", "ag", "ncia", "conta", "cliente", "jo", "da", "silva", "cpf", "per", "odo", "info_table", "table", "info_data", "colwidths", "info_table", "setstyle", "tablestyle", "fontsize", "fontname", "helvetica", "bold", "fontname", "helvetica", "bold", "story", "append", "info_table", "story", "append", "spacer", "saldo", "anterior", "story", "append", "paragraph", "saldo", "anterior", "styles", "normal", "story", "append", "spacer", "transa", "es", "transactions", "data", "descri", "valor", "saldo", "salario", "empresa", "xyz", "pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center", "ifood", "rendimento", "poupanca", "criar", "tabela", "de", "transa", "es", "trans_table", "table", "transactions", "colwidths", "trans_table", "setstyle", "tablestyle", "cabe", "alho", "background", "colors", "grey", "textcolor", "colors", "whitesmoke", "fontname", "helvetica", "bold", "fontsize", "align", "center"]}
{"chunk_id": "f2000ae5c76f554f290afaa5", "file_path": "scripts/create_sample_pdf.py", "start_line": 69, "end_line": 136, "content": "        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"â‚¬ 150,00 D\", \"â‚¬ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"â‚¬ 287,45 D\", \"â‚¬ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"â‚¬ 89,90 D\", \"â‚¬ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"â‚¬ 23,50 D\", \"â‚¬ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"â‚¬ 156,00 D\", \"â‚¬ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"â‚¬ 234,78 D\", \"â‚¬ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"â‚¬ 39,90 D\", \"â‚¬ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"â‚¬ 500,00 C\", \"â‚¬ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"â‚¬ 200,00 D\", \"â‚¬ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"â‚¬ 120,00 D\", \"â‚¬ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"â‚¬ 80,00 C\", \"â‚¬ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"â‚¬ 450,00 D\", \"â‚¬ 6.328,47\"],\n        [\"28/08\", \"IFOOD\", \"â‚¬ 67,80 D\", \"â‚¬ 6.260,67\"],\n        [\"30/08\", \"RENDIMENTO POUPANCA\", \"â‚¬ 15,43 C\", \"â‚¬ 6.276,10\"],\n    ]\n\n    # Criar tabela de transaÃ§Ãµes\n    trans_table = Table(transactions, colWidths=[50, 200, 80, 80])\n    trans_table.setStyle(TableStyle([\n        # CabeÃ§alho\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 10),\n        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n\n        # Corpo\n        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n        ('FONTSIZE', (0, 1), (-1, -1), 9),\n        ('ALIGN', (0, 1), (0, -1), 'CENTER'),\n        ('ALIGN', (2, 1), (-1, -1), 'RIGHT'),\n\n        # Linhas alternadas\n        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n\n        # Bordas\n        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n    ]))\n\n    story.append(trans_table)\n    story.append(Spacer(1, 20))\n\n    # Resumo\n    story.append(Paragraph(\"RESUMO DO PERÃODO\", styles['Heading3']))\n    story.append(Spacer(1, 12))\n\n    summary_data = [\n        [\"Total de CrÃ©ditos:\", \"â‚¬ 5.595,43\"],\n        [\"Total de DÃ©bitos:\", \"â‚¬ 1.819,33\"],\n        [\"Saldo Final:\", \"â‚¬ 6.276,10\"]\n    ]\n\n    summary_table = Table(summary_data, colWidths=[150, 100])\n    summary_table.setStyle(TableStyle([\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('ALIGN', (1, 0), (1, -1), 'RIGHT'),\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n    ]))\n\n    story.append(summary_table)\n\n    # Gerar PDF\n    doc.build(story)\n    print(f\"âœ… PDF de exemplo criado: {filename}\")\n    return filename\n\nif __name__ == \"__main__\":\n    create_sample_statement()", "mtime": 1755109561.8157306, "terms": ["pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center", "ifood", "rendimento", "poupanca", "criar", "tabela", "de", "transa", "es", "trans_table", "table", "transactions", "colwidths", "trans_table", "setstyle", "tablestyle", "cabe", "alho", "background", "colors", "grey", "textcolor", "colors", "whitesmoke", "fontname", "helvetica", "bold", "fontsize", "align", "center", "corpo", "fontname", "helvetica", "fontsize", "align", "center", "align", "right", "linhas", "alternadas", "rowbackgrounds", "colors", "white", "colors", "lightgrey", "bordas", "grid", "colors", "black", "story", "append", "trans_table", "story", "append", "spacer", "resumo", "story", "append", "paragraph", "resumo", "do", "per", "odo", "styles", "heading3", "story", "append", "spacer", "summary_data", "total", "de", "cr", "ditos", "total", "de", "bitos", "saldo", "final", "summary_table", "table", "summary_data", "colwidths", "summary_table", "setstyle", "tablestyle", "fontname", "helvetica", "bold", "align", "right", "fontsize", "story", "append", "summary_table", "gerar", "pdf", "doc", "build", "story", "print", "pdf", "de", "exemplo", "criado", "filename", "return", "filename", "if", "__name__", "__main__", "create_sample_statement"]}
{"chunk_id": "0e6c51198d21e942dae55042", "file_path": "src/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote principal src", "mtime": 1755104706.859261, "terms": ["pacote", "principal", "src"]}
{"chunk_id": "85a385165191d692de3ad527", "file_path": "src/utils/currency_utils.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nUtilitÃ¡rios para formataÃ§Ã£o e detecÃ§Ã£o de moedas.\n\"\"\"\nimport re\nfrom typing import Tuple, Optional\n\n\nclass CurrencyUtils:\n    \"\"\"UtilitÃ¡rios para trabalhar com moedas.\"\"\"\n    \n    # Mapeamento de sÃ­mbolos de moeda\n    CURRENCY_SYMBOLS = {\n        'EUR': 'â‚¬',\n        'USD': '$',\n        'BRL': 'R$',\n        'GBP': 'Â£',\n        'JPY': 'Â¥',\n        'CHF': 'CHF',\n        'CAD': 'C$',\n        'AUD': 'A$'\n    }\n    \n    # PadrÃµes para detectar moedas em texto\n    CURRENCY_PATTERNS = [\n        (r'â‚¬\\s*[\\d.,]+|[\\d.,]+\\s*â‚¬', 'EUR'),\n        (r'R\\$\\s*[\\d.,]+|[\\d.,]+\\s*R\\$', 'BRL'),\n        (r'\\$\\s*[\\d.,]+|[\\d.,]+\\s*\\$', 'USD'),\n        (r'Â£\\s*[\\d.,]+|[\\d.,]+\\s*Â£', 'GBP'),\n        (r'Â¥\\s*[\\d.,]+|[\\d.,]+\\s*Â¥', 'JPY'),\n        (r'CHF\\s*[\\d.,]+|[\\d.,]+\\s*CHF', 'CHF'),\n    ]\n    \n    @classmethod\n    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padrÃ£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padrÃµes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por cÃ³digos de moeda explÃ­citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # PadrÃ£o para Europa (assume EUR se nÃ£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o sÃ­mbolo da moeda.\n        \n        Args:\n            currency_code: CÃ³digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            SÃ­mbolo da moeda (ex: 'â‚¬', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        ", "mtime": 1756149301.0570138, "terms": ["utilit", "rios", "para", "formata", "detec", "de", "moedas", "import", "re", "from", "typing", "import", "tuple", "optional", "class", "currencyutils", "utilit", "rios", "para", "trabalhar", "com", "moedas", "mapeamento", "de", "mbolos", "de", "moeda", "currency_symbols", "eur", "usd", "brl", "gbp", "jpy", "chf", "chf", "cad", "aud", "padr", "es", "para", "detectar", "moedas", "em", "texto", "currency_patterns", "eur", "brl", "usd", "gbp", "jpy", "chf", "chf", "chf", "classmethod", "def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada"]}
{"chunk_id": "4960b6392ed42f335309c6c7", "file_path": "src/utils/currency_utils.py", "start_line": 8, "end_line": 87, "content": "class CurrencyUtils:\n    \"\"\"UtilitÃ¡rios para trabalhar com moedas.\"\"\"\n    \n    # Mapeamento de sÃ­mbolos de moeda\n    CURRENCY_SYMBOLS = {\n        'EUR': 'â‚¬',\n        'USD': '$',\n        'BRL': 'R$',\n        'GBP': 'Â£',\n        'JPY': 'Â¥',\n        'CHF': 'CHF',\n        'CAD': 'C$',\n        'AUD': 'A$'\n    }\n    \n    # PadrÃµes para detectar moedas em texto\n    CURRENCY_PATTERNS = [\n        (r'â‚¬\\s*[\\d.,]+|[\\d.,]+\\s*â‚¬', 'EUR'),\n        (r'R\\$\\s*[\\d.,]+|[\\d.,]+\\s*R\\$', 'BRL'),\n        (r'\\$\\s*[\\d.,]+|[\\d.,]+\\s*\\$', 'USD'),\n        (r'Â£\\s*[\\d.,]+|[\\d.,]+\\s*Â£', 'GBP'),\n        (r'Â¥\\s*[\\d.,]+|[\\d.,]+\\s*Â¥', 'JPY'),\n        (r'CHF\\s*[\\d.,]+|[\\d.,]+\\s*CHF', 'CHF'),\n    ]\n    \n    @classmethod\n    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padrÃ£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padrÃµes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por cÃ³digos de moeda explÃ­citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # PadrÃ£o para Europa (assume EUR se nÃ£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o sÃ­mbolo da moeda.\n        \n        Args:\n            currency_code: CÃ³digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            SÃ­mbolo da moeda (ex: 'â‚¬', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: CÃ³digo da moeda\n            \n        Returns:\n            Valor formatado com sÃ­mbolo da moeda\n        \"\"\"", "mtime": 1756149301.0570138, "terms": ["class", "currencyutils", "utilit", "rios", "para", "trabalhar", "com", "moedas", "mapeamento", "de", "mbolos", "de", "moeda", "currency_symbols", "eur", "usd", "brl", "gbp", "jpy", "chf", "chf", "cad", "aud", "padr", "es", "para", "detectar", "moedas", "em", "texto", "currency_patterns", "eur", "brl", "usd", "gbp", "jpy", "chf", "chf", "chf", "classmethod", "def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda"]}
{"chunk_id": "329b61acf0417825d1d53afb", "file_path": "src/utils/currency_utils.py", "start_line": 34, "end_line": 113, "content": "    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padrÃ£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padrÃµes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por cÃ³digos de moeda explÃ­citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # PadrÃ£o para Europa (assume EUR se nÃ£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o sÃ­mbolo da moeda.\n        \n        Args:\n            currency_code: CÃ³digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            SÃ­mbolo da moeda (ex: 'â‚¬', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: CÃ³digo da moeda\n            \n        Returns:\n            Valor formatado com sÃ­mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # FormataÃ§Ã£o especÃ­fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: â‚¬ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padrÃ£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda detectada ou None se nÃ£o detectada\n        \"\"\"\n        import pandas as pd\n", "mtime": 1756149301.0570138, "terms": ["def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd"]}
{"chunk_id": "c311f58de24690a62a8e2ff2", "file_path": "src/utils/currency_utils.py", "start_line": 64, "end_line": 133, "content": "    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o sÃ­mbolo da moeda.\n        \n        Args:\n            currency_code: CÃ³digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            SÃ­mbolo da moeda (ex: 'â‚¬', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: CÃ³digo da moeda\n            \n        Returns:\n            Valor formatado com sÃ­mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # FormataÃ§Ã£o especÃ­fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: â‚¬ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padrÃ£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda detectada ou None se nÃ£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padrÃµes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padrÃ£o para aceitar mesmo que nÃ£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada nÃ£o estiver explicitamente no texto,\n        # exceto para moedas padrÃ£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "b13538e84a281e5000cb329f", "file_path": "src/utils/currency_utils.py", "start_line": 69, "end_line": 133, "content": "            currency_code: CÃ³digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            SÃ­mbolo da moeda (ex: 'â‚¬', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: CÃ³digo da moeda\n            \n        Returns:\n            Valor formatado com sÃ­mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # FormataÃ§Ã£o especÃ­fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: â‚¬ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padrÃ£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda detectada ou None se nÃ£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padrÃµes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padrÃ£o para aceitar mesmo que nÃ£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada nÃ£o estiver explicitamente no texto,\n        # exceto para moedas padrÃ£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "a5393e059676deb20cbe8646", "file_path": "src/utils/currency_utils.py", "start_line": 77, "end_line": 133, "content": "    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monetÃ¡rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: CÃ³digo da moeda\n            \n        Returns:\n            Valor formatado com sÃ­mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # FormataÃ§Ã£o especÃ­fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: â‚¬ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padrÃ£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda detectada ou None se nÃ£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padrÃµes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padrÃ£o para aceitar mesmo que nÃ£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada nÃ£o estiver explicitamente no texto,\n        # exceto para moedas padrÃ£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "a16b61e1fb968720c77218aa", "file_path": "src/utils/currency_utils.py", "start_line": 102, "end_line": 133, "content": "    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para anÃ¡lise\n\n        Returns:\n            CÃ³digo da moeda detectada ou None se nÃ£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padrÃµes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padrÃ£o para aceitar mesmo que nÃ£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada nÃ£o estiver explicitamente no texto,\n        # exceto para moedas padrÃ£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "30cd15052504f70af79b7ded", "file_path": "src/application/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote application", "mtime": 1755104719.7803147, "terms": ["pacote", "application"]}
{"chunk_id": "1b64a54e6aca1f352194731f", "file_path": "src/application/use_cases.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nCasos de uso para anÃ¡lise de extratos.\n\"\"\"\nfrom pathlib import Path\nfrom typing import Optional, List\n\nfrom src.domain.interfaces import (\n    StatementReader,\n    TransactionCategorizer,\n    StatementAnalyzer,\n    ReportGenerator,\n)\nfrom src.domain.models import BankStatement, AnalysisResult\n\n\nclass AnalyzeStatementUseCase:\n    \"\"\"Caso de uso para analisar extratos bancÃ¡rios.\"\"\"\n\n    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo nÃ£o encontrado: {file_path}\")\n\n        # LÃª o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transaÃ§Ãµes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relatÃ³rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis", "mtime": 1756146923.2629967, "terms": ["casos", "de", "uso", "para", "an", "lise", "de", "extratos", "from", "pathlib", "import", "path", "from", "typing", "import", "optional", "list", "from", "src", "domain", "interfaces", "import", "statementreader", "transactioncategorizer", "statementanalyzer", "reportgenerator", "from", "src", "domain", "models", "import", "bankstatement", "analysisresult", "class", "analyzestatementusecase", "caso", "de", "uso", "para", "analisar", "extratos", "banc", "rios", "def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis"]}
{"chunk_id": "99db0101b0636bae70c43212", "file_path": "src/application/use_cases.py", "start_line": 16, "end_line": 95, "content": "class AnalyzeStatementUseCase:\n    \"\"\"Caso de uso para analisar extratos bancÃ¡rios.\"\"\"\n\n    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo nÃ£o encontrado: {file_path}\")\n\n        # LÃª o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transaÃ§Ãµes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relatÃ³rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:", "mtime": 1756146923.2629967, "terms": ["class", "analyzestatementusecase", "caso", "de", "uso", "para", "analisar", "extratos", "banc", "rios", "def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader"]}
{"chunk_id": "b6da7a8645e9a9f291aa3a19", "file_path": "src/application/use_cases.py", "start_line": 19, "end_line": 98, "content": "    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo nÃ£o encontrado: {file_path}\")\n\n        # LÃª o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transaÃ§Ãµes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relatÃ³rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:", "mtime": 1756146923.2629967, "terms": ["def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers"]}
{"chunk_id": "a4911f621ec84b149fd30b1b", "file_path": "src/application/use_cases.py", "start_line": 31, "end_line": 110, "content": "    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo nÃ£o encontrado: {file_path}\")\n\n        # LÃª o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transaÃ§Ãµes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relatÃ³rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor disponÃ­vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        ", "mtime": 1756146923.2629967, "terms": ["def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path"]}
{"chunk_id": "94a549f7038eb288649ea5dd", "file_path": "src/application/use_cases.py", "start_line": 60, "end_line": 120, "content": "class ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor disponÃ­vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "4c9fac649e2506b50895e22f", "file_path": "src/application/use_cases.py", "start_line": 63, "end_line": 120, "content": "    def __init__(self):\n        # Importa implementaÃ§Ãµes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor disponÃ­vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "76892954e39a296c3d3b54c3", "file_path": "src/application/use_cases.py", "start_line": 69, "end_line": 120, "content": "        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores disponÃ­veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (serÃ¡ substituÃ­do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor disponÃ­vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "77160899a6c3f34bc34d18f7", "file_path": "src/application/use_cases.py", "start_line": 95, "end_line": 120, "content": "    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor disponÃ­vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "46af600437a974066ebb4db9", "file_path": "src/application/use_cases.py", "start_line": 103, "end_line": 120, "content": "    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "6682b38dc6fe6de7bfaad305", "file_path": "src/application/use_cases.py", "start_line": 117, "end_line": 120, "content": "    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "2c1d39ced02fbd133ea9c56d", "file_path": "src/infrastructure/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote infrastructure", "mtime": 1755104726.021574, "terms": ["pacote", "infrastructure"]}
{"chunk_id": "5f29afe18eda4048261c4cad", "file_path": "src/domain/interfaces.py", "start_line": 1, "end_line": 58, "content": "\"\"\"\nInterfaces (protocolos) do domÃ­nio.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom pathlib import Path\n\nfrom src.domain.models import BankStatement, Transaction, AnalysisResult\n\n\nclass StatementReader(ABC):\n    \"\"\"Interface para leitores de extratos.\"\"\"\n    \n    @abstractmethod\n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["interfaces", "protocolos", "do", "dom", "nio", "from", "abc", "import", "abc", "abstractmethod", "from", "typing", "import", "list", "optional", "from", "pathlib", "import", "path", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "analysisresult", "class", "statementreader", "abc", "interface", "para", "leitores", "de", "extratos", "abstractmethod", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "3b7d6623996c3a6ea56a0f36", "file_path": "src/domain/interfaces.py", "start_line": 11, "end_line": 58, "content": "class StatementReader(ABC):\n    \"\"\"Interface para leitores de extratos.\"\"\"\n    \n    @abstractmethod\n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "statementreader", "abc", "interface", "para", "leitores", "de", "extratos", "abstractmethod", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "b53faf4f2b52f07b79c11190", "file_path": "src/domain/interfaces.py", "start_line": 15, "end_line": 58, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "69afbfd16f680e80f685263b", "file_path": "src/domain/interfaces.py", "start_line": 20, "end_line": 58, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "b2e5315ad43533bfa5a2f5e3", "file_path": "src/domain/interfaces.py", "start_line": 25, "end_line": 58, "content": "class TransactionParser(ABC):\n    \"\"\"Interface para parsers de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "2a186bf08e675217021f5071", "file_path": "src/domain/interfaces.py", "start_line": 29, "end_line": 58, "content": "    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transaÃ§Ãµes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "5cb39268e695e1f658ad9852", "file_path": "src/domain/interfaces.py", "start_line": 34, "end_line": 58, "content": "class StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "c51894f929b875be38828537", "file_path": "src/domain/interfaces.py", "start_line": 38, "end_line": 58, "content": "    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "3fdd202cd6c9a81a48d6be9e", "file_path": "src/domain/interfaces.py", "start_line": 43, "end_line": 58, "content": "class ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relatÃ³rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "7b4d145a76a7bfdc2a0b870f", "file_path": "src/domain/interfaces.py", "start_line": 47, "end_line": 58, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio a partir da anÃ¡lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "cfafa4f4c765694c7a3fc2b3", "file_path": "src/domain/interfaces.py", "start_line": 52, "end_line": 58, "content": "class TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transaÃ§Ãµes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "825471932a7d64834b4951a1", "file_path": "src/domain/interfaces.py", "start_line": 56, "end_line": 58, "content": "    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "a727aae8ce334c5ad59fe878", "file_path": "src/domain/models.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nModelos de domÃ­nio para o sistema de anÃ¡lise de extratos bancÃ¡rios.\n\"\"\"\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Optional, List\nfrom uuid import uuid4\n\n\nclass TransactionType(Enum):\n    \"\"\"Tipos de transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    DEBIT = \"DEBIT\"\n    CREDIT = \"CREDIT\"\n    \n    \nclass TransactionCategory(Enum):\n    \"\"\"Categorias de transaÃ§Ã£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"", "mtime": 1756145254.817271, "terms": ["modelos", "de", "dom", "nio", "para", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "from", "dataclasses", "import", "dataclass", "field", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "enum", "import", "enum", "from", "typing", "import", "optional", "list", "from", "uuid", "import", "uuid4", "class", "transactiontype", "enum", "tipos", "de", "transa", "banc", "ria", "debit", "debit", "credit", "credit", "class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa"]}
{"chunk_id": "02eb22cdfe035051b5dc3029", "file_path": "src/domain/models.py", "start_line": 12, "end_line": 91, "content": "class TransactionType(Enum):\n    \"\"\"Tipos de transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    DEBIT = \"DEBIT\"\n    CREDIT = \"CREDIT\"\n    \n    \nclass TransactionCategory(Enum):\n    \"\"\"Categorias de transaÃ§Ã£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income", "mtime": 1756145254.817271, "terms": ["class", "transactiontype", "enum", "tipos", "de", "transa", "banc", "ria", "debit", "debit", "credit", "credit", "class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income"]}
{"chunk_id": "3af765cb053a3e9db2614f51", "file_path": "src/domain/models.py", "start_line": 18, "end_line": 97, "content": "class TransactionCategory(Enum):\n    \"\"\"Categorias de transaÃ§Ã£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(", "mtime": 1756145254.817271, "terms": ["class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum"]}
{"chunk_id": "b642a92992702b39d7980892", "file_path": "src/domain/models.py", "start_line": 36, "end_line": 115, "content": "class Transaction:\n    \"\"\"Representa uma transaÃ§Ã£o bancÃ¡ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        ", "mtime": 1756145254.817271, "terms": ["class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction"]}
{"chunk_id": "6e170b982f2d8d015c479111", "file_path": "src/domain/models.py", "start_line": 47, "end_line": 126, "content": "    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n", "mtime": 1756145254.817271, "terms": ["def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end"]}
{"chunk_id": "6312bb58d1eded2d6e03e429", "file_path": "src/domain/models.py", "start_line": 55, "end_line": 134, "content": "    def is_income(self) -> bool:\n        \"\"\"Verifica se Ã© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal", "mtime": 1756145254.817271, "terms": ["def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal"]}
{"chunk_id": "4f2b5dae3e11597ce15c7ddf", "file_path": "src/domain/models.py", "start_line": 60, "end_line": 139, "content": "    def is_expense(self) -> bool:\n        \"\"\"Verifica se Ã© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)", "mtime": 1756145254.817271, "terms": ["def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list"]}
{"chunk_id": "f1c218e95d647a7f0c2eb75c", "file_path": "src/domain/models.py", "start_line": 66, "end_line": 140, "content": "class BankStatement:\n    \"\"\"Representa um extrato bancÃ¡rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "6917e30bb231b395bc1da3e3", "file_path": "src/domain/models.py", "start_line": 69, "end_line": 140, "content": "    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padrÃ£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "002624d9b348343e8dea7742", "file_path": "src/domain/models.py", "start_line": 79, "end_line": 140, "content": "    def __post_init__(self):\n        \"\"\"ValidaÃ§Ãµes apÃ³s inicializaÃ§Ã£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "cd17cc71ab4118d64cb3bcd0", "file_path": "src/domain/models.py", "start_line": 87, "end_line": 140, "content": "    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "7e51bd1f1610dd3a6c12d0b3", "file_path": "src/domain/models.py", "start_line": 95, "end_line": 140, "content": "    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "a638b425f6b5b26c74587b0c", "file_path": "src/domain/models.py", "start_line": 103, "end_line": 140, "content": "    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo lÃ­quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "775dc246da993a28bbeaabdf", "file_path": "src/domain/models.py", "start_line": 108, "end_line": 140, "content": "    def transaction_count(self) -> int:\n        \"\"\"Retorna o nÃºmero de transaÃ§Ãµes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "2de4ed98b84aeebb3fc36b28", "file_path": "src/domain/models.py", "start_line": 112, "end_line": 140, "content": "    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transaÃ§Ã£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "c81e29d10c2e82c5ed1540a2", "file_path": "src/domain/models.py", "start_line": 116, "end_line": 140, "content": "    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes de uma categoria especÃ­fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "96e0d30f0eb2149842409b29", "file_path": "src/domain/models.py", "start_line": 120, "end_line": 140, "content": "    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transaÃ§Ãµes em um perÃ­odo especÃ­fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "91421a6358d776d42b07513f", "file_path": "src/domain/models.py", "start_line": 129, "end_line": 140, "content": "class AnalysisResult:\n    \"\"\"Resultado da anÃ¡lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "72aba891735258e9f9339e0f", "file_path": "src/domain/models.py", "start_line": 137, "end_line": 140, "content": "    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "28534cd06ce6a551fbbcd474", "file_path": "src/domain/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote domain", "mtime": 1755104713.7980626, "terms": ["pacote", "domain"]}
{"chunk_id": "a2f746f75e19897bcc4c787d", "file_path": "src/domain/exceptions.py", "start_line": 1, "end_line": 33, "content": "\"\"\"\nExceÃ§Ãµes customizadas do domÃ­nio.\n\"\"\"\n\n\nclass DomainException(Exception):\n    \"\"\"ExceÃ§Ã£o base do domÃ­nio.\"\"\"\n    pass\n\n\nclass InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transaÃ§Ã£o Ã© invÃ¡lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato Ã© invÃ¡lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["exce", "es", "customizadas", "do", "dom", "nio", "class", "domainexception", "exception", "exce", "base", "do", "dom", "nio", "pass", "class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "2db100339f253381998313da", "file_path": "src/domain/exceptions.py", "start_line": 6, "end_line": 33, "content": "class DomainException(Exception):\n    \"\"\"ExceÃ§Ã£o base do domÃ­nio.\"\"\"\n    pass\n\n\nclass InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transaÃ§Ã£o Ã© invÃ¡lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato Ã© invÃ¡lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "domainexception", "exception", "exce", "base", "do", "dom", "nio", "pass", "class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "fe5d78b34f4564f7090e4fcb", "file_path": "src/domain/exceptions.py", "start_line": 11, "end_line": 33, "content": "class InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transaÃ§Ã£o Ã© invÃ¡lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato Ã© invÃ¡lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "98aedbb6370172bd7ee2a68a", "file_path": "src/domain/exceptions.py", "start_line": 16, "end_line": 33, "content": "class InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato Ã© invÃ¡lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "07b5667fa20356982563afc3", "file_path": "src/domain/exceptions.py", "start_line": 21, "end_line": 33, "content": "class ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "7734690c1b228eb15e6f43fc", "file_path": "src/domain/exceptions.py", "start_line": 26, "end_line": 33, "content": "class FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo nÃ£o Ã© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "5bc45aa5c03f565013079aba", "file_path": "src/domain/exceptions.py", "start_line": 31, "end_line": 33, "content": "class AnalysisError(DomainException):\n    \"\"\"Erro durante anÃ¡lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "11e0c82412b3e2aa030321d9", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o do analisador bÃ¡sico de extratos.\n\"\"\"\nfrom collections import defaultdict\nfrom decimal import Decimal\nfrom typing import Dict\n\nfrom src.domain.models import BankStatement, AnalysisResult, TransactionCategory\nfrom src.domain.interfaces import StatementAnalyzer\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass BasicStatementAnalyzer(StatementAnalyzer):\n    \"\"\"Analisador bÃ¡sico de extratos bancÃ¡rios.\"\"\"\n    \n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:", "mtime": 1756145358.5171318, "terms": ["implementa", "do", "analisador", "sico", "de", "extratos", "from", "collections", "import", "defaultdict", "from", "decimal", "import", "decimal", "from", "typing", "import", "dict", "from", "src", "domain", "models", "import", "bankstatement", "analysisresult", "transactioncategory", "from", "src", "domain", "interfaces", "import", "statementanalyzer", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "basicstatementanalyzer", "statementanalyzer", "analisador", "sico", "de", "extratos", "banc", "rios", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str"]}
{"chunk_id": "e04394cd51653cc185079166", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 13, "end_line": 92, "content": "class BasicStatementAnalyzer(StatementAnalyzer):\n    \"\"\"Analisador bÃ¡sico de extratos bancÃ¡rios.\"\"\"\n    \n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions ", "mtime": 1756145358.5171318, "terms": ["class", "basicstatementanalyzer", "statementanalyzer", "analisador", "sico", "de", "extratos", "banc", "rios", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions"]}
{"chunk_id": "35c4c02be3e4d783b4668ac4", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 16, "end_line": 95, "content": "    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:", "mtime": 1756145358.5171318, "terms": ["def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions"]}
{"chunk_id": "6aaecd880e25b0e86fe5a6e4", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 51, "end_line": 130, "content": "    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"âš ï¸ {uncategorized} transaÃ§Ãµes nÃ£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias especÃ­ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"âš ï¸ Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transaÃ§Ãµes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"âš ï¸ TransaÃ§Ã£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]", "mtime": 1756145358.5171318, "terms": ["def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category"]}
{"chunk_id": "4a04bbb0052bd6bbd81c6cff", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 62, "end_line": 141, "content": "    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"âš ï¸ {uncategorized} transaÃ§Ãµes nÃ£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias especÃ­ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"âš ï¸ Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transaÃ§Ãµes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"âš ï¸ TransaÃ§Ã£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"ðŸ’¡ Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre mÃ©dia diÃ¡ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days", "mtime": 1756145358.5171318, "terms": ["def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days"]}
{"chunk_id": "528c3986c01fa4da96995a22", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 69, "end_line": 148, "content": "            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"âš ï¸ {uncategorized} transaÃ§Ãµes nÃ£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias especÃ­ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"âš ï¸ Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transaÃ§Ãµes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"âš ï¸ TransaÃ§Ã£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"ðŸ’¡ Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre mÃ©dia diÃ¡ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"ðŸ’¡ MÃ©dia diÃ¡ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padrÃ£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()", "mtime": 1756145358.5171318, "terms": ["if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort"]}
{"chunk_id": "853c66e52e71bdda355fbf4a", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 80, "end_line": 159, "content": "    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na anÃ¡lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"âš ï¸ AtenÃ§Ã£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transaÃ§Ãµes nÃ£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"âš ï¸ {uncategorized} transaÃ§Ãµes nÃ£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias especÃ­ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"âš ï¸ Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transaÃ§Ãµes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"âš ï¸ TransaÃ§Ã£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"ðŸ’¡ Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre mÃ©dia diÃ¡ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"ðŸ’¡ MÃ©dia diÃ¡ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padrÃ£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"ðŸ’¡ {percentage:.0f}% das suas despesas sÃ£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequÃªncia de transaÃ§Ãµes", "mtime": 1756145358.5171318, "terms": ["def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es"]}
{"chunk_id": "857dc31280824f1cf682406e", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 122, "end_line": 189, "content": "    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"ðŸ’¡ Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre mÃ©dia diÃ¡ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"ðŸ’¡ MÃ©dia diÃ¡ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padrÃ£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"ðŸ’¡ {percentage:.0f}% das suas despesas sÃ£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequÃªncia de transaÃ§Ãµes\n        if (statement.transaction_count > 0 and\n            statement.period_end and statement.period_start and\n            statement.period_end > statement.period_start):\n            days = (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            if days > 0:\n                trans_per_day = statement.transaction_count / days\n                insights.append(\n                    f\"ðŸ’¡ MÃ©dia de {trans_per_day:.1f} transaÃ§Ãµes por dia\"\n                )\n        \n        # Insight sobre economia potencial\n        if categories_summary:\n            # Identifica categorias com potencial de economia\n            discretionary = [\n                TransactionCategory.LAZER,\n                TransactionCategory.COMPRAS,\n                TransactionCategory.ALIMENTACAO\n            ]\n            \n            potential_savings = Decimal('0')\n            for cat in discretionary:\n                if cat in categories_summary:\n                    potential_savings += categories_summary[cat] * Decimal('0.2')  # 20% de economia\n            \n            if potential_savings > 0:\n                insights.append(\n                    f\"ðŸ’¡ Potencial de economia: {currency_symbol} {potential_savings:.2f} reduzindo 20% em gastos discricionÃ¡rios\"\n                )\n        \n        return insights[:5]  # Limita a 5 insights mais relevantes", "mtime": 1756145358.5171318, "terms": ["def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es", "if", "statement", "transaction_count", "and", "statement", "period_end", "and", "statement", "period_start", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "if", "days", "trans_per_day", "statement", "transaction_count", "days", "insights", "append", "dia", "de", "trans_per_day", "transa", "es", "por", "dia", "insight", "sobre", "economia", "potencial", "if", "categories_summary", "identifica", "categorias", "com", "potencial", "de", "economia", "discretionary", "transactioncategory", "lazer", "transactioncategory", "compras", "transactioncategory", "alimentacao", "potential_savings", "decimal", "for", "cat", "in", "discretionary", "if", "cat", "in", "categories_summary", "potential_savings", "categories_summary", "cat", "decimal", "de", "economia", "if", "potential_savings", "insights", "append", "potencial", "de", "economia", "currency_symbol", "potential_savings", "reduzindo", "em", "gastos", "discricion", "rios", "return", "insights", "limita", "insights", "mais", "relevantes"]}
{"chunk_id": "bb27cfbbdc1ba88352cabfc7", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 137, "end_line": 189, "content": "        # Insight sobre mÃ©dia diÃ¡ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"ðŸ’¡ MÃ©dia diÃ¡ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padrÃ£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"ðŸ’¡ {percentage:.0f}% das suas despesas sÃ£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequÃªncia de transaÃ§Ãµes\n        if (statement.transaction_count > 0 and\n            statement.period_end and statement.period_start and\n            statement.period_end > statement.period_start):\n            days = (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            if days > 0:\n                trans_per_day = statement.transaction_count / days\n                insights.append(\n                    f\"ðŸ’¡ MÃ©dia de {trans_per_day:.1f} transaÃ§Ãµes por dia\"\n                )\n        \n        # Insight sobre economia potencial\n        if categories_summary:\n            # Identifica categorias com potencial de economia\n            discretionary = [\n                TransactionCategory.LAZER,\n                TransactionCategory.COMPRAS,\n                TransactionCategory.ALIMENTACAO\n            ]\n            \n            potential_savings = Decimal('0')\n            for cat in discretionary:\n                if cat in categories_summary:\n                    potential_savings += categories_summary[cat] * Decimal('0.2')  # 20% de economia\n            \n            if potential_savings > 0:\n                insights.append(\n                    f\"ðŸ’¡ Potencial de economia: {currency_symbol} {potential_savings:.2f} reduzindo 20% em gastos discricionÃ¡rios\"\n                )\n        \n        return insights[:5]  # Limita a 5 insights mais relevantes", "mtime": 1756145358.5171318, "terms": ["insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es", "if", "statement", "transaction_count", "and", "statement", "period_end", "and", "statement", "period_start", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "if", "days", "trans_per_day", "statement", "transaction_count", "days", "insights", "append", "dia", "de", "trans_per_day", "transa", "es", "por", "dia", "insight", "sobre", "economia", "potencial", "if", "categories_summary", "identifica", "categorias", "com", "potencial", "de", "economia", "discretionary", "transactioncategory", "lazer", "transactioncategory", "compras", "transactioncategory", "alimentacao", "potential_savings", "decimal", "for", "cat", "in", "discretionary", "if", "cat", "in", "categories_summary", "potential_savings", "categories_summary", "cat", "decimal", "de", "economia", "if", "potential_savings", "insights", "append", "potencial", "de", "economia", "currency_symbol", "potential_savings", "reduzindo", "em", "gastos", "discricion", "rios", "return", "insights", "limita", "insights", "mais", "relevantes"]}
{"chunk_id": "a4947b1460588a7c3753dd87", "file_path": "src/infrastructure/analyzers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote analyzers", "mtime": 1755104473.5524144, "terms": ["inicializa", "pacote", "analyzers"]}
{"chunk_id": "6bfbc7d0fbdf0fd3aba3abe0", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o de leitor de extratos em PDF.\n\"\"\"\nimport re\nimport json\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\nimport pdfplumber\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\n\n\nclass PDFStatementReader(StatementReader):\n    # Load configuration from a JSON file\n    CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'pdf_reader_config.json')\n\n    with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:\n        config = json.load(config_file)\n\n    date_patterns = [\n        r\"\\d{2}/\\d{2}/\\d{4}\",\n        r\"\\d{2}-\\d{2}-\\d{4}\",\n        r\"\\d{4}-\\d{2}-\\d{2}\",\n    ]\n\n    amount_patterns = [\n        r\"-?\\d{1,3}(?:\\.\\d{3})*,\\d{2}\",  # e.g. 1.234,56 or -1.234,56\n        r\"-?\\d+,\\d{2}\",  # e.g. 1234,56 or -1234,56\n        r\"-?\\d+\\.\\d{2}\",  # e.g. 1234.56 or -1234.56\n    ]\n\n    # Use config for bank names\n    bank_name_patterns = config.get('bank_name_patterns', [])\n\n    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)", "mtime": 1755732670.6391072, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "pdf", "import", "re", "import", "json", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "optional", "tuple", "import", "pdfplumber", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "class", "pdfstatementreader", "statementreader", "load", "configuration", "from", "json", "file", "config_path", "os", "path", "join", "os", "path", "dirname", "__file__", "pdf_reader_config", "json", "with", "open", "config_path", "encoding", "utf", "as", "config_file", "config", "json", "load", "config_file", "date_patterns", "amount_patterns", "or", "or", "or", "use", "config", "for", "bank", "names", "bank_name_patterns", "config", "get", "bank_name_patterns", "def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group"]}
{"chunk_id": "02fb0a2aff8a2e24a5f64251", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 18, "end_line": 97, "content": "class PDFStatementReader(StatementReader):\n    # Load configuration from a JSON file\n    CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'pdf_reader_config.json')\n\n    with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:\n        config = json.load(config_file)\n\n    date_patterns = [\n        r\"\\d{2}/\\d{2}/\\d{4}\",\n        r\"\\d{2}-\\d{2}-\\d{4}\",\n        r\"\\d{4}-\\d{2}-\\d{2}\",\n    ]\n\n    amount_patterns = [\n        r\"-?\\d{1,3}(?:\\.\\d{3})*,\\d{2}\",  # e.g. 1.234,56 or -1.234,56\n        r\"-?\\d+,\\d{2}\",  # e.g. 1234,56 or -1234,56\n        r\"-?\\d+\\.\\d{2}\",  # e.g. 1234.56 or -1234.56\n    ]\n\n    # Use config for bank names\n    bank_name_patterns = config.get('bank_name_patterns', [])\n\n    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"", "mtime": 1755732670.6391072, "terms": ["class", "pdfstatementreader", "statementreader", "load", "configuration", "from", "json", "file", "config_path", "os", "path", "join", "os", "path", "dirname", "__file__", "pdf_reader_config", "json", "with", "open", "config_path", "encoding", "utf", "as", "config_file", "config", "json", "load", "config_file", "date_patterns", "amount_patterns", "or", "or", "or", "use", "config", "for", "bank", "names", "bank_name_patterns", "config", "get", "bank_name_patterns", "def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period"]}
{"chunk_id": "0562c7c670757814c69d824f", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 40, "end_line": 119, "content": "    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')", "mtime": 1755732670.6391072, "terms": ["def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal"]}
{"chunk_id": "e77460a1a703cb9441fec400", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 45, "end_line": 124, "content": "    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',", "mtime": 1755732670.6391072, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency"]}
{"chunk_id": "40170ff61e67321ba5f7ed42", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 48, "end_line": 127, "content": "    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)", "mtime": 1755732670.6391072, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase"]}
{"chunk_id": "512af0cd4baeee17ca40aba0", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 69, "end_line": 148, "content": "            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)", "mtime": 1755732670.6391072, "terms": ["raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line"]}
{"chunk_id": "86efea49cddc93376d5531e2", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 71, "end_line": 150, "content": "    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break", "mtime": 1755732670.6391072, "terms": ["def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break"]}
{"chunk_id": "f72963c14baf6a3fbf7371f6", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 76, "end_line": 155, "content": "    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns"]}
{"chunk_id": "c386d0a1dd02ca46b1998e3f", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 83, "end_line": 162, "content": "    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)", "mtime": 1755732670.6391072, "terms": ["def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group"]}
{"chunk_id": "650b8eb517b343f13897bede", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 96, "end_line": 175, "content": "    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-aÃ ]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n", "mtime": 1755732670.6391072, "terms": ["def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str"]}
{"chunk_id": "d3892b8697bf77be238bf4d9", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 110, "end_line": 189, "content": "    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try"]}
{"chunk_id": "ffd97ccb0770949e3b7cc75c", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 121, "end_line": 200, "content": "    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,", "mtime": 1755732670.6391072, "terms": ["def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj"]}
{"chunk_id": "c7492d2c8d9f68e0d7dfb54c", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 132, "end_line": 211, "content": "    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception"]}
{"chunk_id": "857d8a633a79b1281a1c5834", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 137, "end_line": 212, "content": "            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "7e8aedb547e5325e6f6194e4", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 141, "end_line": 212, "content": "    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "886425a3229408866fd5171a", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 205, "end_line": 212, "content": "\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "6e6ad31219a555542490abbe", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 206, "end_line": 212, "content": "    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "45a1a79d9b12cdcc33deaf8c", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o de leitor de extratos em Excel.\n\"\"\"\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List\nimport pandas as pd\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass ExcelStatementReader(StatementReader):\n    \"\"\"Leitor de extratos bancÃ¡rios em formato Excel.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados estÃ£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:", "mtime": 1756149205.9433913, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "excel", "import", "re", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "import", "pandas", "as", "pd", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "excelstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "excel", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols"]}
{"chunk_id": "e1f205c036b1affc591a6df0", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 17, "end_line": 96, "content": "class ExcelStatementReader(StatementReader):\n    \"\"\"Leitor de extratos bancÃ¡rios em formato Excel.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados estÃ£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")", "mtime": 1756149205.9433913, "terms": ["class", "excelstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "excel", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel"]}
{"chunk_id": "b15eee4a8309b99690ebd347", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 20, "end_line": 99, "content": "    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados estÃ£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:", "mtime": 1756149205.9433913, "terms": ["def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try"]}
{"chunk_id": "80a85e027da04108f95aa195", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 23, "end_line": 102, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados estÃ£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)", "mtime": 1756149205.9433913, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str"]}
{"chunk_id": "b1148243b65451d5067fc66d", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 27, "end_line": 106, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados estÃ£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n", "mtime": 1756149205.9433913, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip"]}
{"chunk_id": "28ce5ce9bd2e054215d95bbf", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 62, "end_line": 141, "content": "    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n\n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n\n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type\n                )\n\n                transactions.append(transaction)\n\n            except Exception:\n                # Ignora transaÃ§Ãµes que nÃ£o podem ser processadas\n                continue\n\n        return transactions\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:", "mtime": 1756149205.9433913, "terms": ["def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "transactions", "append", "transaction", "except", "exception", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try"]}
{"chunk_id": "418bd1f219ba4bd3e77a083c", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 69, "end_line": 148, "content": "        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de possÃ­veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descriÃ§Ã£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n\n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n\n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type\n                )\n\n                transactions.append(transaction)\n\n            except Exception:\n                # Ignora transaÃ§Ãµes que nÃ£o podem ser processadas\n                continue\n\n        return transactions\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"NÃ£o foi possÃ­vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:", "mtime": 1756149205.9433913, "terms": ["amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "transactions", "append", "transaction", "except", "exception", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal"]}
{"chunk_id": "2d41aa89cecc394a08a4cedd", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 127, "end_line": 206, "content": "    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"NÃ£o foi possÃ­vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui vÃ­rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres nÃ£o numÃ©ricos exceto ponto, sinal de menos e dÃ­gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padrÃµes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura por padrÃµes de nÃºmero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # PadrÃ£o genÃ©rico para nÃºmeros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):", "mtime": 1756149205.9433913, "terms": ["def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value"]}
{"chunk_id": "e7b4a89c86c0c027c3be5156", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 137, "end_line": 216, "content": "            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"NÃ£o foi possÃ­vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui vÃ­rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres nÃ£o numÃ©ricos exceto ponto, sinal de menos e dÃ­gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padrÃµes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura por padrÃµes de nÃºmero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # PadrÃ£o genÃ©rico para nÃºmeros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    ", "mtime": 1756149205.9433913, "terms": ["for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions"]}
{"chunk_id": "d2e4a33489301d8d5e57a2c5", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 148, "end_line": 227, "content": "    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui vÃ­rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres nÃ£o numÃ©ricos exceto ponto, sinal de menos e dÃ­gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padrÃµes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura por padrÃµes de nÃºmero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # PadrÃ£o genÃ©rico para nÃºmeros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:", "mtime": 1756149205.9433913, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row"]}
{"chunk_id": "5386d83cb7fbdc0471ac4df0", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 164, "end_line": 243, "content": "    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padrÃµes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura por padrÃµes de nÃºmero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # PadrÃ£o genÃ©rico para nÃºmeros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())", "mtime": 1756149205.9433913, "terms": ["def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group"]}
{"chunk_id": "ff4ce092b154e0c9f46a0746", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 195, "end_line": 265, "content": "    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura por padrÃµes de nÃºmero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # PadrÃ£o genÃ©rico para nÃºmeros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "ec3e2c2fe20176ce668403e3", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 205, "end_line": 265, "content": "                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "9b336731716c4c4435ea13a4", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 211, "end_line": 265, "content": "    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "b7976bf597ba8e352032140b", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 217, "end_line": 265, "content": "    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transaÃ§Ãµes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "4a5a453ce6fe11ad887ee755", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 223, "end_line": 265, "content": "    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas prÃ³ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # PadrÃ£o para valores monetÃ¡rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "d01d9636e620fad3f648f017", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 248, "end_line": 265, "content": "    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padrÃµes de saldo final nas Ãºltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "d422869d4bf3b9df32d045c2", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o de leitor de extratos em CSV.\n\"\"\"\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List\nimport pandas as pd\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass CSVStatementReader(StatementReader):\n    \"\"\"Leitor de extratos bancÃ¡rios em formato CSV.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o", "mtime": 1756149193.6473725, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "csv", "import", "re", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "import", "pandas", "as", "pd", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "csvstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "csv", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri"]}
{"chunk_id": "0fdf58c9f5435fb96712c80e", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 17, "end_line": 96, "content": "class CSVStatementReader(StatementReader):\n    \"\"\"Leitor de extratos bancÃ¡rios em formato CSV.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,", "mtime": 1756149193.6473725, "terms": ["class", "csvstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "csv", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description"]}
{"chunk_id": "cd9c5fde7b0e6a4ae8890351", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 20, "end_line": 99, "content": "    def __init__(self):\n        self.currency = \"EUR\"  # SerÃ¡ detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after", "mtime": 1756149193.6473725, "terms": ["def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after"]}
{"chunk_id": "8cc38c9e27567b885c430706", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 23, "end_line": 102, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)", "mtime": 1756149193.6473725, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction"]}
{"chunk_id": "1278e0950053031262cb84d1", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 29, "end_line": 108, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"LÃª o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # LÃª o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transaÃ§Ãµes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato bancÃ¡rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transaÃ§Ãµes que nÃ£o podem ser processadas\n                continue\n                \n        return transactions", "mtime": 1756149193.6473725, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions"]}
{"chunk_id": "0bae68605443c9868e7fd246", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 61, "end_line": 140, "content": "    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transaÃ§Ãµes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descriÃ§Ã£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transaÃ§Ãµes que nÃ£o podem ser processadas\n                continue\n                \n        return transactions\n    \n    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para comparaÃ§Ã£o (remove acentos e converte para minÃºsculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para minÃºsculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:", "mtime": 1756149193.6473725, "terms": ["def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats"]}
{"chunk_id": "fd0f72348b0ccc40362a9996", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 69, "end_line": 148, "content": "        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo apÃ³s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"NÃ£o foi possÃ­vel identificar as colunas necessÃ¡rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descriÃ§Ã£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo apÃ³s a transaÃ§Ã£o, se disponÃ­vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transaÃ§Ã£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transaÃ§Ãµes que nÃ£o podem ser processadas\n                continue\n                \n        return transactions\n    \n    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para comparaÃ§Ã£o (remove acentos e converte para minÃºsculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para minÃºsculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser automÃ¡tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()", "mtime": 1756149193.6473725, "terms": ["balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime"]}
{"chunk_id": "8c45954875717ff61c771342", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 110, "end_line": 189, "content": "    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para comparaÃ§Ã£o (remove acentos e converte para minÃºsculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para minÃºsculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser automÃ¡tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monetÃ¡rio e determina o tipo de transaÃ§Ã£o.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se Ã© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui vÃ­rgula por ponto se necessÃ¡rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transaÃ§Ã£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"", "mtime": 1756149193.6473725, "terms": ["def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo"]}
{"chunk_id": "cfcbdfd774cf5c083777568b", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 120, "end_line": 199, "content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para comparaÃ§Ã£o (remove acentos e converte para minÃºsculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para minÃºsculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser automÃ¡tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monetÃ¡rio e determina o tipo de transaÃ§Ã£o.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se Ã© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui vÃ­rgula por ponto se necessÃ¡rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transaÃ§Ã£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espaÃ§os e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal", "mtime": 1756149193.6473725, "terms": ["def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal"]}
{"chunk_id": "41affa5b9f4c94187b9f603a", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 129, "end_line": 208, "content": "    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser automÃ¡tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monetÃ¡rio e determina o tipo de transaÃ§Ã£o.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se Ã© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui vÃ­rgula por ponto se necessÃ¡rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transaÃ§Ã£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espaÃ§os e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)", "mtime": 1756149193.6473725, "terms": ["def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned"]}
{"chunk_id": "706ca28da05a1c0fcefbe800", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 137, "end_line": 216, "content": "            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser automÃ¡tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monetÃ¡rio e determina o tipo de transaÃ§Ã£o.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se Ã© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui vÃ­rgula por ponto se necessÃ¡rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transaÃ§Ã£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espaÃ§os e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabeÃ§alhos\n        return \"Banco Desconhecido\"\n    ", "mtime": 1756149193.6473725, "terms": ["for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido"]}
{"chunk_id": "0fdd627c64322a85fda9c062", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 152, "end_line": 231, "content": "    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monetÃ¡rio e determina o tipo de transaÃ§Ã£o.\"\"\"\n        # Remove espaÃ§os e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se Ã© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui vÃ­rgula por ponto se necessÃ¡rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transaÃ§Ã£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"NÃ£o foi possÃ­vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espaÃ§os e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabeÃ§alhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o nÃºmero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'nÃºmero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor nÃ£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:", "mtime": 1756149193.6473725, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions"]}
{"chunk_id": "f1b03036683714c4a560f948", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 188, "end_line": 267, "content": "    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espaÃ§os e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove sÃ­mbolos de moeda e outros caracteres nÃ£o numÃ©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabeÃ§alhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o nÃºmero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'nÃºmero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor nÃ£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)", "mtime": 1756149193.6473725, "terms": ["def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type"]}
{"chunk_id": "67af249621160ad751f044f7", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 205, "end_line": 271, "content": "            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabeÃ§alhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o nÃºmero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'nÃºmero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor nÃ£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "1760e6faf3019d67a91a39f0", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 212, "end_line": 271, "content": "    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabeÃ§alhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o nÃºmero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'nÃºmero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor nÃ£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "4ffe76cf130c5e4490f7d402", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 217, "end_line": 271, "content": "    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nÃºmero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o nÃºmero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'nÃºmero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor nÃ£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "3dbd5bee9138a6949d2ace56", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 229, "end_line": 271, "content": "    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de inÃ­cio do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "7315831eddd00c8b746d111c", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 235, "end_line": 271, "content": "    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do perÃ­odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "2e31dea4bc03ca1d1bba6e29", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 241, "end_line": 271, "content": "    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "8a2a358f3768bf9d20c90f5d", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 256, "end_line": 271, "content": "    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o Ãºltimo valor nÃ£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "88c908446cd1c7eeac45779b", "file_path": "src/infrastructure/readers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote readers", "mtime": 1755104259.9851887, "terms": ["inicializa", "pacote", "readers"]}
{"chunk_id": "9b46726ecdae2fd6c2937b18", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o de categorizador simples de transaÃ§Ãµes baseado em palavras-chave.\n\"\"\"\nimport re\nfrom typing import Dict, List\n\nfrom src.domain.models import Transaction, TransactionCategory\nfrom src.domain.interfaces import TransactionCategorizer\n\n\nclass KeywordCategorizer(TransactionCategorizer):\n    \"\"\"Categorizador de transaÃ§Ãµes baseado em palavras-chave.\"\"\"\n    \n    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'aÃ§ougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'cafÃ©',\n                'cantina', 'refeitorio', 'almoÃ§o', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            ", "mtime": 1755104396.4543626, "terms": ["implementa", "de", "categorizador", "simples", "de", "transa", "es", "baseado", "em", "palavras", "chave", "import", "re", "from", "typing", "import", "dict", "list", "from", "src", "domain", "models", "import", "transaction", "transactioncategory", "from", "src", "domain", "interfaces", "import", "transactioncategorizer", "class", "keywordcategorizer", "transactioncategorizer", "categorizador", "de", "transa", "es", "baseado", "em", "palavras", "chave", "def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq"]}
{"chunk_id": "bde37f22281763134615338e", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 11, "end_line": 90, "content": "class KeywordCategorizer(TransactionCategorizer):\n    \"\"\"Categorizador de transaÃ§Ãµes baseado em palavras-chave.\"\"\"\n    \n    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'aÃ§ougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'cafÃ©',\n                'cantina', 'refeitorio', 'almoÃ§o', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'", "mtime": 1755104396.4543626, "terms": ["class", "keywordcategorizer", "transactioncategorizer", "categorizador", "de", "transa", "es", "baseado", "em", "palavras", "chave", "def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio"]}
{"chunk_id": "e24f28be669915e2c9c33f20", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 14, "end_line": 93, "content": "    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'aÃ§ougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'cafÃ©',\n                'cantina', 'refeitorio', 'almoÃ§o', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    ", "mtime": 1755104396.4543626, "terms": ["def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio"]}
{"chunk_id": "c2f233d8c1755b4803eeb7d6", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 17, "end_line": 96, "content": "    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'aÃ§ougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'cafÃ©',\n                'cantina', 'refeitorio', 'almoÃ§o', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    \n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o baseado em sua descriÃ§Ã£o.\"\"\"\n        description_lower = transaction.description.lower()", "mtime": 1755104396.4543626, "terms": ["def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower"]}
{"chunk_id": "aad7b9e5ed33e83656139292", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 69, "end_line": 134, "content": "            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    \n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o baseado em sua descriÃ§Ã£o.\"\"\"\n        description_lower = transaction.description.lower()\n        \n        # Remove acentos para melhor matching\n        description_normalized = self._normalize_text(description_lower)\n        \n        # Tenta encontrar categoria por palavras-chave\n        for category, keywords in self.keywords.items():\n            for keyword in keywords:\n                keyword_normalized = self._normalize_text(keyword.lower())\n                if keyword_normalized in description_normalized:\n                    transaction.category = category\n                    return transaction\n        \n        # Se nÃ£o encontrou categoria, mantÃ©m como nÃ£o categorizado\n        transaction.category = TransactionCategory.NAO_CATEGORIZADO\n        return transaction\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            'Ã¡': 'a', 'Ã ': 'a', 'Ã£': 'a', 'Ã¢': 'a', 'Ã¤': 'a',\n            'Ã©': 'e', 'Ã¨': 'e', 'Ãª': 'e', 'Ã«': 'e',\n            'Ã­': 'i', 'Ã¬': 'i', 'Ã®': 'i', 'Ã¯': 'i',\n            'Ã³': 'o', 'Ã²': 'o', 'Ãµ': 'o', 'Ã´': 'o', 'Ã¶': 'o',\n            'Ãº': 'u', 'Ã¹': 'u', 'Ã»': 'u', 'Ã¼': 'u',\n            'Ã§': 'c', 'Ã±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espaÃ§os e nÃºmeros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espaÃ§os mÃºltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower", "remove", "acentos", "para", "melhor", "matching", "description_normalized", "self", "_normalize_text", "description_lower", "tenta", "encontrar", "categoria", "por", "palavras", "chave", "for", "category", "keywords", "in", "self", "keywords", "items", "for", "keyword", "in", "keywords", "keyword_normalized", "self", "_normalize_text", "keyword", "lower", "if", "keyword_normalized", "in", "description_normalized", "transaction", "category", "category", "return", "transaction", "se", "encontrou", "categoria", "mant", "como", "categorizado", "transaction", "category", "transactioncategory", "nao_categorizado", "return", "transaction", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "594ac875fd1f65f74cc96d99", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 94, "end_line": 134, "content": "    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transaÃ§Ã£o baseado em sua descriÃ§Ã£o.\"\"\"\n        description_lower = transaction.description.lower()\n        \n        # Remove acentos para melhor matching\n        description_normalized = self._normalize_text(description_lower)\n        \n        # Tenta encontrar categoria por palavras-chave\n        for category, keywords in self.keywords.items():\n            for keyword in keywords:\n                keyword_normalized = self._normalize_text(keyword.lower())\n                if keyword_normalized in description_normalized:\n                    transaction.category = category\n                    return transaction\n        \n        # Se nÃ£o encontrou categoria, mantÃ©m como nÃ£o categorizado\n        transaction.category = TransactionCategory.NAO_CATEGORIZADO\n        return transaction\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            'Ã¡': 'a', 'Ã ': 'a', 'Ã£': 'a', 'Ã¢': 'a', 'Ã¤': 'a',\n            'Ã©': 'e', 'Ã¨': 'e', 'Ãª': 'e', 'Ã«': 'e',\n            'Ã­': 'i', 'Ã¬': 'i', 'Ã®': 'i', 'Ã¯': 'i',\n            'Ã³': 'o', 'Ã²': 'o', 'Ãµ': 'o', 'Ã´': 'o', 'Ã¶': 'o',\n            'Ãº': 'u', 'Ã¹': 'u', 'Ã»': 'u', 'Ã¼': 'u',\n            'Ã§': 'c', 'Ã±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espaÃ§os e nÃºmeros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espaÃ§os mÃºltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower", "remove", "acentos", "para", "melhor", "matching", "description_normalized", "self", "_normalize_text", "description_lower", "tenta", "encontrar", "categoria", "por", "palavras", "chave", "for", "category", "keywords", "in", "self", "keywords", "items", "for", "keyword", "in", "keywords", "keyword_normalized", "self", "_normalize_text", "keyword", "lower", "if", "keyword_normalized", "in", "description_normalized", "transaction", "category", "category", "return", "transaction", "se", "encontrou", "categoria", "mant", "como", "categorizado", "transaction", "category", "transactioncategory", "nao_categorizado", "return", "transaction", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "323262c310b31012122c5eaa", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 113, "end_line": 134, "content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            'Ã¡': 'a', 'Ã ': 'a', 'Ã£': 'a', 'Ã¢': 'a', 'Ã¤': 'a',\n            'Ã©': 'e', 'Ã¨': 'e', 'Ãª': 'e', 'Ã«': 'e',\n            'Ã­': 'i', 'Ã¬': 'i', 'Ã®': 'i', 'Ã¯': 'i',\n            'Ã³': 'o', 'Ã²': 'o', 'Ãµ': 'o', 'Ã´': 'o', 'Ã¶': 'o',\n            'Ãº': 'u', 'Ã¹': 'u', 'Ã»': 'u', 'Ã¼': 'u',\n            'Ã§': 'c', 'Ã±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espaÃ§os e nÃºmeros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espaÃ§os mÃºltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "978b6e5656128a30ed538e88", "file_path": "src/infrastructure/categorizers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote categorizers", "mtime": 1755104402.6686974, "terms": ["inicializa", "pacote", "categorizers"]}
{"chunk_id": "98ff3dce7c4ebd7b8a5dea25", "file_path": "src/infrastructure/reports/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote reports", "mtime": 1755104542.934863, "terms": ["inicializa", "pacote", "reports"]}
{"chunk_id": "356a881d60a3f3868d5af108", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementaÃ§Ã£o de gerador de relatÃ³rios em texto.\n\"\"\"\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom src.domain.models import AnalysisResult, TransactionCategory\nfrom src.domain.interfaces import ReportGenerator\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass TextReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relatÃ³rios em formato texto.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em texto a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELATÃ“RIO DE ANÃLISE DE EXTRATO BANCÃRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do PerÃ­odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'MÃªs':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:", "mtime": 1756145388.113131, "terms": ["implementa", "de", "gerador", "de", "relat", "rios", "em", "texto", "from", "datetime", "import", "datetime", "from", "pathlib", "import", "path", "from", "typing", "import", "optional", "from", "src", "domain", "models", "import", "analysisresult", "transactioncategory", "from", "src", "domain", "interfaces", "import", "reportgenerator", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "textreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "texto", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts"]}
{"chunk_id": "25a43c5f07d75499aa53532c", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 13, "end_line": 92, "content": "class TextReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relatÃ³rios em formato texto.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em texto a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELATÃ“RIO DE ANÃLISE DE EXTRATO BANCÃRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do PerÃ­odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'MÃªs':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDAÃ‡Ã•ES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados", "mtime": 1756145388.113131, "terms": ["class", "textreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "texto", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados"]}
{"chunk_id": "06af7ee77570163b6856b2d5", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 16, "end_line": 95, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em texto a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELATÃ“RIO DE ANÃLISE DE EXTRATO BANCÃRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do PerÃ­odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'MÃªs':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDAÃ‡Ã•ES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMAÃ‡Ã•ES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)", "mtime": 1756145388.113131, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append"]}
{"chunk_id": "e961a0d0187c39d3b3e5d64d", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 19, "end_line": 98, "content": "        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELATÃ“RIO DE ANÃLISE DE EXTRATO BANCÃRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do PerÃ­odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'MÃªs':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDAÃ‡Ã•ES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMAÃ‡Ã•ES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"Total de transaÃ§Ãµes: {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:", "mtime": 1756145388.113131, "terms": ["def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata"]}
{"chunk_id": "021bbe2bbad462c53be4f95b", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 69, "end_line": 148, "content": "                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDAÃ‡Ã•ES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMAÃ‡Ã•ES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"Total de transaÃ§Ãµes: {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"PerÃ­odo analisado: {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n\n        # RodapÃ©\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"Fim do RelatÃ³rio\")\n        report_lines.append(\"=\" * 80)\n\n        report_content = \"\\n\".join(report_lines)\n\n        # Salva o arquivo se especificado\n        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content\n\n\nclass MarkdownReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relatÃ³rios em formato Markdown.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em Markdown a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"# RelatÃ³rio de AnÃ¡lise de Extrato BancÃ¡rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## ðŸ’° Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do PerÃ­odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## ðŸ“Š Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")", "mtime": 1756145388.113131, "terms": ["format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append", "rodap", "report_lines", "append", "report_lines", "append", "fim", "do", "relat", "rio", "report_lines", "append", "report_content", "join", "report_lines", "salva", "arquivo", "se", "especificado", "if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content", "class", "markdownreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "markdown", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual"]}
{"chunk_id": "bd3876feaf5680c1ff0c7e52", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 116, "end_line": 195, "content": "class MarkdownReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relatÃ³rios em formato Markdown.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em Markdown a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"# RelatÃ³rio de AnÃ¡lise de Extrato BancÃ¡rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## ðŸ’° Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do PerÃ­odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## ðŸ“Š Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## ðŸ“… Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| MÃªs | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## âš ï¸ Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## ðŸ’¡ Insights e RecomendaÃ§Ãµes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## â„¹ï¸ InformaÃ§Ãµes Adicionais\")\n            report_lines.append(\"\")", "mtime": 1756145388.113131, "terms": ["class", "markdownreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "markdown", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append"]}
{"chunk_id": "3436bdec7971a576f1ee9c8c", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 119, "end_line": 198, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relatÃ³rio em Markdown a partir da anÃ¡lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"# RelatÃ³rio de AnÃ¡lise de Extrato BancÃ¡rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## ðŸ’° Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do PerÃ­odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## ðŸ“Š Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## ðŸ“… Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| MÃªs | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## âš ï¸ Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## ðŸ’¡ Insights e RecomendaÃ§Ãµes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## â„¹ï¸ InformaÃ§Ãµes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transaÃ§Ãµes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:", "mtime": 1756145388.113131, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata"]}
{"chunk_id": "b75f59ffce86d5b3f271826a", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 122, "end_line": 201, "content": "        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # CabeÃ§alho\n        report_lines.append(\"# RelatÃ³rio de AnÃ¡lise de Extrato BancÃ¡rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## ðŸ’° Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do PerÃ­odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## ðŸ“Š Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## ðŸ“… Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| MÃªs | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## âš ï¸ Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## ðŸ’¡ Insights e RecomendaÃ§Ãµes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## â„¹ï¸ InformaÃ§Ãµes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transaÃ§Ãµes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"- **PerÃ­odo analisado:** {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n", "mtime": 1756145388.113131, "terms": ["def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append"]}
{"chunk_id": "b382c859a155634fa4112aa0", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 137, "end_line": 208, "content": "        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do PerÃ­odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # AnÃ¡lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## ðŸ“Š Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## ðŸ“… Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| MÃªs | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## âš ï¸ Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## ðŸ’¡ Insights e RecomendaÃ§Ãµes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## â„¹ï¸ InformaÃ§Ãµes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transaÃ§Ãµes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"- **PerÃ­odo analisado:** {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n\n        report_content = \"\\n\".join(report_lines)\n\n        # Salva o arquivo se especificado\n        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content", "mtime": 1756145388.113131, "terms": ["report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append", "report_content", "join", "report_lines", "salva", "arquivo", "se", "especificado", "if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content"]}
{"chunk_id": "e43f9c9a3286d4317f524144", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 205, "end_line": 208, "content": "        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content", "mtime": 1756145388.113131, "terms": ["if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content"]}
{"chunk_id": "1d453cc663a82c33098b9305", "file_path": "mcp_system/embeddings/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Embeddings Module\n\"\"\"\nMÃ³dulo de embeddings semÃ¢nticos para busca avanÃ§ada de cÃ³digo.\n\"\"\"\n\nfrom .semantic_search import *", "mtime": 1755701269.9533663, "terms": ["mcp", "system", "embeddings", "module", "dulo", "de", "embeddings", "sem", "nticos", "para", "busca", "avan", "ada", "de", "digo", "from", "semantic_search", "import"]}
{"chunk_id": "2a76c039e2de03f171d572bf", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca semÃ¢ntica usando embeddings locais\nIntegraÃ§Ã£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()", "mtime": 1756157462.8336525, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest"]}
{"chunk_id": "c9a7fee4bfc4236d23450b6d", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 28, "end_line": 107, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou", "mtime": 1756157462.8336525, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou"]}
{"chunk_id": "7ff98c4efc32e8ee6b6f5851", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 36, "end_line": 115, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")", "mtime": 1756157462.8336525, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id"]}
{"chunk_id": "7eb3a095dd31c712c93de3ef", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 42, "end_line": 121, "content": "    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"âš ï¸  sentence-transformers nÃ£o encontrado. Busca semÃ¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]", "mtime": 1756157462.8336525, "terms": ["def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated"]}
{"chunk_id": "25bada44e06790f0cf61c3ff", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 58, "end_line": 137, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding", "mtime": 1756157462.8336525, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding"]}
{"chunk_id": "1007892ad8e3796ef8b23647", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:", "mtime": 1756157462.8336525, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "7c28a119c5566a7b8780dd03", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 70, "end_line": 149, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1756157462.8336525, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "f6c3786ef46e1a6a4c2f7fb8", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 74, "end_line": 153, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta", "mtime": 1756157462.8336525, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "dd0cdb09bb74312893501205", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 78, "end_line": 157, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:", "mtime": 1756157462.8336525, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "664c1cb4c9b76767ad5768ba", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 82, "end_line": 161, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []", "mtime": 1756157462.8336525, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return"]}
{"chunk_id": "d1fa618a10ca4b0f2dc73d4e", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))", "mtime": 1756157462.8336525, "terms": ["self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "7efd51a008fd4b37818f43fb", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 147, "end_line": 226, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o", "mtime": 1756157462.8336525, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele"]}
{"chunk_id": "b95f300cd70552cf5f4fa24f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 186, "end_line": 265, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1756157462.8336525, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "ecaf676ab58c80fd03e6e23e", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    ", "mtime": 1756157462.8336525, "terms": ["results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none"]}
{"chunk_id": "f98f3736a9dd7e0e7c363c77", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 271, "end_line": 308, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756157462.8336525, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "909d4c220f71e45ecad7bbaf", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 273, "end_line": 308, "content": "        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756157462.8336525, "terms": ["cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "9f7af800a37739fdf17c9fc2", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 285, "end_line": 308, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756157462.8336525, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "8b05260274385573f1829358", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 300, "end_line": 308, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756157462.8336525, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "c2cded8a1a0c55659753faf0", "file_path": "mcp_system/cache/search_cache.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nMÃ³dulo para gerenciamento de cache de resultados de busca\n\"\"\"\nimport json\nimport hashlib\nimport time\nfrom typing import Any, Dict, Optional\nimport pathlib\n\n# Removendo imports nÃ£o utilizados que podem causar problemas\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\nclass SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()", "mtime": 1756155043.6534107, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gerenciamento", "de", "cache", "de", "resultados", "de", "busca", "import", "json", "import", "hashlib", "import", "time", "from", "typing", "import", "any", "dict", "optional", "import", "pathlib", "removendo", "imports", "utilizados", "que", "podem", "causar", "problemas", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "4ce7917802936d4d062ceeab", "file_path": "mcp_system/cache/search_cache.py", "start_line": 16, "end_line": 95, "content": "class SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio", "mtime": 1756155043.6534107, "terms": ["class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio"]}
{"chunk_id": "3af3a9d92cf861865651f60e", "file_path": "mcp_system/cache/search_cache.py", "start_line": 19, "end_line": 98, "content": "    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: NÃºmero mÃ¡ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:", "mtime": 1756155043.6534107, "terms": ["def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order"]}
{"chunk_id": "e4950e5693b80d741bb5ab62", "file_path": "mcp_system/cache/search_cache.py", "start_line": 32, "end_line": 111, "content": "    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave Ãºnica para uma consulta e seus parÃ¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Chave hash Ãºnica para o cache\n        \"\"\"\n        # Criar um dicionÃ¡rio ordenado com todos os parÃ¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consistÃªncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n", "mtime": 1756155043.6534107, "terms": ["def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "a85b1314fc4a5fe1e9dbeecd", "file_path": "mcp_system/cache/search_cache.py", "start_line": 53, "end_line": 132, "content": "    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se disponÃ­veis e vÃ¡lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros parÃ¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se nÃ£o disponÃ­veis/invÃ¡lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)", "mtime": 1756155043.6534107, "terms": ["def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key"]}
{"chunk_id": "8dbd353be41fcf6167fb5e5a", "file_path": "mcp_system/cache/search_cache.py", "start_line": 69, "end_line": 148, "content": "\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n", "mtime": 1756155043.6534107, "terms": ["verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache"]}
{"chunk_id": "41301c4e9afe3ef1d0779512", "file_path": "mcp_system/cache/search_cache.py", "start_line": 84, "end_line": 163, "content": "    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros parÃ¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }", "mtime": 1756155043.6534107, "terms": ["def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds"]}
{"chunk_id": "162a44b0d38ed2bc4fc558b0", "file_path": "mcp_system/cache/search_cache.py", "start_line": 112, "end_line": 167, "content": "    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo especÃ­fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados contÃªm referÃªncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "0924278789039792ac131142", "file_path": "mcp_system/cache/search_cache.py", "start_line": 137, "end_line": 167, "content": "            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "a226be655a743135c6e772fa", "file_path": "mcp_system/cache/search_cache.py", "start_line": 140, "end_line": 167, "content": "    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "1de80eddef7e1d6890a15169", "file_path": "mcp_system/cache/search_cache.py", "start_line": 145, "end_line": 167, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estatÃ­sticas do cache.\n\n        Returns:\n            DicionÃ¡rio com estatÃ­sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# InstÃ¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "82497e02d05040ef40048507", "file_path": "mcp_system/cache/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Cache Module\n\"\"\"\nSistema de cache para embeddings e Ã­ndices BM25.\n\"\"\"\n\nfrom .search_cache import *", "mtime": 1755701282.161089, "terms": ["mcp", "system", "cache", "module", "sistema", "de", "cache", "para", "embeddings", "ndices", "bm25", "from", "search_cache", "import"]}
{"chunk_id": "ba991c54d630e62f500bc537", "file_path": "mcp_system/utils/__init__.py", "start_line": 1, "end_line": 7, "content": "# MCP System - Utils Module\n\"\"\"\nUtilitÃ¡rios do sistema MCP: embeddings, file watcher, etc.\n\"\"\"\n\nfrom .embeddings import *\nfrom .file_watcher import *", "mtime": 1755701275.657908, "terms": ["mcp", "system", "utils", "module", "utilit", "rios", "do", "sistema", "mcp", "embeddings", "file", "watcher", "etc", "from", "embeddings", "import", "from", "file_watcher", "import"]}
{"chunk_id": "30daac062edbf0d2c61b5086", "file_path": "mcp_system/utils/embeddings.py", "start_line": 1, "end_line": 70, "content": "#!/usr/bin/env python3\n\"\"\"\nMÃ³dulo para geraÃ§Ã£o de embeddings usando Sentence Transformers\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Union\n\n\nclass EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gera", "de", "embeddings", "usando", "sentence", "transformers", "from", "sentence_transformers", "import", "sentencetransformer", "import", "numpy", "as", "np", "from", "typing", "import", "union", "class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "41af9880584da37a250fc4a2", "file_path": "mcp_system/utils/embeddings.py", "start_line": 10, "end_line": 70, "content": "class EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "66bd7019da4646ec07d6b2a0", "file_path": "mcp_system/utils/embeddings.py", "start_line": 13, "end_line": 70, "content": "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "fdd17cea8efc2913e72e6547", "file_path": "mcp_system/utils/embeddings.py", "start_line": 24, "end_line": 70, "content": "    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impressÃ£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "3dc38ce4a72483f2403b4462", "file_path": "mcp_system/utils/embeddings.py", "start_line": 36, "end_line": 70, "content": "    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleatÃ³rio se modelo nÃ£o estiver disponÃ­vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleatÃ³rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "3fb90fa9e3e9400815788996", "file_path": "mcp_system/utils/embeddings.py", "start_line": 57, "end_line": 70, "content": "    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimensÃ£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            DimensÃ£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # DimensÃ£o padrÃ£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "973af24920e98d12e092952c", "file_path": "mcp_system/utils/embeddings.py", "start_line": 69, "end_line": 70, "content": "# InstÃ¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "09d216e43f6f919e65f213a9", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 1, "end_line": 80, "content": "# src/utils/file_watcher.py\n\"\"\"\nSistema de monitoramento de arquivos para auto-indexaÃ§Ã£o\nDetecta mudanÃ§as e reindexar automaticamente arquivos modificados\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport time\nimport threading\nfrom pathlib import Path\nfrom typing import Set, Callable, Dict, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport hashlib\nimport pathlib\n\nimport sys\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from watchdog.observers import Observer\n    from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent\n    HAS_WATCHDOG = True\nexcept ImportError:\n    HAS_WATCHDOG = False\n    Observer = None\n    FileSystemEventHandler = None\n\n@dataclass\nclass IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False", "mtime": 1756155021.429876, "terms": ["src", "utils", "file_watcher", "py", "sistema", "de", "monitoramento", "de", "arquivos", "para", "auto", "indexa", "detecta", "mudan", "as", "reindexar", "automaticamente", "arquivos", "modificados", "from", "__future__", "import", "annotations", "import", "os", "import", "time", "import", "threading", "from", "pathlib", "import", "path", "from", "typing", "import", "set", "callable", "dict", "optional", "from", "dataclasses", "import", "dataclass", "from", "concurrent", "futures", "import", "threadpoolexecutor", "import", "hashlib", "import", "pathlib", "import", "sys", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "watchdog", "observers", "import", "observer", "from", "watchdog", "events", "import", "filesystemeventhandler", "filemodifiedevent", "filecreatedevent", "filedeletedevent", "has_watchdog", "true", "except", "importerror", "has_watchdog", "false", "observer", "none", "filesystemeventhandler", "none", "dataclass", "class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false"]}
{"chunk_id": "011ca07bac2167d13f0fc625", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 33, "end_line": 112, "content": "class IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce", "mtime": 1756155021.429876, "terms": ["class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce"]}
{"chunk_id": "14912b31261e36f9f6af6810", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 38, "end_line": 117, "content": "class FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudanÃ§as\n    e agenda reindexaÃ§Ã£o automÃ¡tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:", "mtime": 1756155021.429876, "terms": ["class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer"]}
{"chunk_id": "c9d32597fbc33fe112445f6b", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 44, "end_line": 123, "content": "    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # EstatÃ­sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks"]}
{"chunk_id": "edd472a9074578137a38711a", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 69, "end_line": 148, "content": "            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"âš ï¸  watchdog nÃ£o encontrado. Auto-indexaÃ§Ã£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    ", "mtime": 1756155021.429876, "terms": ["events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear"]}
{"chunk_id": "0f754497948ef15b4065dfe6", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 77, "end_line": 156, "content": "    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extensÃ£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diretÃ³rios especÃ­ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos temporÃ¡rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return", "mtime": 1756155021.429876, "terms": ["def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return"]}
{"chunk_id": "4a4fdd2c991b434d8a103e07", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 97, "end_line": 176, "content": "    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexaÃ§Ã£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n", "mtime": 1756155021.429876, "terms": ["def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr"]}
{"chunk_id": "37256eaccd29aafd6cca356c", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 115, "end_line": 194, "content": "    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')", "mtime": 1756155021.429876, "terms": ["def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted"]}
{"chunk_id": "15cdb388361c58c077d75159", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 127, "end_line": 206, "content": "    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por aÃ§Ã£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)", "mtime": 1756155021.429876, "terms": ["def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr"]}
{"chunk_id": "f09d96deefb43f34c7347baa", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 137, "end_line": 216, "content": "                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"âŒ Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )", "mtime": 1756155021.429876, "terms": ["tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true"]}
{"chunk_id": "793a8eff9eccfd7666282c5e", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 149, "end_line": 228, "content": "    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexaÃ§Ã£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"ðŸ”„ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"âœ… {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"âŒ Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remoÃ§Ã£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"â„¹ï¸  {len(deleted_files)} arquivo(s) deletado(s) (remoÃ§Ã£o de Ã­ndice nÃ£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)", "mtime": 1756155021.429876, "terms": ["def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr"]}
{"chunk_id": "5648195fbcefb785eedf8b50", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 177, "end_line": 256, "content": "class WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:", "mtime": 1756155021.429876, "terms": ["class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as"]}
{"chunk_id": "24b82ec795de19a23c981cd8", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 180, "end_line": 259, "content": "    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict"]}
{"chunk_id": "84f0c44c477e3317dd6a16ad", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 184, "end_line": 263, "content": "    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,", "mtime": 1756155021.429876, "terms": ["def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running"]}
{"chunk_id": "6ae19459e25e66a982d7b9e5", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 188, "end_line": 267, "content": "    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats", "mtime": 1756155021.429876, "terms": ["def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats"]}
{"chunk_id": "4fbc0db3cf208d0aa61b9b57", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 192, "end_line": 271, "content": "    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"", "mtime": 1756155021.429876, "terms": ["def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher"]}
{"chunk_id": "5cc1643c33d4eaae76df7067", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 196, "end_line": 275, "content": "class FileWatcher(FileWatcher):\n    \"\"\"ExtensÃ£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    ", "mtime": 1756155021.429876, "terms": ["class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as"]}
{"chunk_id": "26c1b5de96520a92ac1fcb68", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 199, "end_line": 278, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"âŒ watchdog nÃ£o disponÃ­vel. Auto-indexaÃ§Ã£o nÃ£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,", "mtime": 1756155021.429876, "terms": ["def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none"]}
{"chunk_id": "b4dda0319994c5372a6e4af0", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 205, "end_line": 284, "content": "        if self.is_running:\n            print(\"âš ï¸  File watcher jÃ¡ estÃ¡ rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"âœ… File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"âŒ Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {", "mtime": 1756155021.429876, "terms": ["if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or"]}
{"chunk_id": "08514bd3db9742a5f9caf378", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 231, "end_line": 310, "content": "    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"âœ… File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []", "mtime": 1756155021.429876, "terms": ["def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files"]}
{"chunk_id": "8b64965c58475378f6acd0f3", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 248, "end_line": 327, "content": "    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que estÃ£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"âš ï¸  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou", "mtime": 1756155021.429876, "terms": ["def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou"]}
{"chunk_id": "0f663d2eece14bd0e36fbfcf", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 259, "end_line": 338, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        ", "mtime": 1756155021.429876, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files"]}
{"chunk_id": "61ad2a38d1db52d21ef5471a", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 270, "end_line": 349, "content": "class SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog nÃ£o estÃ¡ disponÃ­vel\n    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:", "mtime": 1756155021.429876, "terms": ["class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files"]}
{"chunk_id": "07fc1cc6e8e8d2db8a1b4a6c", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 273, "end_line": 352, "content": "    Usa polling para detectar mudanÃ§as\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n", "mtime": 1756155021.429876, "terms": ["usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr"]}
{"chunk_id": "501fe5697dc4944c3596c5fb", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 276, "end_line": 355, "content": "    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files"]}
{"chunk_id": "091c98bf21cd4d72123ce821", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 300, "end_line": 379, "content": "    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudanÃ§as\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        ", "mtime": 1756155021.429876, "terms": ["def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start"]}
{"chunk_id": "68181808871efe705d0aa982", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 308, "end_line": 387, "content": "    def _scan_for_changes(self):\n        \"\"\"Escaneia diretÃ³rio em busca de mudanÃ§as\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():", "mtime": 1756155021.429876, "terms": ["def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive"]}
{"chunk_id": "5151408d4258af305dea539f", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 341, "end_line": 410, "content": "    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"ðŸ”„ Detectadas mudanÃ§as em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"âœ… {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"âŒ Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"âŒ Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda prÃ³ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "eae42d0848cb3793944b84da", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 367, "end_line": 410, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"âš ï¸  Simple file watcher jÃ¡ estÃ¡ rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"âœ… Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"ðŸ“Š Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "35e89241e437db4b0eada589", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 384, "end_line": 410, "content": "    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"âœ… Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "983627edbbeebb5d615f1ea8", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 391, "end_line": 410, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estatÃ­sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "092c1c41268f95f32c87238d", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 402, "end_line": 410, "content": "def create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher disponÃ­vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "8ca2c4ca3f7448a93a858702", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 409, "end_line": 410, "content": "        print(\"ðŸ“ Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "10d239d2cab55843f6dfb5b5", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nResumo de mÃ©tricas do MCP (context_pack) a partir de mcp_system/.mcp_index/metrics.csv\n\nCampos esperados no CSV:\n  ts, query, chunk_count, total_tokens, budget_tokens, budget_utilization, latency_ms\n\nUso bÃ¡sico:\n  python summarize_metrics.py\n  python summarize_metrics.py --file mcp_system/.mcp_index/metrics.csv\n  python summarize_metrics.py --since 7\n  python summarize_metrics.py --filter \"minha funcao\"\n  python summarize_metrics.py --json\n\nSaÃ­da:\n- Resumo geral (perÃ­odo, N linhas, tokens mÃ©dios, p95, etc.)\n- Tabela diÃ¡ria com avg/median/p95 de tokens e latÃªncia\n\"\"\"\n\nimport os, sys, csv, argparse, datetime as dt, statistics as st, json\nfrom math import floor\nfrom typing import List, Dict, Any\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ndef p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\ndef parse_dt(s):\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            if fmt.endswith(\"%z\"):\n                return dt.datetime.strptime(s, fmt)\n            else:\n                return dt.datetime.strptime(s, fmt).replace(tzinfo=dt.timezone.utc)\n        except ValueError:\n            continue\n    return None\n\ndef load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)", "mtime": 1756154881.852048, "terms": ["usr", "bin", "env", "python3", "resumo", "de", "tricas", "do", "mcp", "context_pack", "partir", "de", "mcp_system", "mcp_index", "metrics", "csv", "campos", "esperados", "no", "csv", "ts", "query", "chunk_count", "total_tokens", "budget_tokens", "budget_utilization", "latency_ms", "uso", "sico", "python", "summarize_metrics", "py", "python", "summarize_metrics", "py", "file", "mcp_system", "mcp_index", "metrics", "csv", "python", "summarize_metrics", "py", "since", "python", "summarize_metrics", "py", "filter", "minha", "funcao", "python", "summarize_metrics", "py", "json", "sa", "da", "resumo", "geral", "per", "odo", "linhas", "tokens", "dios", "p95", "etc", "tabela", "di", "ria", "com", "avg", "median", "p95", "de", "tokens", "lat", "ncia", "import", "os", "sys", "csv", "argparse", "datetime", "as", "dt", "statistics", "as", "st", "json", "from", "math", "import", "floor", "from", "typing", "import", "list", "dict", "any", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "if", "fmt", "endswith", "return", "dt", "datetime", "strptime", "fmt", "else", "return", "dt", "datetime", "strptime", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "except", "valueerror", "continue", "return", "none", "def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader"]}
{"chunk_id": "db390d5229d989002b837f12", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 28, "end_line": 107, "content": "def p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\ndef parse_dt(s):\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            if fmt.endswith(\"%z\"):\n                return dt.datetime.strptime(s, fmt)\n            else:\n                return dt.datetime.strptime(s, fmt).replace(tzinfo=dt.timezone.utc)\n        except ValueError:\n            continue\n    return None\n\ndef load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n", "mtime": 1756154881.852048, "terms": ["def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "if", "fmt", "endswith", "return", "dt", "datetime", "strptime", "fmt", "else", "return", "dt", "datetime", "strptime", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "except", "valueerror", "continue", "return", "none", "def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows"]}
{"chunk_id": "1f47afc7ab0b28b455e401b7", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 34, "end_line": 113, "content": "def coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\ndef parse_dt(s):\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            if fmt.endswith(\"%z\"):\n                return dt.datetime.strptime(s, fmt)\n            else:\n                return dt.datetime.strptime(s, fmt).replace(tzinfo=dt.timezone.utc)\n        except ValueError:\n            continue\n    return None\n\ndef load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")", "mtime": 1756154881.852048, "terms": ["def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "if", "fmt", "endswith", "return", "dt", "datetime", "strptime", "fmt", "else", "return", "dt", "datetime", "strptime", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "except", "valueerror", "continue", "return", "none", "def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado"]}
{"chunk_id": "b98100f7416917a652aae00d", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 40, "end_line": 119, "content": "def coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\ndef parse_dt(s):\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            if fmt.endswith(\"%z\"):\n                return dt.datetime.strptime(s, fmt)\n            else:\n                return dt.datetime.strptime(s, fmt).replace(tzinfo=dt.timezone.utc)\n        except ValueError:\n            continue\n    return None\n\ndef load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))", "mtime": 1756154881.852048, "terms": ["def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "if", "fmt", "endswith", "return", "dt", "datetime", "strptime", "fmt", "else", "return", "dt", "datetime", "strptime", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "except", "valueerror", "continue", "return", "none", "def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts"]}
{"chunk_id": "80546208852e36d6c3fbd530", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 46, "end_line": 125, "content": "def parse_dt(s):\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            if fmt.endswith(\"%z\"):\n                return dt.datetime.strptime(s, fmt)\n            else:\n                return dt.datetime.strptime(s, fmt).replace(tzinfo=dt.timezone.utc)\n        except ValueError:\n            continue\n    return None\n\ndef load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]", "mtime": 1756154881.852048, "terms": ["def", "parse_dt", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "if", "fmt", "endswith", "return", "dt", "datetime", "strptime", "fmt", "else", "return", "dt", "datetime", "strptime", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "except", "valueerror", "continue", "return", "none", "def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows"]}
{"chunk_id": "9c1e546d70855b44dadaa513", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 60, "end_line": 139, "content": "def load_csv(path: str) -> List[Dict[str, Any]]:\n    # Se nenhum caminho foi especificado, usar o padrÃ£o dentro da pasta mcp_system\n    if path == \"mcp_system/.mcp_index/metrics.csv\":\n        path = str(CURRENT_DIR / \".mcp_index/metrics.csv\")\n    \n    if not os.path.exists(path):\n        print(f\"[erro] CSV nÃ£o encontrado: {path}\")\n        sys.exit(1)\n    \n    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]\n    \n    # Calcular estatÃ­sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n    \n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n    \n    # Agrupar por dia\n    daily_stats = {}", "mtime": 1756154881.852048, "terms": ["def", "load_csv", "path", "str", "list", "dict", "str", "any", "se", "nenhum", "caminho", "foi", "especificado", "usar", "padr", "dentro", "da", "pasta", "mcp_system", "if", "path", "mcp_system", "mcp_index", "metrics", "csv", "path", "str", "current_dir", "mcp_index", "metrics", "csv", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "sys", "exit", "with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "daily_stats"]}
{"chunk_id": "b8fe383e8d78f74db0e9a183", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 69, "end_line": 148, "content": "    with open(path) as f:\n        # Detectar se Ã© o arquivo de mÃ©tricas de indexaÃ§Ã£o ou de context pack\n        first_line = f.readline()\n        f.seek(0)\n        \n        if \"query\" in first_line:\n            # Arquivo de mÃ©tricas de context pack\n            reader = csv.DictReader(f)\n            return [row for row in reader]\n        else:\n            # Arquivo de mÃ©tricas de indexaÃ§Ã£o - converter para o formato esperado\n            reader = csv.DictReader(f)\n            converted_rows = []\n            \n            for row in reader:\n                # Converter mÃ©tricas de indexaÃ§Ã£o para o formato de context pack\n                converted_row = {\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa de tokens\n                    \"budget_tokens\": \"8000\",  # Valor padrÃ£o\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),  # Estimativa\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000)\n                }\n                converted_rows.append(converted_row)\n            \n            return converted_rows\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]\n    \n    # Calcular estatÃ­sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n    \n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n    \n    # Agrupar por dia\n    daily_stats = {}\n    for row in rows:\n        ts = parse_dt(row[\"ts\"])\n        if not ts:\n            continue\n        day = format_dt(ts)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],", "mtime": 1756154881.852048, "terms": ["with", "open", "path", "as", "detectar", "se", "arquivo", "de", "tricas", "de", "indexa", "ou", "de", "context", "pack", "first_line", "readline", "seek", "if", "query", "in", "first_line", "arquivo", "de", "tricas", "de", "context", "pack", "reader", "csv", "dictreader", "return", "row", "for", "row", "in", "reader", "else", "arquivo", "de", "tricas", "de", "indexa", "converter", "para", "formato", "esperado", "reader", "csv", "dictreader", "converted_rows", "for", "row", "in", "reader", "converter", "tricas", "de", "indexa", "para", "formato", "de", "context", "pack", "converted_row", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "de", "tokens", "budget_tokens", "valor", "padr", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "estimativa", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "converted_rows", "append", "converted_row", "return", "converted_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "daily_stats", "for", "row", "in", "rows", "ts", "parse_dt", "row", "ts", "if", "not", "ts", "continue", "day", "format_dt", "ts", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens"]}
{"chunk_id": "ba94a53ce008576b87195434", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 98, "end_line": 177, "content": "def filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r[\"ts\"]) and parse_dt(r[\"ts\"]) >= cutoff]\n    \n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in r[\"query\"].lower()]\n    \n    return rows\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]\n    \n    # Calcular estatÃ­sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n    \n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n    \n    # Agrupar por dia\n    daily_stats = {}\n    for row in rows:\n        ts = parse_dt(row[\"ts\"])\n        if not ts:\n            continue\n        day = format_dt(ts)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": []\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(row[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(row[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(row[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(row[\"latency_ms\"]))\n    \n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n    \n    # SaÃ­da JSON se solicitado\n    if args.json:", "mtime": 1756154881.852048, "terms": ["def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "ts", "and", "parse_dt", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "daily_stats", "for", "row", "in", "rows", "ts", "parse_dt", "row", "ts", "if", "not", "ts", "continue", "day", "format_dt", "ts", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "row", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "row", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "row", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "row", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json"]}
{"chunk_id": "f394fdd0b0009c22855e1267", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 108, "end_line": 187, "content": "def format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]\n    \n    # Calcular estatÃ­sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n    \n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n    \n    # Agrupar por dia\n    daily_stats = {}\n    for row in rows:\n        ts = parse_dt(row[\"ts\"])\n        if not ts:\n            continue\n        day = format_dt(ts)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": []\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(row[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(row[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(row[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(row[\"latency_ms\"]))\n    \n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n    \n    # SaÃ­da JSON se solicitado\n    if args.json:\n        output = {\n            \"summary\": {\n                \"period_start\": format_dt(first_ts) if first_ts else \"\",\n                \"period_end\": format_dt(last_ts) if last_ts else \"\",\n                \"total_queries\": total_rows,\n                \"avg_chunks\": avg_chunks,\n                \"avg_tokens\": avg_tokens,\n                \"avg_budget_utilization_pct\": avg_util,\n                \"avg_latency_ms\": avg_latency,\n                \"p95_chunks\": p95_chunks,", "mtime": 1756154881.852048, "terms": ["def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "daily_stats", "for", "row", "in", "rows", "ts", "parse_dt", "row", "ts", "if", "not", "ts", "continue", "day", "format_dt", "ts", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "row", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "row", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "row", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "row", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output", "summary", "period_start", "format_dt", "first_ts", "if", "first_ts", "else", "period_end", "format_dt", "last_ts", "if", "last_ts", "else", "total_queries", "total_rows", "avg_chunks", "avg_chunks", "avg_tokens", "avg_tokens", "avg_budget_utilization_pct", "avg_util", "avg_latency_ms", "avg_latency", "p95_chunks", "p95_chunks"]}
{"chunk_id": "098f979e02c51963a2b51109", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 111, "end_line": 190, "content": "def summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Resumo geral\n    total_rows = len(rows)\n    first_ts = min(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    last_ts = max(parse_dt(r[\"ts\"]) for r in rows if parse_dt(r[\"ts\"]))\n    \n    # Converter valores para nÃºmeros\n    chunk_counts = [coerce_int(r[\"chunk_count\"]) for r in rows]\n    total_tokens_list = [coerce_int(r[\"total_tokens\"]) for r in rows]\n    budget_utilizations = [coerce_float(r[\"budget_utilization\"]) for r in rows]\n    latencies = [coerce_float(r[\"latency_ms\"]) for r in rows]\n    \n    # Calcular estatÃ­sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n    \n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n    \n    # Agrupar por dia\n    daily_stats = {}\n    for row in rows:\n        ts = parse_dt(row[\"ts\"])\n        if not ts:\n            continue\n        day = format_dt(ts)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": []\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(row[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(row[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(row[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(row[\"latency_ms\"]))\n    \n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n    \n    # SaÃ­da JSON se solicitado\n    if args.json:\n        output = {\n            \"summary\": {\n                \"period_start\": format_dt(first_ts) if first_ts else \"\",\n                \"period_end\": format_dt(last_ts) if last_ts else \"\",\n                \"total_queries\": total_rows,\n                \"avg_chunks\": avg_chunks,\n                \"avg_tokens\": avg_tokens,\n                \"avg_budget_utilization_pct\": avg_util,\n                \"avg_latency_ms\": avg_latency,\n                \"p95_chunks\": p95_chunks,\n                \"p95_tokens\": p95_tokens,\n                \"p95_budget_utilization_pct\": p95_util,\n                \"p95_latency_ms\": p95_latency", "mtime": 1756154881.852048, "terms": ["def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "resumo", "geral", "total_rows", "len", "rows", "first_ts", "min", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "last_ts", "max", "parse_dt", "ts", "for", "in", "rows", "if", "parse_dt", "ts", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "rows", "latencies", "coerce_float", "latency_ms", "for", "in", "rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "daily_stats", "for", "row", "in", "rows", "ts", "parse_dt", "row", "ts", "if", "not", "ts", "continue", "day", "format_dt", "ts", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "row", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "row", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "row", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "row", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output", "summary", "period_start", "format_dt", "first_ts", "if", "first_ts", "else", "period_end", "format_dt", "last_ts", "if", "last_ts", "else", "total_queries", "total_rows", "avg_chunks", "avg_chunks", "avg_tokens", "avg_tokens", "avg_budget_utilization_pct", "avg_util", "avg_latency_ms", "avg_latency", "p95_chunks", "p95_chunks", "p95_tokens", "p95_tokens", "p95_budget_utilization_pct", "p95_util", "p95_latency_ms", "p95_latency"]}
{"chunk_id": "76363d0cf64a818e78f4e154", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 137, "end_line": 216, "content": "    \n    # Agrupar por dia\n    daily_stats = {}\n    for row in rows:\n        ts = parse_dt(row[\"ts\"])\n        if not ts:\n            continue\n        day = format_dt(ts)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": []\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(row[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(row[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(row[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(row[\"latency_ms\"]))\n    \n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n    \n    # SaÃ­da JSON se solicitado\n    if args.json:\n        output = {\n            \"summary\": {\n                \"period_start\": format_dt(first_ts) if first_ts else \"\",\n                \"period_end\": format_dt(last_ts) if last_ts else \"\",\n                \"total_queries\": total_rows,\n                \"avg_chunks\": avg_chunks,\n                \"avg_tokens\": avg_tokens,\n                \"avg_budget_utilization_pct\": avg_util,\n                \"avg_latency_ms\": avg_latency,\n                \"p95_chunks\": p95_chunks,\n                \"p95_tokens\": p95_tokens,\n                \"p95_budget_utilization_pct\": p95_util,\n                \"p95_latency_ms\": p95_latency\n            },\n            \"daily\": daily_rows\n        }\n        print(json.dumps(output, indent=2))\n        return\n    \n    # SaÃ­da em texto formatado\n    print(\"=== RESUMO DE MÃ‰TRICAS DO MCP ===\")\n    print(f\"PerÃ­odo: {format_dt(first_ts) if first_ts else 'N/A'} a {format_dt(last_ts) if last_ts else 'N/A'} ({total_rows} consultas)\")\n    print()\n    print(\"MÃ©dia Geral:\")\n    print(f\"  Chunks:     {avg_chunks:.1f} (p95: {p95_chunks:.1f})\")\n    print(f\"  Tokens:     {avg_tokens:.0f} (p95: {p95_tokens:.0f})\")\n    print(f\"  UtilizaÃ§Ã£o: {avg_util:.1f}% (p95: {p95_util:.1f}%)\")\n    print(f\"  LatÃªncia:   {avg_latency:.1f}ms (p95: {p95_latency:.1f}ms)\")\n    print()\n    print(\"DiÃ¡rio:\")\n    print(f\"{'Dia':<12} {'Chunks':<24} {'Tokens':<24} {'UtilizaÃ§Ã£o':<24} {'LatÃªncia (ms)':<24}\")\n    print(f\"{'':<12} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24}\")\n    print(\"-\" * 100)\n    \n    for row in daily_rows:\n        chunks_str = f\"{row['avg_chunks']:.1f}/{row['median_chunks']:.1f}/{row['p95_chunks']:.1f}\"\n        tokens_str = f\"{row['avg_tokens']:.0f}/{row['median_tokens']:.0f}/{row['p95_tokens']:.0f}\"\n        util_str = f\"{row['avg_util']:.1f}/{row['median_util']:.1f}/{row['p95_util']:.1f}\"\n        latency_str = f\"{row['avg_latency']:.1f}/{row['median_latency']:.1f}/{row['p95_latency']:.1f}\"", "mtime": 1756154881.852048, "terms": ["agrupar", "por", "dia", "daily_stats", "for", "row", "in", "rows", "ts", "parse_dt", "row", "ts", "if", "not", "ts", "continue", "day", "format_dt", "ts", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "row", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "row", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "row", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "row", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output", "summary", "period_start", "format_dt", "first_ts", "if", "first_ts", "else", "period_end", "format_dt", "last_ts", "if", "last_ts", "else", "total_queries", "total_rows", "avg_chunks", "avg_chunks", "avg_tokens", "avg_tokens", "avg_budget_utilization_pct", "avg_util", "avg_latency_ms", "avg_latency", "p95_chunks", "p95_chunks", "p95_tokens", "p95_tokens", "p95_budget_utilization_pct", "p95_util", "p95_latency_ms", "p95_latency", "daily", "daily_rows", "print", "json", "dumps", "output", "indent", "return", "sa", "da", "em", "texto", "formatado", "print", "resumo", "de", "tricas", "do", "mcp", "print", "per", "odo", "format_dt", "first_ts", "if", "first_ts", "else", "format_dt", "last_ts", "if", "last_ts", "else", "total_rows", "consultas", "print", "print", "dia", "geral", "print", "chunks", "avg_chunks", "p95", "p95_chunks", "print", "tokens", "avg_tokens", "p95", "p95_tokens", "print", "utiliza", "avg_util", "p95", "p95_util", "print", "lat", "ncia", "avg_latency", "ms", "p95", "p95_latency", "ms", "print", "print", "di", "rio", "print", "dia", "chunks", "tokens", "utiliza", "lat", "ncia", "ms", "print", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "print", "for", "row", "in", "daily_rows", "chunks_str", "row", "avg_chunks", "row", "median_chunks", "row", "p95_chunks", "tokens_str", "row", "avg_tokens", "row", "median_tokens", "row", "p95_tokens", "util_str", "row", "avg_util", "row", "median_util", "row", "p95_util", "latency_str", "row", "avg_latency", "row", "median_latency", "row", "p95_latency"]}
{"chunk_id": "ba294a550d92826da87c73b9", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 205, "end_line": 238, "content": "    print(f\"  LatÃªncia:   {avg_latency:.1f}ms (p95: {p95_latency:.1f}ms)\")\n    print()\n    print(\"DiÃ¡rio:\")\n    print(f\"{'Dia':<12} {'Chunks':<24} {'Tokens':<24} {'UtilizaÃ§Ã£o':<24} {'LatÃªncia (ms)':<24}\")\n    print(f\"{'':<12} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24}\")\n    print(\"-\" * 100)\n    \n    for row in daily_rows:\n        chunks_str = f\"{row['avg_chunks']:.1f}/{row['median_chunks']:.1f}/{row['p95_chunks']:.1f}\"\n        tokens_str = f\"{row['avg_tokens']:.0f}/{row['median_tokens']:.0f}/{row['p95_tokens']:.0f}\"\n        util_str = f\"{row['avg_util']:.1f}/{row['median_util']:.1f}/{row['p95_util']:.1f}\"\n        latency_str = f\"{row['avg_latency']:.1f}/{row['median_latency']:.1f}/{row['p95_latency']:.1f}\"\n        \n        print(f\"{row['day']:<12} {chunks_str:<24} {tokens_str:<24} {util_str:<24} {latency_str:<24}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP\")\n    parser.add_argument(\"--file\", default=str(CURRENT_DIR / \".mcp_index/metrics.csv\"), help=\"Arquivo CSV de mÃ©tricas\")\n    parser.add_argument(\"--since\", type=int, default=0, help=\"Filtrar por dias recentes\")\n    parser.add_argument(\"--filter\", default=\"\", help=\"Filtrar por termo na query\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"SaÃ­da em JSON\")\n    \n    args = parser.parse_args()\n    \n    try:\n        rows = load_csv(args.file)\n        rows = filter_rows(rows, args.since, args.filter)\n        summarize(rows, args)\n    except Exception as e:\n        print(f\"[erro] {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1756154881.852048, "terms": ["print", "lat", "ncia", "avg_latency", "ms", "p95", "p95_latency", "ms", "print", "print", "di", "rio", "print", "dia", "chunks", "tokens", "utiliza", "lat", "ncia", "ms", "print", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "print", "for", "row", "in", "daily_rows", "chunks_str", "row", "avg_chunks", "row", "median_chunks", "row", "p95_chunks", "tokens_str", "row", "avg_tokens", "row", "median_tokens", "row", "p95_tokens", "util_str", "row", "avg_util", "row", "median_util", "row", "p95_util", "latency_str", "row", "avg_latency", "row", "median_latency", "row", "p95_latency", "print", "row", "day", "chunks_str", "tokens_str", "util_str", "latency_str", "def", "main", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "parser", "add_argument", "file", "default", "str", "current_dir", "mcp_index", "metrics", "csv", "help", "arquivo", "csv", "de", "tricas", "parser", "add_argument", "since", "type", "int", "default", "help", "filtrar", "por", "dias", "recentes", "parser", "add_argument", "filter", "default", "help", "filtrar", "por", "termo", "na", "query", "parser", "add_argument", "json", "action", "store_true", "help", "sa", "da", "em", "json", "args", "parser", "parse_args", "try", "rows", "load_csv", "args", "file", "rows", "filter_rows", "rows", "args", "since", "args", "filter", "summarize", "rows", "args", "except", "exception", "as", "print", "erro", "sys", "exit", "if", "__name__", "__main__", "main"]}
{"chunk_id": "854347f75e00ec94dac3f0ed", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 220, "end_line": 238, "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Resumo de mÃ©tricas do MCP\")\n    parser.add_argument(\"--file\", default=str(CURRENT_DIR / \".mcp_index/metrics.csv\"), help=\"Arquivo CSV de mÃ©tricas\")\n    parser.add_argument(\"--since\", type=int, default=0, help=\"Filtrar por dias recentes\")\n    parser.add_argument(\"--filter\", default=\"\", help=\"Filtrar por termo na query\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"SaÃ­da em JSON\")\n    \n    args = parser.parse_args()\n    \n    try:\n        rows = load_csv(args.file)\n        rows = filter_rows(rows, args.since, args.filter)\n        summarize(rows, args)\n    except Exception as e:\n        print(f\"[erro] {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1756154881.852048, "terms": ["def", "main", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "parser", "add_argument", "file", "default", "str", "current_dir", "mcp_index", "metrics", "csv", "help", "arquivo", "csv", "de", "tricas", "parser", "add_argument", "since", "type", "int", "default", "help", "filtrar", "por", "dias", "recentes", "parser", "add_argument", "filter", "default", "help", "filtrar", "por", "termo", "na", "query", "parser", "add_argument", "json", "action", "store_true", "help", "sa", "da", "em", "json", "args", "parser", "parse_args", "try", "rows", "load_csv", "args", "file", "rows", "filter_rows", "rows", "args", "since", "args", "filter", "summarize", "rows", "args", "except", "exception", "as", "print", "erro", "sys", "exit", "if", "__name__", "__main__", "main"]}
{"chunk_id": "14821db9c5d23b2b01aa2daf", "file_path": "mcp_system/scripts/mcp_client_stats.py", "start_line": 1, "end_line": 51, "content": "#!/usr/bin/env python3\n\"\"\"\nCliente MCP para obter estatÃ­sticas do sistema\n\"\"\"\n\nimport json\nimport asyncio\nimport os\nimport time\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def get_stats():\n    \"\"\"ObtÃ©m estatÃ­sticas do servidor MCP\"\"\"\n    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[\"-u\", \"-m\", \"mcp_system.mcp_server_enhanced\"],\n        env={\n            \"INDEX_DIR\": os.path.join(base_dir, \".mcp_index\"),\n            \"INDEX_ROOT\": os.path.abspath(os.path.join(base_dir, '..'))\n        }\n    )\n\n    max_retries = 10\n    retry_delay = 1  # segundos\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            for attempt in range(max_retries):\n                try:\n                    tools = await session.list_tools()\n                    print(\"Ferramentas disponÃ­veis:\")\n                    for tool in tools:\n                        print(f\"  - {tool.name}: {tool.description}\")\n\n                    print(\"\\nExecutando comando get_stats...\")\n                    result = await session.call_tool(\"get_stats\", {})\n                    print(\"Resultado:\")\n                    print(json.dumps(result, indent=2, ensure_ascii=False))\n                    break\n                except Exception as e:\n                    print(f\"Tentativa {attempt+1} falhou: {e}\")\n                    if attempt == max_retries - 1:\n                        print(\"Falha ao executar comando apÃ³s vÃ¡rias tentativas.\")\n                        break\n                    await asyncio.sleep(retry_delay)\n\nif __name__ == \"__main__\":\n    asyncio.run(get_stats())", "mtime": 1756157736.4330692, "terms": ["usr", "bin", "env", "python3", "cliente", "mcp", "para", "obter", "estat", "sticas", "do", "sistema", "import", "json", "import", "asyncio", "import", "os", "import", "time", "from", "mcp", "import", "clientsession", "stdioserverparameters", "from", "mcp", "client", "stdio", "import", "stdio_client", "async", "def", "get_stats", "obt", "estat", "sticas", "do", "servidor", "mcp", "base_dir", "os", "path", "abspath", "os", "path", "join", "os", "path", "dirname", "__file__", "server_params", "stdioserverparameters", "command", "python", "args", "mcp_system", "mcp_server_enhanced", "env", "index_dir", "os", "path", "join", "base_dir", "mcp_index", "index_root", "os", "path", "abspath", "os", "path", "join", "base_dir", "max_retries", "retry_delay", "segundos", "async", "with", "stdio_client", "server_params", "as", "read", "write", "async", "with", "clientsession", "read", "write", "as", "session", "for", "attempt", "in", "range", "max_retries", "try", "tools", "await", "session", "list_tools", "print", "ferramentas", "dispon", "veis", "for", "tool", "in", "tools", "print", "tool", "name", "tool", "description", "print", "nexecutando", "comando", "get_stats", "result", "await", "session", "call_tool", "get_stats", "print", "resultado", "print", "json", "dumps", "result", "indent", "ensure_ascii", "false", "break", "except", "exception", "as", "print", "tentativa", "attempt", "falhou", "if", "attempt", "max_retries", "print", "falha", "ao", "executar", "comando", "ap", "rias", "tentativas", "break", "await", "asyncio", "sleep", "retry_delay", "if", "__name__", "__main__", "asyncio", "run", "get_stats"]}
{"chunk_id": "9e88b4f970088b5939f4ff2f", "file_path": "mcp_system/scripts/get_stats.py", "start_line": 1, "end_line": 34, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para obter estatÃ­sticas do sistema MCP\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\n\ndef get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estatÃ­sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisiÃ§Ã£o para o servidor MCP...\")\n    print(f\"RequisiÃ§Ã£o: {request_str}\")\n    \n    # Aqui vocÃª precisaria se conectar ao servidor MCP\n    # Esta Ã© uma implementaÃ§Ã£o simplificada\n    print(\"\\nPara executar este comando, vocÃª pode:\")\n    print(\"1. Usar a interface do VS Code com a extensÃ£o MCP\")\n    print(\"2. Enviar a requisiÃ§Ã£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["usr", "bin", "env", "python3", "script", "para", "obter", "estat", "sticas", "do", "sistema", "mcp", "import", "json", "import", "subprocess", "import", "sys", "def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "a3cb00211e2d5d818758fc89", "file_path": "mcp_system/scripts/get_stats.py", "start_line": 10, "end_line": 34, "content": "def get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estatÃ­sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisiÃ§Ã£o para o servidor MCP...\")\n    print(f\"RequisiÃ§Ã£o: {request_str}\")\n    \n    # Aqui vocÃª precisaria se conectar ao servidor MCP\n    # Esta Ã© uma implementaÃ§Ã£o simplificada\n    print(\"\\nPara executar este comando, vocÃª pode:\")\n    print(\"1. Usar a interface do VS Code com a extensÃ£o MCP\")\n    print(\"2. Enviar a requisiÃ§Ã£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "4eb28aca90a9e99b1efae389", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 1, "end_line": 80, "content": "# mcp_server_enhanced.py\n\"\"\"\nServidor MCP melhorado com busca semÃ¢ntica e auto-indexaÃ§Ã£o\nUsando FastMCP para API simplificada com decorators\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, List\nimport pathlib\nimport threading\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from mcp.server.fastmcp import FastMCP\n    HAS_MCP = True\n    HAS_FASTMCP = True\nexcept ImportError:\n    try:\n        # Fallback para versÃµes mais antigas\n        from mcp.server import Server\n        from mcp import types\n        HAS_MCP = True\n        HAS_FASTMCP = False\n    except ImportError:\n        sys.stderr.write(\"[mcp_server_enhanced] ERROR: MCP SDK nÃ£o encontrado. Instale `mcp`.\\n\")\n        raise\n\n# Importa funcionalidades melhoradas\ntry:\n    from .code_indexer_enhanced import (\n        EnhancedCodeIndexer,\n        enhanced_search_code,\n        enhanced_build_context_pack,\n        enhanced_index_repo_paths,\n        BaseCodeIndexer,\n        search_code,\n        build_context_pack,\n        index_repo_paths\n    )\n    HAS_ENHANCED = True\n    sys.stderr.write(\"[mcp_server_enhanced] âœ… Funcionalidades melhoradas carregadas\\n\")\nexcept ImportError as e:\n    sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Funcionalidades melhoradas nÃ£o disponÃ­veis: {e}\\n\")\n    sys.stderr.write(\"[mcp_server_enhanced] ðŸ”„ Usando versÃ£o base integrada\\n\")\n\n    # Fallback para versÃ£o base integrada\n    try:\n        from .code_indexer_enhanced import (\n            BaseCodeIndexer,\n            search_code,\n            build_context_pack,\n            index_repo_paths\n        )\n        HAS_ENHANCED = False\n    except ImportError as e2:\n        sys.stderr.write(f\"[mcp_server_enhanced] âŒ Erro crÃ­tico: {e2}\\n\")\n        raise\n\nHAS_ENHANCED_FEATURES = HAS_ENHANCED\n\n# Config / instÃ¢ncias - agora usando caminhos relativos Ã  pasta mcp_system\nINDEX_DIR = os.environ.get(\"INDEX_DIR\", str(CURRENT_DIR / \".mcp_index\"))\nINDEX_ROOT = os.environ.get(\"INDEX_ROOT\", str(CURRENT_DIR.parent))\n\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXAÃ‡ÃƒO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default", "mtime": 1756159310.238677, "terms": ["mcp_server_enhanced", "py", "servidor", "mcp", "melhorado", "com", "busca", "sem", "ntica", "auto", "indexa", "usando", "fastmcp", "para", "api", "simplificada", "com", "decorators", "import", "os", "import", "sys", "from", "typing", "import", "any", "dict", "list", "import", "pathlib", "import", "threading", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "mcp", "server", "fastmcp", "import", "fastmcp", "has_mcp", "true", "has_fastmcp", "true", "except", "importerror", "try", "fallback", "para", "vers", "es", "mais", "antigas", "from", "mcp", "server", "import", "server", "from", "mcp", "import", "types", "has_mcp", "true", "has_fastmcp", "false", "except", "importerror", "sys", "stderr", "write", "mcp_server_enhanced", "error", "mcp", "sdk", "encontrado", "instale", "mcp", "raise", "importa", "funcionalidades", "melhoradas", "try", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "true", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "carregadas", "except", "importerror", "as", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "dispon", "veis", "sys", "stderr", "write", "mcp_server_enhanced", "usando", "vers", "base", "integrada", "fallback", "para", "vers", "base", "integrada", "try", "from", "code_indexer_enhanced", "import", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "false", "except", "importerror", "as", "e2", "sys", "stderr", "write", "mcp_server_enhanced", "erro", "cr", "tico", "e2", "raise", "has_enhanced_features", "has_enhanced", "config", "inst", "ncias", "agora", "usando", "caminhos", "relativos", "pasta", "mcp_system", "index_dir", "os", "environ", "get", "index_dir", "str", "current_dir", "mcp_index", "index_root", "os", "environ", "get", "index_root", "str", "current_dir", "parent", "if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default"]}
{"chunk_id": "3ed2d29355a5015c74d0533c", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 69, "end_line": 148, "content": "    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXAÃ‡ÃƒO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n", "mtime": 1756159310.238677, "terms": ["_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive"]}
{"chunk_id": "06a9a4ab8dc5948245283abe", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 78, "end_line": 157, "content": "def _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")", "mtime": 1756159310.238677, "terms": ["def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr"]}
{"chunk_id": "cd944327b2d0ba0b5795bfb5", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 90, "end_line": 169, "content": "def _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] ðŸš€ Iniciando indexaÃ§Ã£o automÃ¡tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        if HAS_ENHANCED_FEATURES:\n            enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] âœ… IndexaÃ§Ã£o inicial concluÃ­da\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ Falha na indexaÃ§Ã£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)", "mtime": 1756159310.238677, "terms": ["def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "if", "has_enhanced_features", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit"]}
{"chunk_id": "f71d375002283832e849e3e8", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 121, "end_line": 200, "content": "def _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"ðŸ” [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        ", "mtime": 1756159310.238677, "terms": ["def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget"]}
{"chunk_id": "0482d9fdc30979affcccafd0", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 137, "end_line": 216, "content": "            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"âŒ [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):", "mtime": 1756159310.238677, "terms": ["if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing"]}
{"chunk_id": "b734a513ea7089a617aafedf", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 155, "end_line": 234, "content": "def _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar cÃ³digo\"\"\"\n    sys.stderr.write(f\"ðŸ” [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:", "mtime": 1756159310.238677, "terms": ["def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as"]}
{"chunk_id": "84543f82d731779d2efbd565", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 181, "end_line": 260, "content": "def _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"ðŸ“¦ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"", "mtime": 1756159310.238677, "terms": ["def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache"]}
{"chunk_id": "14e7baec22391cb9dc0c3a3e", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 205, "end_line": 284, "content": "        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"âŒ [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:", "mtime": 1756159310.238677, "terms": ["except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp"]}
{"chunk_id": "09ccd7023556abcec87af9c7", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 211, "end_line": 290, "content": "def _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexaÃ§Ã£o\"\"\"\n    sys.stderr.write(f\"ðŸ‘ï¸  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexaÃ§Ã£o nÃ£o disponÃ­vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'AÃ§Ã£o invÃ¡lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()", "mtime": 1756159310.238677, "terms": ["def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start"]}
{"chunk_id": "84720995a187992751e1ee26", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 237, "end_line": 316, "content": "def _handle_get_stats():\n    \"\"\"Handler para obter estatÃ­sticas\"\"\"\n    sys.stderr.write(\"ðŸ“Š [get_stats] Coletando estatÃ­sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informaÃ§Ãµes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,", "mtime": 1756159310.238677, "terms": ["def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int"]}
{"chunk_id": "58d36d563c79a143f7ceef8f", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 259, "end_line": 338, "content": "def _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gestÃ£o de cache\"\"\"\n    sys.stderr.write(f\"ðŸ—„ï¸  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui vocÃª pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)", "mtime": 1756159310.238677, "terms": ["def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true"]}
{"chunk_id": "0b819732e715cca72976ddc4", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 273, "end_line": 352, "content": "        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background para nÃ£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    # ===== TOOLS BÃSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:", "mtime": 1756159310.238677, "terms": ["elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as"]}
{"chunk_id": "377ce7e1c944fed65f092d94", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 297, "end_line": 376, "content": "    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de cÃ³digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",", "mtime": 1756159310.238677, "terms": ["def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object"]}
{"chunk_id": "7762aea7857111a7775f6786", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 306, "end_line": 385, "content": "    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),", "mtime": 1756159310.238677, "terms": ["def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query"]}
{"chunk_id": "e725c6cb95bab693d7bd8d54", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 314, "end_line": 393, "content": "    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},", "mtime": 1756159310.238677, "terms": ["def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default"]}
{"chunk_id": "72554f1c7456ac96b599260b", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 322, "end_line": 401, "content": "    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",", "mtime": 1756159310.238677, "terms": ["def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index"]}
{"chunk_id": "ee12afbba9a205037e1911e9", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 327, "end_line": 406, "content": "    def get_stats() -> dict:\n        \"\"\"ObtÃ©m estatÃ­sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},", "mtime": 1756159310.238677, "terms": ["def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status"]}
{"chunk_id": "c4503e2b068c745b5e42032b", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 332, "end_line": 411, "content": "    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; jÃ¡ iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),", "mtime": 1756159310.238677, "terms": ["def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true"]}
{"chunk_id": "a2bf46cea1cdbe1df367c47c", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 341, "end_line": 420, "content": "\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexaÃ§Ã£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] âš ï¸ NÃ£o foi possÃ­vel iniciar thread de indexaÃ§Ã£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de cÃ³digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca cÃ³digo usando busca hÃ­brida (BM25 + semÃ¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexaÃ§Ã£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                }\n            ),\n            Tool(", "mtime": 1756159310.238677, "terms": ["else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool"]}
{"chunk_id": "c24d36c0729cdd6282e36696", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 409, "end_line": 488, "content": "                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"ObtÃ©m estatÃ­sticas do indexador\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                }\n            ),\n            Tool(\n                name=\"cache_management\",\n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"cache_type\": {\"type\": \"string\", \"default\": \"all\"}\n                    }\n                }\n            )\n        ]\n\n    @server.call_tool()\n    async def call_tool(name: str, arguments: Dict[str, Any] = None):\n        arguments = arguments or {}\n        try:\n            if name == \"index_path\":\n                return _handle_index_path(\n                    arguments.get(\"path\", \".\"),\n                    arguments.get(\"recursive\", True),\n                    arguments.get(\"enable_semantic\", True),\n                    arguments.get(\"auto_start_watcher\", False),\n                    arguments.get(\"exclude_globs\", None)\n                )\n            elif name == \"search_code\":\n                return _handle_search_code(\n                    arguments.get(\"query\"),\n                    arguments.get(\"limit\", 10),\n                    arguments.get(\"semantic_weight\", 0.3),\n                    arguments.get(\"use_mmr\", True)\n                )\n            elif name == \"context_pack\":\n                return _handle_context_pack(\n                    arguments.get(\"query\"),\n                    arguments.get(\"token_budget\", 8000),\n                    arguments.get(\"max_chunks\", 20),\n                    arguments.get(\"strategy\", \"mmr\")\n                )\n            elif name == \"auto_index\":\n                return _handle_auto_index(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"paths\", None),\n                    arguments.get(\"recursive\", True)\n                )\n            elif name == \"get_stats\":\n                return _handle_get_stats()\n            elif name == \"cache_management\":\n                return _handle_cache_management(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"cache_type\", \"all\")\n                )\n            else:\n                return {\"status\": \"error\", \"error\": f\"Ferramenta desconhecida: {name}\"}\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n\n    # Iniciar servidor\n    async def main():\n        # Removido: jÃ¡ iniciamos a thread antes\n        # threading.Thread(target=_initial_index, daemon=True).start()\n        async with stdio_server() as (read_stream, write_stream):\n            await server.run(read_stream, write_stream, server_name=\"code-indexer-enhanced\")\n\n    if __name__ == \"__main__\":\n        import asyncio\n        asyncio.run(main())\n\nif HAS_FASTMCP and __name__ == \"__main__\":", "mtime": 1756159310.238677, "terms": ["tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "cache_type", "type", "string", "default", "all", "server", "call_tool", "async", "def", "call_tool", "name", "str", "arguments", "dict", "str", "any", "none", "arguments", "arguments", "or", "try", "if", "name", "index_path", "return", "_handle_index_path", "arguments", "get", "path", "arguments", "get", "recursive", "true", "arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "none", "elif", "name", "search_code", "return", "_handle_search_code", "arguments", "get", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack", "return", "_handle_context_pack", "arguments", "get", "query", "arguments", "get", "token_budget", "arguments", "get", "max_chunks", "arguments", "get", "strategy", "mmr", "elif", "name", "auto_index", "return", "_handle_auto_index", "arguments", "get", "action", "status", "arguments", "get", "paths", "none", "arguments", "get", "recursive", "true", "elif", "name", "get_stats", "return", "_handle_get_stats", "elif", "name", "cache_management", "return", "_handle_cache_management", "arguments", "get", "action", "status", "arguments", "get", "cache_type", "all", "else", "return", "status", "error", "error", "ferramenta", "desconhecida", "name", "except", "exception", "as", "return", "status", "error", "error", "str", "iniciar", "servidor", "async", "def", "main", "removido", "iniciamos", "thread", "antes", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "async", "with", "stdio_server", "as", "read_stream", "write_stream", "await", "server", "run", "read_stream", "write_stream", "server_name", "code", "indexer", "enhanced", "if", "__name__", "__main__", "import", "asyncio", "asyncio", "run", "main", "if", "has_fastmcp", "and", "__name__", "__main__"]}
{"chunk_id": "447febdbf67ab906cb0ab6b3", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 477, "end_line": 490, "content": "    # Iniciar servidor\n    async def main():\n        # Removido: jÃ¡ iniciamos a thread antes\n        # threading.Thread(target=_initial_index, daemon=True).start()\n        async with stdio_server() as (read_stream, write_stream):\n            await server.run(read_stream, write_stream, server_name=\"code-indexer-enhanced\")\n\n    if __name__ == \"__main__\":\n        import asyncio\n        asyncio.run(main())\n\nif HAS_FASTMCP and __name__ == \"__main__\":\n    # Executa o servidor FastMCP quando disponÃ­vel\n    mcp.run()", "mtime": 1756159310.238677, "terms": ["iniciar", "servidor", "async", "def", "main", "removido", "iniciamos", "thread", "antes", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "async", "with", "stdio_server", "as", "read_stream", "write_stream", "await", "server", "run", "read_stream", "write_stream", "server_name", "code", "indexer", "enhanced", "if", "__name__", "__main__", "import", "asyncio", "asyncio", "run", "main", "if", "has_fastmcp", "and", "__name__", "__main__", "executa", "servidor", "fastmcp", "quando", "dispon", "vel", "mcp", "run"]}
{"chunk_id": "b9636576c3f6398c865c908f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca semÃ¢ntica usando embeddings locais\nIntegraÃ§Ã£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"", "mtime": 1756159523.0917702, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray"]}
{"chunk_id": "b8b1d69a30fa5b0deb88f3e2", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 28, "end_line": 107, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding", "mtime": 1756159523.0917702, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding"]}
{"chunk_id": "29e2f61fabb71fabbff0e981", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 36, "end_line": 115, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:", "mtime": 1756159523.0917702, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try"]}
{"chunk_id": "a7528c63cedd472b9041d747", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 42, "end_line": 121, "content": "    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)", "mtime": 1756159523.0917702, "terms": ["def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding"]}
{"chunk_id": "19183dee28a3636a3536760f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 55, "end_line": 134, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding", "mtime": 1756159523.0917702, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding"]}
{"chunk_id": "3847dbc31a8035d68aa21716", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 67, "end_line": 146, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1756159523.0917702, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "0cc3531171f0e14ae94cd05d", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        ", "mtime": 1756159523.0917702, "terms": ["return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query"]}
{"chunk_id": "a302c2916f1efa0570a0d50f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 71, "end_line": 150, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta", "mtime": 1756159523.0917702, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "0307af24c1cd48765e235ca5", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 75, "end_line": 154, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:", "mtime": 1756159523.0917702, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "061c08d184bbd95270d2a603", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 79, "end_line": 158, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n        \n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []", "mtime": 1756159523.0917702, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return"]}
{"chunk_id": "bbc0a601567325a65221fde3", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado", "mtime": 1756159523.0917702, "terms": ["return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado"]}
{"chunk_id": "d8898f2074cfefb35c0bab94", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 144, "end_line": 223, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o", "mtime": 1756159523.0917702, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele"]}
{"chunk_id": "49e976b225710940dc35d1e5", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 183, "end_line": 262, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1756159523.0917702, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "3315f53365ed97a06a4cb9e1", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))", "mtime": 1756159523.0917702, "terms": ["chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy"]}
{"chunk_id": "d1dc50099c603fe0ed7a001a", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 268, "end_line": 305, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159523.0917702, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "d96547fa68451b78ca325f4c", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 273, "end_line": 305, "content": "        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159523.0917702, "terms": ["if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "11cc18ca0d143f29ae3b82a7", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 282, "end_line": 305, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159523.0917702, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "a63d6dcf2514296b12ce2285", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 297, "end_line": 305, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159523.0917702, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "f33e9104b86d272a7e2afc9a", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca semÃ¢ntica usando embeddings locais\nIntegraÃ§Ã£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nimport pathlib\n\n# Obter o diretÃ³rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"", "mtime": 1756159582.9433653, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray"]}
{"chunk_id": "16e5d8070d1eaddb4b003a54", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 28, "end_line": 107, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                ", "mtime": 1756159582.9433653, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load"]}
{"chunk_id": "f356fbce42cd73f0ff72e1d4", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 36, "end_line": 115, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca semÃ¢ntica que combina embeddings com BM25\n    para busca hÃ­brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys", "mtime": 1756159582.9433653, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys"]}
{"chunk_id": "ad95d53b51a170160d5bf8bc", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 42, "end_line": 121, "content": "    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: nÃ£o carregar o modelo no __init__\n        # Se sentence-transformers nÃ£o estiver disponÃ­vel, os mÃ©todos farÃ£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content", "mtime": 1756159582.9433653, "terms": ["def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content"]}
{"chunk_id": "eb3cf6f084fc7029e4e4f382", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 55, "end_line": 134, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"ðŸ”„ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"âœ… Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"âŒ Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:", "mtime": 1756159582.9433653, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as"]}
{"chunk_id": "de4f156e5fe9ac181b58059a", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 67, "end_line": 146, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None", "mtime": 1756159582.9433653, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none"]}
{"chunk_id": "ee7737b2f63e82c7590f4881", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],", "mtime": 1756159582.9433653, "terms": ["return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray"]}
{"chunk_id": "dde80ca94ef8afafe26fdfdf", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 71, "end_line": 150, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1756159582.9433653, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "6c08fb627e8e7e4958a6e4a0", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 75, "end_line": 154, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conteÃºdo para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n\n        Args:\n            query: Texto da consulta", "mtime": 1756159582.9433653, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "a4e1cbf78511cdaef8128408", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 79, "end_line": 158, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        ObtÃ©m embedding para um chunk, usando cache quando possÃ­vel\n\n        Args:\n            chunk_id: Identificador Ãºnico do chunk\n            content: ConteÃºdo do chunk para gerar embedding\n            force_regenerate: Se True, forÃ§a regeneraÃ§Ã£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se nÃ£o forÃ§ar regeneraÃ§Ã£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conteÃºdo nÃ£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"âš ï¸  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conteÃºdo para nÃ£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n\n        Returns:", "mtime": 1756159582.9433653, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "6c638b70a72f80d09b7a0134", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            # Atualiza cache em memÃ³ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n\n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return []\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return []\n\n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n\n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n\n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n\n            return similarities[:top_k]\n\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,", "mtime": 1756159582.9433653, "terms": ["atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "return", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score"]}
{"chunk_id": "dfcb1dbe94678fd02f73e4ed", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 148, "end_line": 227, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente Ã  query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: NÃºmero mÃ¡ximo de resultados\n\n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return []\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return []\n\n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n\n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n\n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n\n            return similarities[:top_k]\n\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"âŒ Erro na busca semÃ¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []", "mtime": 1756159582.9433653, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "return", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return"]}
{"chunk_id": "3b86c3ec936843eb966239af", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 191, "end_line": 270, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca semÃ¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade semÃ¢ntica (0-1)\n            top_k: NÃºmero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1756159582.9433653, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "9461bfdfb0ea529e4db7fda5", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo estÃ¡ carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # ObtÃ©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor seleÃ§Ã£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca semÃ¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()", "mtime": 1756159582.9433653, "terms": ["returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink"]}
{"chunk_id": "88d435b73dc3de43f9321f60", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 273, "end_line": 313, "content": "        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "a36cb181613158755970a97f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 276, "end_line": 313, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk especÃ­fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em memÃ³ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "65018965f1f33654849aab73", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 290, "end_line": 313, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estatÃ­sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "03511a5eb7501204553fbd7d", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 305, "end_line": 313, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
