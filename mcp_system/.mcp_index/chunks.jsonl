{"chunk_id": "525fdfff378cc1ca15548144", "file_path": "mcp_server_enhanced.py", "start_line": 1, "end_line": 80, "content": "# mcp_server_enhanced.py\n\"\"\"\nServidor MCP melhorado com busca sem√¢ntica e auto-indexa√ß√£o\nUsando FastMCP para API simplificada com decorators\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, List\n\ntry:\n    from mcp.server.fastmcp import FastMCP\n    HAS_MCP = True\n    HAS_FASTMCP = True\nexcept ImportError:\n    try:\n        # Fallback para vers√µes mais antigas\n        from mcp.server import Server\n        from mcp import types\n        HAS_MCP = True\n        HAS_FASTMCP = False\n    except ImportError:\n        sys.stderr.write(\"[mcp_server_enhanced] ERROR: MCP SDK n√£o encontrado. Instale `mcp`.\\n\")\n        raise\n\n# Importa funcionalidades melhoradas\ntry:\n    from code_indexer_enhanced import (\n        EnhancedCodeIndexer,\n        enhanced_search_code,\n        enhanced_build_context_pack,\n        enhanced_index_repo_paths,\n        BaseCodeIndexer,\n        search_code,\n        build_context_pack,\n        index_repo_paths\n    )\n    HAS_ENHANCED = True\n    sys.stderr.write(\"[mcp_server_enhanced] ‚úÖ Funcionalidades melhoradas carregadas\\n\")\nexcept ImportError as e:\n    sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Funcionalidades melhoradas n√£o dispon√≠veis: {e}\\n\")\n    sys.stderr.write(\"[mcp_server_enhanced] üîÑ Usando vers√£o base integrada\\n\")\n\n    # Fallback para vers√£o base integrada\n    try:\n        from code_indexer_enhanced import (\n            BaseCodeIndexer,\n            search_code,\n            build_context_pack,\n            index_repo_paths\n        )\n        HAS_ENHANCED = False\n    except ImportError as e2:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ùå Erro cr√≠tico: {e2}\\n\")\n        raise\n\nHAS_ENHANCED_FEATURES = HAS_ENHANCED\n\n# Config / inst√¢ncias\nINDEX_DIR = os.environ.get(\"INDEX_DIR\", \".mcp_index\")\nINDEX_ROOT = os.environ.get(\"INDEX_ROOT\", os.getcwd())\n\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)", "mtime": 1755725603.7595248, "terms": ["mcp_server_enhanced", "py", "servidor", "mcp", "melhorado", "com", "busca", "sem", "ntica", "auto", "indexa", "usando", "fastmcp", "para", "api", "simplificada", "com", "decorators", "import", "os", "import", "sys", "from", "typing", "import", "any", "dict", "list", "try", "from", "mcp", "server", "fastmcp", "import", "fastmcp", "has_mcp", "true", "has_fastmcp", "true", "except", "importerror", "try", "fallback", "para", "vers", "es", "mais", "antigas", "from", "mcp", "server", "import", "server", "from", "mcp", "import", "types", "has_mcp", "true", "has_fastmcp", "false", "except", "importerror", "sys", "stderr", "write", "mcp_server_enhanced", "error", "mcp", "sdk", "encontrado", "instale", "mcp", "raise", "importa", "funcionalidades", "melhoradas", "try", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "true", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "carregadas", "except", "importerror", "as", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "dispon", "veis", "sys", "stderr", "write", "mcp_server_enhanced", "usando", "vers", "base", "integrada", "fallback", "para", "vers", "base", "integrada", "try", "from", "code_indexer_enhanced", "import", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "false", "except", "importerror", "as", "e2", "sys", "stderr", "write", "mcp_server_enhanced", "erro", "cr", "tico", "e2", "raise", "has_enhanced_features", "has_enhanced", "config", "inst", "ncias", "index_dir", "os", "environ", "get", "index_dir", "mcp_index", "index_root", "os", "environ", "get", "index_root", "os", "getcwd", "if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path"]}
{"chunk_id": "d64aec4e862e9210eb3b413d", "file_path": "mcp_server_enhanced.py", "start_line": 69, "end_line": 148, "content": "    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n        \n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n            \n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive\n            )\n        \n        return result\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, ", "mtime": 1755725603.7595248, "terms": ["_indexer", "basecodeindexer", "index_dir", "index_dir", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer"]}
{"chunk_id": "402f31fa1234ea354ac0bd8c", "file_path": "mcp_server_enhanced.py", "start_line": 73, "end_line": 152, "content": "def _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n    \n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n        \n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n            \n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing(paths=[path], recursive=recursive)\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer, \n                [path], \n                recursive=recursive\n            )\n        \n        return result\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        ", "mtime": 1755725603.7595248, "terms": ["def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "paths", "path", "recursive", "recursive", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget"]}
{"chunk_id": "69f2a81a39e962d4e4a977c2", "file_path": "mcp_server_enhanced.py", "start_line": 107, "end_line": 186, "content": "def _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:", "mtime": 1755725603.7595248, "terms": ["def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as"]}
{"chunk_id": "af1a24781b9552b215ed9b85", "file_path": "mcp_server_enhanced.py", "start_line": 133, "end_line": 212, "content": "def _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"", "mtime": 1755725603.7595248, "terms": ["def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache"]}
{"chunk_id": "c35424b37aea9bc7905d47ea", "file_path": "mcp_server_enhanced.py", "start_line": 137, "end_line": 216, "content": "    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':", "mtime": 1755725603.7595248, "terms": ["try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear"]}
{"chunk_id": "97d2eb55b420ae7e96017f88", "file_path": "mcp_server_enhanced.py", "start_line": 163, "end_line": 242, "content": "def _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing(paths=paths or [INDEX_ROOT], recursive=recursive)\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()", "mtime": 1755725603.7595248, "terms": ["def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "paths", "paths", "or", "index_root", "recursive", "recursive", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool"]}
{"chunk_id": "0c5147e904add8f218522691", "file_path": "mcp_server_enhanced.py", "start_line": 189, "end_line": 268, "content": "def _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:", "mtime": 1755725603.7595248, "terms": ["def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict"]}
{"chunk_id": "26da784ffaf79e796c5c7b5a", "file_path": "mcp_server_enhanced.py", "start_line": 205, "end_line": 284, "content": "            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server", "mtime": 1755725603.7595248, "terms": ["stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server"]}
{"chunk_id": "c842824a4c1b69c9c4413dab", "file_path": "mcp_server_enhanced.py", "start_line": 211, "end_line": 290, "content": "def _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():", "mtime": 1755725603.7595248, "terms": ["def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools"]}
{"chunk_id": "6d75d0b05ec70dfe3fefac93", "file_path": "mcp_server_enhanced.py", "start_line": 243, "end_line": 322, "content": "    def index_path(path: str = \".\", \n                   recursive: bool = True, \n                   enable_semantic: bool = True, \n                   auto_start_watcher: bool = False, \n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", ", "mtime": 1755725603.7595248, "terms": ["def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms"]}
{"chunk_id": "e53909575b4112a562e92eff", "file_path": "mcp_server_enhanced.py", "start_line": 252, "end_line": 331, "content": "    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]", "mtime": 1755725603.7595248, "terms": ["def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query"]}
{"chunk_id": "cf589303ecf2c663fc5f0d54", "file_path": "mcp_server_enhanced.py", "start_line": 260, "end_line": 339, "content": "    def context_pack(query: str, \n                     token_budget: int = 8000, \n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool() \n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {", "mtime": 1755725603.7595248, "terms": ["def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties"]}
{"chunk_id": "6a106e3a4b64351fae44126f", "file_path": "mcp_server_enhanced.py", "start_line": 268, "end_line": 347, "content": "    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",", "mtime": 1755725603.7595248, "terms": ["def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats"]}
{"chunk_id": "0c885805d07d19602ac739f3", "file_path": "mcp_server_enhanced.py", "start_line": 273, "end_line": 352, "content": "    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"Obt√©m estat√≠sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", ", "mtime": 1755725603.7595248, "terms": ["def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management"]}
{"chunk_id": "c66a38357e225d401b5f9b36", "file_path": "mcp_server_enhanced.py", "start_line": 278, "end_line": 357, "content": "    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n    \n    server = Server(name=\"code-indexer-enhanced\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\", \n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\", \n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\", \n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"Obt√©m estat√≠sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", \n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},", "mtime": 1755725603.7595248, "terms": ["def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status"]}
{"chunk_id": "998ec497be3e419face9fa35", "file_path": "mcp_server_enhanced.py", "start_line": 341, "end_line": 420, "content": "                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"Obt√©m estat√≠sticas do indexador\",\n                inputSchema={\"type\": \"object\", \"properties\": {}}\n            ),\n            Tool(\n                name=\"cache_management\", \n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"cache_type\": {\"type\": \"string\", \"default\": \"all\"}\n                    }\n                }\n            )\n        ]\n\n    @server.call_tool()\n    async def call_tool(name: str, arguments: dict):\n        if name == \"index_path\":\n            result = _handle_index_path(\n                arguments.get(\"path\", \".\"),\n                arguments.get(\"recursive\", True), \n                arguments.get(\"enable_semantic\", True),\n                arguments.get(\"auto_start_watcher\", False),\n                arguments.get(\"exclude_globs\")\n            )\n        elif name == \"search_code\":\n            result = _handle_search_code(\n                arguments[\"query\"],\n                arguments.get(\"limit\", 10),\n                arguments.get(\"semantic_weight\", 0.3),\n                arguments.get(\"use_mmr\", True)\n            )\n        elif name == \"context_pack\":\n            result = _handle_context_pack(\n                arguments[\"query\"],\n                arguments.get(\"token_budget\", 8000),\n                arguments.get(\"max_chunks\", 20),\n                arguments.get(\"strategy\", \"mmr\")\n            )\n        elif name == \"auto_index\":\n            result = _handle_auto_index(\n                arguments.get(\"action\", \"status\"),\n                arguments.get(\"paths\"),\n                arguments.get(\"recursive\", True)\n            )\n        elif name == \"get_stats\":\n            result = _handle_get_stats()\n        elif name == \"cache_management\":\n            result = _handle_cache_management(\n                arguments.get(\"action\", \"status\"),\n                arguments.get(\"cache_type\", \"all\")\n            )\n        else:\n            result = {\"status\": \"error\", \"error\": f\"Tool {name} not found\"}\n        \n        return [TextContent(type=\"text\", text=str(result))]\n\n# ===== INICIALIZA√á√ÉO DO SERVIDOR =====\n\nif __name__ == \"__main__\":\n    # Log de inicializa√ß√£o\n    sys.stderr.write(\"üîÑ Iniciando servidor MCP melhorado...\\n\")\n    \n    # Mostra status das funcionalidades  \n    if HAS_ENHANCED_FEATURES:\n        sys.stderr.write(\"‚úÖ Sistema de busca sem√¢ntica ativado\\n\")\n        sys.stderr.write(\"‚úÖ Sistema de auto-indexa√ß√£o ativado\\n\")\n    else:\n        sys.stderr.write(\"‚ö†Ô∏è  [mcp_server_enhanced] Recursos b√°sicos apenas\\n\")\n        sys.stderr.write(\"   üí° Instale sentence-transformers e watchdog para recursos completos\\n\")\n    \n    sys.stderr.write(\"‚úÖ [mcp_server_enhanced] Servidor MCP melhorado iniciado\\n\")", "mtime": 1755725603.7595248, "terms": ["paths", "type", "array", "items", "type", "string", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "cache_type", "type", "string", "default", "all", "server", "call_tool", "async", "def", "call_tool", "name", "str", "arguments", "dict", "if", "name", "index_path", "result", "_handle_index_path", "arguments", "get", "path", "arguments", "get", "recursive", "true", "arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "elif", "name", "search_code", "result", "_handle_search_code", "arguments", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack", "result", "_handle_context_pack", "arguments", "query", "arguments", "get", "token_budget", "arguments", "get", "max_chunks", "arguments", "get", "strategy", "mmr", "elif", "name", "auto_index", "result", "_handle_auto_index", "arguments", "get", "action", "status", "arguments", "get", "paths", "arguments", "get", "recursive", "true", "elif", "name", "get_stats", "result", "_handle_get_stats", "elif", "name", "cache_management", "result", "_handle_cache_management", "arguments", "get", "action", "status", "arguments", "get", "cache_type", "all", "else", "result", "status", "error", "error", "tool", "name", "not", "found", "return", "textcontent", "type", "text", "text", "str", "result", "inicializa", "do", "servidor", "if", "__name__", "__main__", "log", "de", "inicializa", "sys", "stderr", "write", "iniciando", "servidor", "mcp", "melhorado", "mostra", "status", "das", "funcionalidades", "if", "has_enhanced_features", "sys", "stderr", "write", "sistema", "de", "busca", "sem", "ntica", "ativado", "sys", "stderr", "write", "sistema", "de", "auto", "indexa", "ativado", "else", "sys", "stderr", "write", "mcp_server_enhanced", "recursos", "sicos", "apenas", "sys", "stderr", "write", "instale", "sentence", "transformers", "watchdog", "para", "recursos", "completos", "sys", "stderr", "write", "mcp_server_enhanced", "servidor", "mcp", "melhorado", "iniciado"]}
{"chunk_id": "c1dec021c0dee2f024ed21c3", "file_path": "mcp_server_enhanced.py", "start_line": 409, "end_line": 433, "content": "    # Log de inicializa√ß√£o\n    sys.stderr.write(\"üîÑ Iniciando servidor MCP melhorado...\\n\")\n    \n    # Mostra status das funcionalidades  \n    if HAS_ENHANCED_FEATURES:\n        sys.stderr.write(\"‚úÖ Sistema de busca sem√¢ntica ativado\\n\")\n        sys.stderr.write(\"‚úÖ Sistema de auto-indexa√ß√£o ativado\\n\")\n    else:\n        sys.stderr.write(\"‚ö†Ô∏è  [mcp_server_enhanced] Recursos b√°sicos apenas\\n\")\n        sys.stderr.write(\"   üí° Instale sentence-transformers e watchdog para recursos completos\\n\")\n    \n    sys.stderr.write(\"‚úÖ [mcp_server_enhanced] Servidor MCP melhorado iniciado\\n\")\n    sys.stderr.write(f\"   üìç √çndice: {INDEX_DIR}\\n\")\n    sys.stderr.write(f\"   üìÅ Reposit√≥rio: {INDEX_ROOT}\\n\")\n    sys.stderr.write(f\"   üß† Busca sem√¢ntica: {'Dispon√≠vel' if HAS_ENHANCED_FEATURES else 'Indispon√≠vel'}\\n\")\n    sys.stderr.write(f\"   üëÅÔ∏è  Auto-indexa√ß√£o: {'Dispon√≠vel' if HAS_ENHANCED_FEATURES else 'Indispon√≠vel'}\\n\")\n    \n    # Executa servidor com a API correta\n    if HAS_FASTMCP:\n        # FastMCP n√£o usa stdio=True, executa diretamente\n        mcp.run()\n    else:\n        # MCP tradicional usa stdio_server\n        import asyncio\n        asyncio.run(stdio_server(server))", "mtime": 1755725603.7595248, "terms": ["log", "de", "inicializa", "sys", "stderr", "write", "iniciando", "servidor", "mcp", "melhorado", "mostra", "status", "das", "funcionalidades", "if", "has_enhanced_features", "sys", "stderr", "write", "sistema", "de", "busca", "sem", "ntica", "ativado", "sys", "stderr", "write", "sistema", "de", "auto", "indexa", "ativado", "else", "sys", "stderr", "write", "mcp_server_enhanced", "recursos", "sicos", "apenas", "sys", "stderr", "write", "instale", "sentence", "transformers", "watchdog", "para", "recursos", "completos", "sys", "stderr", "write", "mcp_server_enhanced", "servidor", "mcp", "melhorado", "iniciado", "sys", "stderr", "write", "ndice", "index_dir", "sys", "stderr", "write", "reposit", "rio", "index_root", "sys", "stderr", "write", "busca", "sem", "ntica", "dispon", "vel", "if", "has_enhanced_features", "else", "indispon", "vel", "sys", "stderr", "write", "auto", "indexa", "dispon", "vel", "if", "has_enhanced_features", "else", "indispon", "vel", "executa", "servidor", "com", "api", "correta", "if", "has_fastmcp", "fastmcp", "usa", "stdio", "true", "executa", "diretamente", "mcp", "run", "else", "mcp", "tradicional", "usa", "stdio_server", "import", "asyncio", "asyncio", "run", "stdio_server", "server"]}
{"chunk_id": "f16a41869d42dee0a41238ff", "file_path": "__init__.py", "start_line": 1, "end_line": 25, "content": "# MCP System - Model Context Protocol\n\"\"\"\nSistema avan√ßado de indexa√ß√£o e busca de c√≥digo para desenvolvimento assistido por IA.\nReduz drasticamente o consumo de tokens fornecendo apenas contexto relevante.\n\"\"\"\n\n__version__ = \"1.0.0\"\n__author__ = \"Assistant\"\n\n# Importa√ß√µes principais\nfrom .code_indexer_enhanced import (\n    EnhancedCodeIndexer,\n    BaseCodeIndexer,\n    search_code,\n    build_context_pack,\n    index_repo_paths\n)\n\n__all__ = [\n    \"EnhancedCodeIndexer\",\n    \"BaseCodeIndexer\", \n    \"search_code\",\n    \"build_context_pack\",\n    \"index_repo_paths\"\n]", "mtime": 1755701264.1672254, "terms": ["mcp", "system", "model", "context", "protocol", "sistema", "avan", "ado", "de", "indexa", "busca", "de", "digo", "para", "desenvolvimento", "assistido", "por", "ia", "reduz", "drasticamente", "consumo", "de", "tokens", "fornecendo", "apenas", "contexto", "relevante", "__version__", "__author__", "assistant", "importa", "es", "principais", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "__all__", "enhancedcodeindexer", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths"]}
{"chunk_id": "b10e7095bcdd6413fc033021", "file_path": "code_indexer_enhanced.py", "start_line": 1, "end_line": 80, "content": "# code_indexer_enhanced.py\n# Sistema MCP melhorado com busca h√≠brida, auto-indexa√ß√£o e cache inteligente\nfrom __future__ import annotations\nimport os, re, json, math, time, hashlib, threading, csv, datetime as dt\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\n\n#logs para m√©tricas\n\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: dict):\n    \"\"\"Append de uma linha de m√©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avan√ßados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from src.embeddings.semantic_search import SemanticSearchEngine\n    from src.utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILIT√ÅRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------", "mtime": 1755730603.8711107, "terms": ["code_indexer_enhanced", "py", "sistema", "mcp", "melhorado", "com", "busca", "brida", "auto", "indexa", "cache", "inteligente", "from", "__future__", "import", "annotations", "import", "os", "re", "json", "math", "time", "hashlib", "threading", "csv", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "logs", "para", "tricas", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "src", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "src", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence"]}
{"chunk_id": "27768a55e38187835aebdb5d", "file_path": "code_indexer_enhanced.py", "start_line": 12, "end_line": 91, "content": "def _log_metrics(row: dict):\n    \"\"\"Append de uma linha de m√©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avan√ßados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from src.embeddings.semantic_search import SemanticSearchEngine\n    from src.utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILIT√ÅRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():", "mtime": 1755730603.8711107, "terms": ["def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "src", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "src", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists"]}
{"chunk_id": "b8c197b282c1dcaef0dbd45e", "file_path": "code_indexer_enhanced.py", "start_line": 54, "end_line": 133, "content": "def now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)", "mtime": 1755730603.8711107, "terms": ["def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add"]}
{"chunk_id": "f6e75ee5e5224c473bce27d0", "file_path": "code_indexer_enhanced.py", "start_line": 57, "end_line": 136, "content": "def tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)", "mtime": 1755730603.8711107, "terms": ["def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add"]}
{"chunk_id": "145a2fe3bbbd93ff5e39c143", "file_path": "code_indexer_enhanced.py", "start_line": 60, "end_line": 139, "content": "def est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []", "mtime": 1755730603.8711107, "terms": ["def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks"]}
{"chunk_id": "96f659cdf3150fc11d2650a4", "file_path": "code_indexer_enhanced.py", "start_line": 64, "end_line": 143, "content": "def hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])", "mtime": 1755730603.8711107, "terms": ["def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end"]}
{"chunk_id": "cd76301a2d1d666d69ad8acf", "file_path": "code_indexer_enhanced.py", "start_line": 69, "end_line": 148, "content": "class BaseCodeIndexer:\n    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:", "mtime": 1755730603.8711107, "terms": ["class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int"]}
{"chunk_id": "1e31e98351da26f510bf9f59", "file_path": "code_indexer_enhanced.py", "start_line": 70, "end_line": 149, "content": "    def __init__(self, index_dir: str = \".mcp_index\", repo_root: str = \".\") -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)", "mtime": 1755730603.8711107, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path"]}
{"chunk_id": "72c9ccbb20b9efe98abc52d3", "file_path": "code_indexer_enhanced.py", "start_line": 81, "end_line": 160, "content": "    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)", "mtime": 1755730603.8711107, "terms": ["def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key"]}
{"chunk_id": "3ec55d96ba8803522308392c", "file_path": "code_indexer_enhanced.py", "start_line": 89, "end_line": 168, "content": "    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,", "mtime": 1755730603.8711107, "terms": ["def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end"]}
{"chunk_id": "a0a0ab5d242eb524114570ec", "file_path": "code_indexer_enhanced.py", "start_line": 103, "end_line": 182, "content": "    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}", "mtime": 1755730603.8711107, "terms": ["def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen"]}
{"chunk_id": "5e9566f91fcd434f32c38fcd", "file_path": "code_indexer_enhanced.py", "start_line": 113, "end_line": 192, "content": "    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(", "mtime": 1755730603.8711107, "terms": ["def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths"]}
{"chunk_id": "7bbb3ab8879db52db530e00e", "file_path": "code_indexer_enhanced.py", "start_line": 122, "end_line": 201, "content": "    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0", "mtime": 1755730603.8711107, "terms": ["def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed"]}
{"chunk_id": "048827d2aa67a758342b9752", "file_path": "code_indexer_enhanced.py", "start_line": 137, "end_line": 216, "content": "            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:", "mtime": 1755730603.8711107, "terms": ["max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths"]}
{"chunk_id": "7379b29914f6b52b3b1738a9", "file_path": "code_indexer_enhanced.py", "start_line": 148, "end_line": 227, "content": "    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        self.file_mtime[str(file_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{file_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(file_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n", "mtime": 1755730603.8711107, "terms": ["def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "self", "file_mtime", "str", "file_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "file_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "file_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass"]}
{"chunk_id": "4361dd209b210e9b3dd0afe7", "file_path": "code_indexer_enhanced.py", "start_line": 179, "end_line": 258, "content": "    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue", "mtime": 1755730603.8711107, "terms": ["def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue"]}
{"chunk_id": "654b17b7b22468178a4289e3", "file_path": "code_indexer_enhanced.py", "start_line": 192, "end_line": 271, "content": "def index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n", "mtime": 1755730603.8711107, "terms": ["def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items"]}
{"chunk_id": "8b32e209a42877f2b64bf639", "file_path": "code_indexer_enhanced.py", "start_line": 205, "end_line": 284, "content": "        pth = Path(p)\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None", "mtime": 1755730603.8711107, "terms": ["pth", "path", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none"]}
{"chunk_id": "3ca8e1543f215f56d6f65a2e", "file_path": "code_indexer_enhanced.py", "start_line": 233, "end_line": 312, "content": "def _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))", "mtime": 1755730603.8711107, "terms": ["def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys"]}
{"chunk_id": "a96159104c4cf27eadacfaad", "file_path": "code_indexer_enhanced.py", "start_line": 252, "end_line": 331, "content": "def _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],", "mtime": 1755730603.8711107, "terms": ["def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line"]}
{"chunk_id": "82f9d27d125a3dfd10b3f1a7", "file_path": "code_indexer_enhanced.py", "start_line": 264, "end_line": 343, "content": "def _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:", "mtime": 1755730603.8711107, "terms": ["def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str"]}
{"chunk_id": "c08888dfc4fbc24d60d4926c", "file_path": "code_indexer_enhanced.py", "start_line": 272, "end_line": 351, "content": "def _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n", "mtime": 1755730603.8711107, "terms": ["def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens"]}
{"chunk_id": "29e91e98a265d43ca4bcdfb3", "file_path": "code_indexer_enhanced.py", "start_line": 273, "end_line": 352, "content": "    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []", "mtime": 1755730603.8711107, "terms": ["if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked"]}
{"chunk_id": "a1441f371df9f3586788d47e", "file_path": "code_indexer_enhanced.py", "start_line": 280, "end_line": 359, "content": "def _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n", "mtime": 1755730603.8711107, "terms": ["def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break"]}
{"chunk_id": "d919483a9871bdca0990c58f", "file_path": "code_indexer_enhanced.py", "start_line": 302, "end_line": 381, "content": "def search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",", "mtime": 1755730603.8711107, "terms": ["def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end"]}
{"chunk_id": "299c53f919d1572988c90da1", "file_path": "code_indexer_enhanced.py", "start_line": 340, "end_line": 419, "content": "def get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:", "mtime": 1755730603.8711107, "terms": ["def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks"]}
{"chunk_id": "244cb5d079537b170ad28735", "file_path": "code_indexer_enhanced.py", "start_line": 341, "end_line": 420, "content": "    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue", "mtime": 1755730603.8711107, "terms": ["return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue"]}
{"chunk_id": "df5e3988cd850cc2b8468f4d", "file_path": "code_indexer_enhanced.py", "start_line": 343, "end_line": 422, "content": "def _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento", "mtime": 1755730603.8711107, "terms": ["def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento"]}
{"chunk_id": "d916b4219aedc2c2b0939c82", "file_path": "code_indexer_enhanced.py", "start_line": 366, "end_line": 445, "content": "def build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por itera√ß√£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG M√âTRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),", "mtime": 1755730603.8711107, "terms": ["def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds"]}
{"chunk_id": "797056d12a0c533f0c37aa91", "file_path": "code_indexer_enhanced.py", "start_line": 409, "end_line": 488, "content": "    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por itera√ß√£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG M√âTRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n            \"query\": query[:160],\n            \"chunk_count\": len(pack[\"chunks\"]),\n            \"total_tokens\": pack[\"total_tokens\"],\n            \"budget_tokens\": budget_tokens,\n            \"budget_utilization\": round(pack[\"total_tokens\"] / max(1, budget_tokens), 3),\n            \"latency_ms\": latency_ms,\n        })\n    except Exception:\n        pass  # n√£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de c√≥digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca sem√¢ntica com embeddings\n    - Auto-indexa√ß√£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None", "mtime": 1755730603.8711107, "terms": ["for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "query", "query", "chunk_count", "len", "pack", "chunks", "total_tokens", "pack", "total_tokens", "budget_tokens", "budget_tokens", "budget_utilization", "round", "pack", "total_tokens", "max", "budget_tokens", "latency_ms", "latency_ms", "except", "exception", "pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none"]}
{"chunk_id": "2653fa1a7bb5333d3d704ccd", "file_path": "code_indexer_enhanced.py", "start_line": 461, "end_line": 540, "content": "class EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de c√≥digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca sem√¢ntica com embeddings\n    - Auto-indexa√ß√£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,", "mtime": 1755730603.8711107, "terms": ["class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "6cbbf718fcf269c6e273699a", "file_path": "code_indexer_enhanced.py", "start_line": 470, "end_line": 549, "content": "    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir", "mtime": 1755730603.8711107, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir"]}
{"chunk_id": "acd7e6f7b109a29ae457f918", "file_path": "code_indexer_enhanced.py", "start_line": 477, "end_line": 556, "content": "        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n        \n        # Lock para thread safety\n        self._lock = threading.RLock()\n    \n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:", "mtime": 1755730603.8711107, "terms": ["inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock"]}
{"chunk_id": "382f74dfa4b6d6a4c824a0a0", "file_path": "code_indexer_enhanced.py", "start_line": 516, "end_line": 595, "content": "    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o", "mtime": 1755730603.8711107, "terms": ["def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr"]}
{"chunk_id": "13d6da67628311c1d48b9788", "file_path": "code_indexer_enhanced.py", "start_line": 537, "end_line": 616, "content": "    def index_files(self, \n                   paths: List[str], \n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25", "mtime": 1755730603.8711107, "terms": ["def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25"]}
{"chunk_id": "9132a313d4c2bc6da021e0bd", "file_path": "code_indexer_enhanced.py", "start_line": 545, "end_line": 624, "content": "        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca h√≠brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,", "mtime": 1755730603.8711107, "terms": ["args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results"]}
{"chunk_id": "0afddbb70bc8153c79759101", "file_path": "code_indexer_enhanced.py", "start_line": 582, "end_line": 661, "content": "    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca h√≠brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ö†Ô∏è  Erro na busca sem√¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )", "mtime": 1755730603.8711107, "terms": ["def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic"]}
{"chunk_id": "49c7c6b0ad99849652510cd4", "file_path": "code_indexer_enhanced.py", "start_line": 613, "end_line": 692, "content": "            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca h√≠brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ö†Ô∏è  Erro na busca sem√¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa fun√ß√£o de constru√ß√£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela fun√ß√£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # Constr√≥i pack usando fun√ß√£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informa√ß√µes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}", "mtime": 1755730603.8711107, "terms": ["sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str"]}
{"chunk_id": "9afd6b3fe36c5568d4e4e1f1", "file_path": "code_indexer_enhanced.py", "start_line": 637, "end_line": 716, "content": "    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa fun√ß√£o de constru√ß√£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela fun√ß√£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # Constr√≥i pack usando fun√ß√£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informa√ß√µes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    ", "mtime": 1755730603.8711107, "terms": ["def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false"]}
{"chunk_id": "35752f0da946c62c848d878b", "file_path": "code_indexer_enhanced.py", "start_line": 681, "end_line": 760, "content": "            \n            # Adiciona informa√ß√µes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"", "mtime": 1755730603.8711107, "terms": ["adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr"]}
{"chunk_id": "668375cfff581667e4f9391e", "file_path": "code_indexer_enhanced.py", "start_line": 694, "end_line": 771, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "0cf543ad0e3f823263eb2122", "file_path": "code_indexer_enhanced.py", "start_line": 705, "end_line": 771, "content": "    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "c7af84bf38bad457a493bb32", "file_path": "code_indexer_enhanced.py", "start_line": 717, "end_line": 771, "content": "    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "2af217ebb1c8db14d4f633ab", "file_path": "code_indexer_enhanced.py", "start_line": 731, "end_line": 771, "content": "def enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "eebb0e7bbd2d471620ef3e4e", "file_path": "code_indexer_enhanced.py", "start_line": 746, "end_line": 771, "content": "def enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "c0da20d4b426623494a55d51", "file_path": "code_indexer_enhanced.py", "start_line": 749, "end_line": 771, "content": "    top_k: int = 30, \n    filters: Optional[Dict] = None\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "18bc80edac607c608c1402e4", "file_path": "code_indexer_enhanced.py", "start_line": 755, "end_line": 771, "content": "def enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1755730603.8711107, "terms": ["def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "0bba6801cff3c217b480bf79", "file_path": "reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diret√≥rio de √≠ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de m√©tricas (MCP_METRICS_FILE ou .mcp_index/metrics.csv).\n- (Opcional) Calcula baseline aproximada de tokens do reposit√≥rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# ---- Config m√©tricas (mesmo padr√£o do context_pack)\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)", "mtime": 1755731278.2792592, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "config", "tricas", "mesmo", "padr", "do", "context_pack", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir"]}
{"chunk_id": "65e30d04e19ef87e8bf8e523", "file_path": "reindex.py", "start_line": 34, "end_line": 113, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1755731278.2792592, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "d8de839123e293bddabe57d6", "file_path": "reindex.py", "start_line": 43, "end_line": 122, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1755731278.2792592, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "5ae6096b36d042f60fa99f45", "file_path": "reindex.py", "start_line": 47, "end_line": 126, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1755731278.2792592, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "b7d68729e9b09df68aa8f961", "file_path": "reindex.py", "start_line": 65, "end_line": 144, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"üìä Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1755731278.2792592, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "bfc299e4c1673c806f2690b1", "file_path": "reindex.py", "start_line": 69, "end_line": 148, "content": "\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"üìä Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,", "mtime": 1755731278.2792592, "terms": ["importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed"]}
{"chunk_id": "327882a4217d07453fd44454", "file_path": "reindex.py", "start_line": 137, "end_line": 163, "content": "    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # n√£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1755731278.2792592, "terms": ["loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "f5c4d977878c14ec095b435d", "file_path": "setup_enhanced_mcp.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSetup autom√°tico do sistema MCP melhorado\nConfigura busca sem√¢ntica, auto-indexa√ß√£o e otimiza√ß√µes\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\ndef print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üöÄ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")", "mtime": 1755701372.735274, "terms": ["usr", "bin", "env", "python3", "setup", "autom", "tico", "do", "sistema", "mcp", "melhorado", "configura", "busca", "sem", "ntica", "auto", "indexa", "otimiza", "es", "import", "os", "import", "sys", "import", "subprocess", "import", "time", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "any", "def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn"]}
{"chunk_id": "2d583ca387f4256c4101b3a8", "file_path": "setup_enhanced_mcp.py", "start_line": 14, "end_line": 93, "content": "def print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üöÄ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")", "mtime": 1755701372.735274, "terms": ["def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt"]}
{"chunk_id": "64ff037144681d7c65f86e41", "file_path": "setup_enhanced_mcp.py", "start_line": 20, "end_line": 99, "content": "def print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            ", "mtime": 1755701372.735274, "terms": ["def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true"]}
{"chunk_id": "2d39b52c8fd0280e613a4454", "file_path": "setup_enhanced_mcp.py", "start_line": 31, "end_line": 110, "content": "def check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n        \n        # Instala√ß√£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep", "mtime": 1755701372.735274, "terms": ["def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep"]}
{"chunk_id": "514599c8ced1d4f461743cdd", "file_path": "setup_enhanced_mcp.py", "start_line": 69, "end_line": 148, "content": "    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n        \n        # Instala√ß√£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(f\"{dep} instalado\", \"ok\")\n            else:\n                print_step(f\"Erro ao instalar {dep}: {result.stderr}\", \"warn\")\n                \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro na instala√ß√£o: {e}\", \"error\")\n        return False\n\ndef setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"", "mtime": 1755701372.735274, "terms": ["except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "dep", "instalado", "ok", "else", "print_step", "erro", "ao", "instalar", "dep", "result", "stderr", "warn", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "instala", "error", "return", "false", "def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json"]}
{"chunk_id": "995f65108b625e37a8ac5052", "file_path": "setup_enhanced_mcp.py", "start_line": 84, "end_line": 163, "content": "def install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = Path(\"requirements_enhanced.txt\")\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n        \n        # Instala√ß√£o individual\n        for dep in missing_deps:\n            print_step(f\"Instalando {dep}\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", dep\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(f\"{dep} instalado\", \"ok\")\n            else:\n                print_step(f\"Erro ao instalar {dep}: {result.stderr}\", \"warn\")\n                \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro na instala√ß√£o: {e}\", \"error\")\n        return False\n\ndef setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"Configura√ß√£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexa√ß√£o inicial do projeto\"\"\"\n    print_header(\"Indexa√ß√£o Inicial do Projeto\")\n\n    try:", "mtime": 1755701372.735274, "terms": ["def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "path", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "instala", "individual", "for", "dep", "in", "missing_deps", "print_step", "instalando", "dep", "result", "subprocess", "run", "sys", "executable", "pip", "install", "dep", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "dep", "instalado", "ok", "else", "print_step", "erro", "ao", "instalar", "dep", "result", "stderr", "warn", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "instala", "error", "return", "false", "def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try"]}
{"chunk_id": "2e1e0fb2d9f8e82d67781d3a", "file_path": "setup_enhanced_mcp.py", "start_line": 124, "end_line": 203, "content": "def setup_mcp_config() -> bool:\n    \"\"\"Configura arquivo MCP para usar servidor melhorado\"\"\"\n    print_header(\"Configurando MCP\")\n    \n    try:\n        vscode_dir = Path(\".vscode\")\n        vscode_dir.mkdir(exist_ok=True)\n        \n        mcp_config = {\n            \"servers\": {\n                \"code-indexer-enhanced\": {\n                    \"type\": \"stdio\",\n                    \"command\": \"python\",\n                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"Configura√ß√£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexa√ß√£o inicial do projeto\"\"\"\n    print_header(\"Indexa√ß√£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # N√£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"fun√ß√£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado n√£o dispon√≠vel, usando vers√£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # Indexa√ß√£o b√°sica usando fun√ß√£o integrada\n            print_step(\"Indexando com vers√£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n", "mtime": 1755701372.735274, "terms": ["def", "setup_mcp_config", "bool", "configura", "arquivo", "mcp", "para", "usar", "servidor", "melhorado", "print_header", "configurando", "mcp", "try", "vscode_dir", "path", "vscode", "vscode_dir", "mkdir", "exist_ok", "true", "mcp_config", "servers", "code", "indexer", "enhanced", "type", "stdio", "command", "python", "args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks"]}
{"chunk_id": "292fc4caad52275461186d32", "file_path": "setup_enhanced_mcp.py", "start_line": 137, "end_line": 216, "content": "                    \"args\": [\"-u\", \"mcp_server_enhanced.py\"],\n                    \"cwd\": \"${workspaceFolder}\",\n                    \"env\": {\n                        \"INDEX_DIR\": \".mcp_index\",\n                        \"INDEX_ROOT\": \"${workspaceFolder}\"\n                    }\n                }\n            }\n        }\n        \n        import json\n        mcp_file = vscode_dir / \"mcp.json\"\n        with open(mcp_file, 'w', encoding='utf-8') as f:\n            json.dump(mcp_config, f, indent=2)\n        \n        print_step(f\"Configura√ß√£o salva em {mcp_file}\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro ao configurar MCP: {e}\", \"error\")\n        return False\n\ndef initial_indexing() -> bool:\n    \"\"\"Executa indexa√ß√£o inicial do projeto\"\"\"\n    print_header(\"Indexa√ß√£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # N√£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"fun√ß√£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado n√£o dispon√≠vel, usando vers√£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # Indexa√ß√£o b√°sica usando fun√ß√£o integrada\n            print_step(\"Indexando com vers√£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n\n            print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n            return True\n        except Exception as e:\n            print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP est√° funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    ", "mtime": 1755701372.735274, "terms": ["args", "mcp_server_enhanced", "py", "cwd", "workspacefolder", "env", "index_dir", "mcp_index", "index_root", "workspacefolder", "import", "json", "mcp_file", "vscode_dir", "mcp", "json", "with", "open", "mcp_file", "encoding", "utf", "as", "json", "dump", "mcp_config", "indent", "print_step", "configura", "salva", "em", "mcp_file", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "configurar", "mcp", "error", "return", "false", "def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp"]}
{"chunk_id": "0c9119775e982ce45e9d9908", "file_path": "setup_enhanced_mcp.py", "start_line": 159, "end_line": 238, "content": "def initial_indexing() -> bool:\n    \"\"\"Executa indexa√ß√£o inicial do projeto\"\"\"\n    print_header(\"Indexa√ß√£o Inicial do Projeto\")\n\n    try:\n        # Importa indexador melhorado\n        from mcp_system import EnhancedCodeIndexer\n\n        print_step(\"Iniciando indexador melhorado\")\n        indexer = EnhancedCodeIndexer(\n            index_dir=\".mcp_index\",\n            repo_root=\".\",\n            enable_semantic=True,\n            enable_auto_indexing=False  # N√£o inicia watcher ainda\n        )\n\n        # Indexa projeto atual\n        print_step(\"Indexando arquivos do projeto...\")\n        result = indexer.index_files([\".\"])\n\n        files_indexed = result.get('files_indexed', 0)\n        chunks_created = result.get('chunks', 0)\n\n        print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n\n        # Testa busca\n        print_step(\"Testando busca...\")\n        search_results = indexer.search_code(\"fun√ß√£o main\", top_k=3)\n        print_step(f\"Busca retornou {len(search_results)} resultados\", \"ok\")\n\n        return True\n\n    except ImportError:\n        print_step(\"Indexador melhorado n√£o dispon√≠vel, usando vers√£o base integrada\", \"warn\")\n        try:\n            from mcp_system import BaseCodeIndexer, index_repo_paths\n            indexer = BaseCodeIndexer(index_dir=\".mcp_index\", repo_root=\".\")\n\n            # Indexa√ß√£o b√°sica usando fun√ß√£o integrada\n            print_step(\"Indexando com vers√£o base...\")\n            result = index_repo_paths(indexer, [\".\"])\n\n            files_indexed = result.get('files_indexed', 0)\n            chunks_created = result.get('chunks', 0)\n\n            print_step(f\"Indexados {files_indexed} arquivos, {chunks_created} chunks\", \"ok\")\n            return True\n        except Exception as e:\n            print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP est√° funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importa√ß√£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools est√£o dispon√≠veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools dispon√≠veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  ‚Ä¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"", "mtime": 1755701372.735274, "terms": ["def", "initial_indexing", "bool", "executa", "indexa", "inicial", "do", "projeto", "print_header", "indexa", "inicial", "do", "projeto", "try", "importa", "indexador", "melhorado", "from", "mcp_system", "import", "enhancedcodeindexer", "print_step", "iniciando", "indexador", "melhorado", "indexer", "enhancedcodeindexer", "index_dir", "mcp_index", "repo_root", "enable_semantic", "true", "enable_auto_indexing", "false", "inicia", "watcher", "ainda", "indexa", "projeto", "atual", "print_step", "indexando", "arquivos", "do", "projeto", "result", "indexer", "index_files", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "testa", "busca", "print_step", "testando", "busca", "search_results", "indexer", "search_code", "fun", "main", "top_k", "print_step", "busca", "retornou", "len", "search_results", "resultados", "ok", "return", "true", "except", "importerror", "print_step", "indexador", "melhorado", "dispon", "vel", "usando", "vers", "base", "integrada", "warn", "try", "from", "mcp_system", "import", "basecodeindexer", "index_repo_paths", "indexer", "basecodeindexer", "index_dir", "mcp_index", "repo_root", "indexa", "sica", "usando", "fun", "integrada", "print_step", "indexando", "com", "vers", "base", "result", "index_repo_paths", "indexer", "files_indexed", "result", "get", "files_indexed", "chunks_created", "result", "get", "chunks", "print_step", "indexados", "files_indexed", "arquivos", "chunks_created", "chunks", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema"]}
{"chunk_id": "972dc0bde8ae8b3c39d7f083", "file_path": "setup_enhanced_mcp.py", "start_line": 205, "end_line": 284, "content": "            return True\n        except Exception as e:\n            print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n            return False\n    except Exception as e:\n        print_step(f\"Erro na indexa√ß√£o: {e}\", \"error\")\n        return False\n\ndef test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP est√° funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importa√ß√£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools est√£o dispon√≠veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools dispon√≠veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  ‚Ä¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nüéØ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nüìã COMANDOS PRINCIPAIS:\n\n1Ô∏è‚É£ Via MCP Tools (no Claude/cursor):\n   ‚Ä¢ index_path: Indexa arquivos/diret√≥rios\n   ‚Ä¢ search_code: Busca h√≠brida BM25 + sem√¢ntica\n   ‚Ä¢ context_pack: Gera contexto or√ßamentado\n   ‚Ä¢ auto_index: Controla auto-indexa√ß√£o\n   ‚Ä¢ get_stats: Estat√≠sticas do sistema\n   ‚Ä¢ cache_management: Gerencia caches\n\n2Ô∏è‚É£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3Ô∏è‚É£ Auto-indexa√ß√£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudan√ßas automaticamente\n   ```\n\nüîß CONFIGURA√á√ïES:\n\n‚Ä¢ √çndice armazenado em: .mcp_index/\n‚Ä¢ Busca sem√¢ntica: Ativada (se sentence-transformers dispon√≠vel)\n‚Ä¢ Auto-indexa√ß√£o: Dispon√≠vel (se watchdog dispon√≠vel)\n‚Ä¢ Cache persistente: Sim\n‚Ä¢ Or√ßamento de tokens: Configur√°vel\n\n‚ö° PR√ìXIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar c√≥digo\n3. Configure auto-indexa√ß√£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nüìä BENEF√çCIOS:\n", "mtime": 1755701372.735274, "terms": ["return", "true", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "except", "exception", "as", "print_step", "erro", "na", "indexa", "error", "return", "false", "def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios"]}
{"chunk_id": "8a0b7abadd232bb1004f0793", "file_path": "setup_enhanced_mcp.py", "start_line": 213, "end_line": 292, "content": "def test_mcp_server() -> bool:\n    \"\"\"Testa se o servidor MCP est√° funcionando\"\"\"\n    print_header(\"Testando Servidor MCP\")\n    \n    try:\n        # Testa importa√ß√£o do servidor\n        print_step(\"Importando servidor MCP...\")\n        import mcp_server_enhanced\n        print_step(\"Servidor importado\", \"ok\")\n        \n        # Verifica se as tools est√£o dispon√≠veis\n        server = mcp_server_enhanced.EnhancedCompatServer(\"test\")\n        tools = server.list_tools()\n        \n        print_step(f\"Encontradas {len(tools)} tools dispon√≠veis\", \"ok\")\n        for tool in tools:\n            print_step(f\"  ‚Ä¢ {tool.name}: {tool.description}\", \"\")\n        \n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro no teste do servidor: {e}\", \"error\")\n        return False\n\ndef show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nüéØ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nüìã COMANDOS PRINCIPAIS:\n\n1Ô∏è‚É£ Via MCP Tools (no Claude/cursor):\n   ‚Ä¢ index_path: Indexa arquivos/diret√≥rios\n   ‚Ä¢ search_code: Busca h√≠brida BM25 + sem√¢ntica\n   ‚Ä¢ context_pack: Gera contexto or√ßamentado\n   ‚Ä¢ auto_index: Controla auto-indexa√ß√£o\n   ‚Ä¢ get_stats: Estat√≠sticas do sistema\n   ‚Ä¢ cache_management: Gerencia caches\n\n2Ô∏è‚É£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3Ô∏è‚É£ Auto-indexa√ß√£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudan√ßas automaticamente\n   ```\n\nüîß CONFIGURA√á√ïES:\n\n‚Ä¢ √çndice armazenado em: .mcp_index/\n‚Ä¢ Busca sem√¢ntica: Ativada (se sentence-transformers dispon√≠vel)\n‚Ä¢ Auto-indexa√ß√£o: Dispon√≠vel (se watchdog dispon√≠vel)\n‚Ä¢ Cache persistente: Sim\n‚Ä¢ Or√ßamento de tokens: Configur√°vel\n\n‚ö° PR√ìXIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar c√≥digo\n3. Configure auto-indexa√ß√£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nüìä BENEF√çCIOS:\n\n‚úÖ 95% menos tokens irrelevantes\n‚úÖ Busca sem√¢ntica + lexical h√≠brida\n‚úÖ Auto-reindexa√ß√£o em mudan√ßas\n‚úÖ Cache inteligente persistente\n‚úÖ Or√ßamento de contexto controlado\n\nüÜò SUPORTE:\n", "mtime": 1755701372.735274, "terms": ["def", "test_mcp_server", "bool", "testa", "se", "servidor", "mcp", "est", "funcionando", "print_header", "testando", "servidor", "mcp", "try", "testa", "importa", "do", "servidor", "print_step", "importando", "servidor", "mcp", "import", "mcp_server_enhanced", "print_step", "servidor", "importado", "ok", "verifica", "se", "as", "tools", "est", "dispon", "veis", "server", "mcp_server_enhanced", "enhancedcompatserver", "test", "tools", "server", "list_tools", "print_step", "encontradas", "len", "tools", "tools", "dispon", "veis", "ok", "for", "tool", "in", "tools", "print_step", "tool", "name", "tool", "description", "return", "true", "except", "exception", "as", "print_step", "erro", "no", "teste", "do", "servidor", "error", "return", "false", "def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte"]}
{"chunk_id": "2171df10feac9d6fa11f98f0", "file_path": "setup_enhanced_mcp.py", "start_line": 237, "end_line": 316, "content": "def show_usage_guide():\n    \"\"\"Mostra guia de uso do sistema\"\"\"\n    print_header(\"Guia de Uso\")\n    \n    print(\"\"\"\nüéØ SISTEMA MCP CONFIGURADO COM SUCESSO!\n\nüìã COMANDOS PRINCIPAIS:\n\n1Ô∏è‚É£ Via MCP Tools (no Claude/cursor):\n   ‚Ä¢ index_path: Indexa arquivos/diret√≥rios\n   ‚Ä¢ search_code: Busca h√≠brida BM25 + sem√¢ntica\n   ‚Ä¢ context_pack: Gera contexto or√ßamentado\n   ‚Ä¢ auto_index: Controla auto-indexa√ß√£o\n   ‚Ä¢ get_stats: Estat√≠sticas do sistema\n   ‚Ä¢ cache_management: Gerencia caches\n\n2Ô∏è‚É£ Via Python:\n   ```python\n   from mcp_system import EnhancedCodeIndexer\n\n   indexer = EnhancedCodeIndexer()\n   results = indexer.search_code(\"sua consulta\")\n   context = indexer.build_context_pack(\"sua consulta\", budget_tokens=3000)\n   ```\n\n3Ô∏è‚É£ Auto-indexa√ß√£o:\n   ```python\n   indexer.start_auto_indexing()  # Monitora mudan√ßas automaticamente\n   ```\n\nüîß CONFIGURA√á√ïES:\n\n‚Ä¢ √çndice armazenado em: .mcp_index/\n‚Ä¢ Busca sem√¢ntica: Ativada (se sentence-transformers dispon√≠vel)\n‚Ä¢ Auto-indexa√ß√£o: Dispon√≠vel (se watchdog dispon√≠vel)\n‚Ä¢ Cache persistente: Sim\n‚Ä¢ Or√ßamento de tokens: Configur√°vel\n\n‚ö° PR√ìXIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar c√≥digo\n3. Configure auto-indexa√ß√£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nüìä BENEF√çCIOS:\n\n‚úÖ 95% menos tokens irrelevantes\n‚úÖ Busca sem√¢ntica + lexical h√≠brida\n‚úÖ Auto-reindexa√ß√£o em mudan√ßas\n‚úÖ Cache inteligente persistente\n‚úÖ Or√ßamento de contexto controlado\n\nüÜò SUPORTE:\n\n‚Ä¢ Verifique logs: get_stats\n‚Ä¢ Limpe cache: cache_management {\"action\": \"clear\"}\n‚Ä¢ Reinicie indexa√ß√£o: Apague .mcp_index/ e reindexe\n\"\"\")\n\ndef main():\n    \"\"\"Fun√ß√£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurar√° busca sem√¢ntica + auto-indexa√ß√£o\")\n    \n    # 1. Verificar depend√™ncias\n    deps = check_dependencies()\n    \n    # 2. Instalar depend√™ncias se necess√°rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nüí° Depend√™ncias opcionais em falta: {', '.join(missing)}\")", "mtime": 1755701372.735274, "terms": ["def", "show_usage_guide", "mostra", "guia", "de", "uso", "do", "sistema", "print_header", "guia", "de", "uso", "print", "sistema", "mcp", "configurado", "com", "sucesso", "comandos", "principais", "via", "mcp", "tools", "no", "claude", "cursor", "index_path", "indexa", "arquivos", "diret", "rios", "search_code", "busca", "brida", "bm25", "sem", "ntica", "context_pack", "gera", "contexto", "or", "amentado", "auto_index", "controla", "auto", "indexa", "get_stats", "estat", "sticas", "do", "sistema", "cache_management", "gerencia", "caches", "via", "python", "python", "from", "mcp_system", "import", "enhancedcodeindexer", "indexer", "enhancedcodeindexer", "results", "indexer", "search_code", "sua", "consulta", "context", "indexer", "build_context_pack", "sua", "consulta", "budget_tokens", "auto", "indexa", "python", "indexer", "start_auto_indexing", "monitora", "mudan", "as", "automaticamente", "configura", "es", "ndice", "armazenado", "em", "mcp_index", "busca", "sem", "ntica", "ativada", "se", "sentence", "transformers", "dispon", "vel", "auto", "indexa", "dispon", "vel", "se", "watchdog", "dispon", "vel", "cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte", "verifique", "logs", "get_stats", "limpe", "cache", "cache_management", "action", "clear", "reinicie", "indexa", "apague", "mcp_index", "reindexe", "def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing"]}
{"chunk_id": "c63f56149a642e057c779d49", "file_path": "setup_enhanced_mcp.py", "start_line": 273, "end_line": 349, "content": "‚Ä¢ Cache persistente: Sim\n‚Ä¢ Or√ßamento de tokens: Configur√°vel\n\n‚ö° PR√ìXIMOS PASSOS:\n\n1. Reinicie seu editor (VSCode/Cursor)\n2. Use as MCP tools para indexar e buscar c√≥digo\n3. Configure auto-indexa√ß√£o com: auto_index {\"action\": \"start\"}\n4. Monitore performance com: get_stats\n\nüìä BENEF√çCIOS:\n\n‚úÖ 95% menos tokens irrelevantes\n‚úÖ Busca sem√¢ntica + lexical h√≠brida\n‚úÖ Auto-reindexa√ß√£o em mudan√ßas\n‚úÖ Cache inteligente persistente\n‚úÖ Or√ßamento de contexto controlado\n\nüÜò SUPORTE:\n\n‚Ä¢ Verifique logs: get_stats\n‚Ä¢ Limpe cache: cache_management {\"action\": \"clear\"}\n‚Ä¢ Reinicie indexa√ß√£o: Apague .mcp_index/ e reindexe\n\"\"\")\n\ndef main():\n    \"\"\"Fun√ß√£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurar√° busca sem√¢ntica + auto-indexa√ß√£o\")\n    \n    # 1. Verificar depend√™ncias\n    deps = check_dependencies()\n    \n    # 2. Instalar depend√™ncias se necess√°rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nüí° Depend√™ncias opcionais em falta: {', '.join(missing)}\")\n        install = input(\"Deseja instalar agora? (s/N): \").lower().strip()\n        if install in ['s', 'sim', 'y', 'yes']:\n            install_dependencies(missing)\n    \n    # 3. Configurar MCP\n    if not setup_mcp_config():\n        print(\"\\n‚ùå Falha na configura√ß√£o do MCP\")\n        return False\n    \n    # 4. Indexa√ß√£o inicial\n    print(\"\\nüí° Executando indexa√ß√£o inicial (pode demorar um pouco)...\")\n    if not initial_indexing():\n        print(\"\\n‚ö†Ô∏è  Falha na indexa√ß√£o inicial, mas sistema pode funcionar\")\n    \n    # 5. Testar servidor\n    if not test_mcp_server():\n        print(\"\\n‚ö†Ô∏è  Problemas detectados no servidor MCP\")\n    \n    # 6. Mostrar guia de uso\n    show_usage_guide()\n    \n    print(\"\\nüéâ Setup conclu√≠do! Reinicie seu editor para usar o sistema MCP melhorado.\")\n    return True\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\n‚ö†Ô∏è Setup cancelado pelo usu√°rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\n‚ùå Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["cache", "persistente", "sim", "or", "amento", "de", "tokens", "configur", "vel", "pr", "ximos", "passos", "reinicie", "seu", "editor", "vscode", "cursor", "use", "as", "mcp", "tools", "para", "indexar", "buscar", "digo", "configure", "auto", "indexa", "com", "auto_index", "action", "start", "monitore", "performance", "com", "get_stats", "benef", "cios", "menos", "tokens", "irrelevantes", "busca", "sem", "ntica", "lexical", "brida", "auto", "reindexa", "em", "mudan", "as", "cache", "inteligente", "persistente", "or", "amento", "de", "contexto", "controlado", "suporte", "verifique", "logs", "get_stats", "limpe", "cache", "cache_management", "action", "clear", "reinicie", "indexa", "apague", "mcp_index", "reindexe", "def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing", "install", "input", "deseja", "instalar", "agora", "lower", "strip", "if", "install", "in", "sim", "yes", "install_dependencies", "missing", "configurar", "mcp", "if", "not", "setup_mcp_config", "print", "falha", "na", "configura", "do", "mcp", "return", "false", "indexa", "inicial", "print", "executando", "indexa", "inicial", "pode", "demorar", "um", "pouco", "if", "not", "initial_indexing", "print", "falha", "na", "indexa", "inicial", "mas", "sistema", "pode", "funcionar", "testar", "servidor", "if", "not", "test_mcp_server", "print", "problemas", "detectados", "no", "servidor", "mcp", "mostrar", "guia", "de", "uso", "show_usage_guide", "print", "setup", "conclu", "do", "reinicie", "seu", "editor", "para", "usar", "sistema", "mcp", "melhorado", "return", "true", "if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "04ef4728811811ee7a0284e5", "file_path": "setup_enhanced_mcp.py", "start_line": 298, "end_line": 349, "content": "def main():\n    \"\"\"Fun√ß√£o principal do setup\"\"\"\n    print_header(\"Setup do Sistema MCP Melhorado\")\n    print(\"Este script configurar√° busca sem√¢ntica + auto-indexa√ß√£o\")\n    \n    # 1. Verificar depend√™ncias\n    deps = check_dependencies()\n    \n    # 2. Instalar depend√™ncias se necess√°rio\n    missing = []\n    if not deps.get('sentence_transformers'):\n        missing.append('sentence-transformers')\n    if not deps.get('watchdog'):\n        missing.append('watchdog')\n    if not deps.get('numpy'):\n        missing.append('numpy')\n    \n    if missing:\n        print(f\"\\nüí° Depend√™ncias opcionais em falta: {', '.join(missing)}\")\n        install = input(\"Deseja instalar agora? (s/N): \").lower().strip()\n        if install in ['s', 'sim', 'y', 'yes']:\n            install_dependencies(missing)\n    \n    # 3. Configurar MCP\n    if not setup_mcp_config():\n        print(\"\\n‚ùå Falha na configura√ß√£o do MCP\")\n        return False\n    \n    # 4. Indexa√ß√£o inicial\n    print(\"\\nüí° Executando indexa√ß√£o inicial (pode demorar um pouco)...\")\n    if not initial_indexing():\n        print(\"\\n‚ö†Ô∏è  Falha na indexa√ß√£o inicial, mas sistema pode funcionar\")\n    \n    # 5. Testar servidor\n    if not test_mcp_server():\n        print(\"\\n‚ö†Ô∏è  Problemas detectados no servidor MCP\")\n    \n    # 6. Mostrar guia de uso\n    show_usage_guide()\n    \n    print(\"\\nüéâ Setup conclu√≠do! Reinicie seu editor para usar o sistema MCP melhorado.\")\n    return True\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\n‚ö†Ô∏è Setup cancelado pelo usu√°rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\n‚ùå Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["def", "main", "fun", "principal", "do", "setup", "print_header", "setup", "do", "sistema", "mcp", "melhorado", "print", "este", "script", "configurar", "busca", "sem", "ntica", "auto", "indexa", "verificar", "depend", "ncias", "deps", "check_dependencies", "instalar", "depend", "ncias", "se", "necess", "rio", "missing", "if", "not", "deps", "get", "sentence_transformers", "missing", "append", "sentence", "transformers", "if", "not", "deps", "get", "watchdog", "missing", "append", "watchdog", "if", "not", "deps", "get", "numpy", "missing", "append", "numpy", "if", "missing", "print", "depend", "ncias", "opcionais", "em", "falta", "join", "missing", "install", "input", "deseja", "instalar", "agora", "lower", "strip", "if", "install", "in", "sim", "yes", "install_dependencies", "missing", "configurar", "mcp", "if", "not", "setup_mcp_config", "print", "falha", "na", "configura", "do", "mcp", "return", "false", "indexa", "inicial", "print", "executando", "indexa", "inicial", "pode", "demorar", "um", "pouco", "if", "not", "initial_indexing", "print", "falha", "na", "indexa", "inicial", "mas", "sistema", "pode", "funcionar", "testar", "servidor", "if", "not", "test_mcp_server", "print", "problemas", "detectados", "no", "servidor", "mcp", "mostrar", "guia", "de", "uso", "show_usage_guide", "print", "setup", "conclu", "do", "reinicie", "seu", "editor", "para", "usar", "sistema", "mcp", "melhorado", "return", "true", "if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "680ca7c74f7822fbe34f55e3", "file_path": "setup_enhanced_mcp.py", "start_line": 341, "end_line": 349, "content": "if __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n\\n‚ö†Ô∏è Setup cancelado pelo usu√°rio\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"\\n‚ùå Erro durante setup: {e}\")\n        sys.exit(1)", "mtime": 1755701372.735274, "terms": ["if", "__name__", "__main__", "try", "main", "except", "keyboardinterrupt", "print", "setup", "cancelado", "pelo", "usu", "rio", "sys", "exit", "except", "exception", "as", "print", "erro", "durante", "setup", "sys", "exit"]}
{"chunk_id": "946797749e83a7383de7e803", "file_path": "embeddings/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Embeddings Module\n\"\"\"\nM√≥dulo de embeddings sem√¢nticos para busca avan√ßada de c√≥digo.\n\"\"\"\n\nfrom .semantic_search import *", "mtime": 1755701269.9533663, "terms": ["mcp", "system", "embeddings", "module", "dulo", "de", "embeddings", "sem", "nticos", "para", "busca", "avan", "ada", "de", "digo", "from", "semantic_search", "import"]}
{"chunk_id": "f278ef05522b8657f521598c", "file_path": "embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca sem√¢ntica usando embeddings locais\nIntegra√ß√£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"‚ö†Ô∏è  sentence-transformers n√£o encontrado. Busca sem√¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel", "mtime": 1755697115.1540978, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel"]}
{"chunk_id": "b33e9c866a4325e4972e400b", "file_path": "embeddings/semantic_search.py", "start_line": 24, "end_line": 103, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"‚ö†Ô∏è  sentence-transformers n√£o encontrado. Busca sem√¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou", "mtime": 1755697115.1540978, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou"]}
{"chunk_id": "390832dbcf1cbbba6b557366", "file_path": "embeddings/semantic_search.py", "start_line": 32, "end_line": 111, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"‚ö†Ô∏è  sentence-transformers n√£o encontrado. Busca sem√¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")", "mtime": 1755697115.1540978, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id"]}
{"chunk_id": "b8c8887984dde3c04857215a", "file_path": "embeddings/semantic_search.py", "start_line": 38, "end_line": 117, "content": "    def __init__(self, cache_dir: str = \".mcp_index\", model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n        \n        if HAS_SENTENCE_TRANSFORMERS:\n            self._initialize_model()\n        else:\n            import sys\n            sys.stderr.write(\"‚ö†Ô∏è  sentence-transformers n√£o encontrado. Busca sem√¢ntica desabilitada.\\n\")\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]", "mtime": 1755697115.1540978, "terms": ["def", "__init__", "self", "cache_dir", "str", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "if", "has_sentence_transformers", "self", "_initialize_model", "else", "import", "sys", "sys", "stderr", "write", "sentence", "transformers", "encontrado", "busca", "sem", "ntica", "desabilitada", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated"]}
{"chunk_id": "14042ffeba35f7c8336cdcf3", "file_path": "embeddings/semantic_search.py", "start_line": 54, "end_line": 133, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding", "mtime": 1755697115.1540978, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding"]}
{"chunk_id": "2b7114f80fb9604f6f9bf4c8", "file_path": "embeddings/semantic_search.py", "start_line": 66, "end_line": 145, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1755697115.1540978, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "88c4fc03b20171daf3298d55", "file_path": "embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:", "mtime": 1755697115.1540978, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args"]}
{"chunk_id": "de8579eee3fadc223fedb0c4", "file_path": "embeddings/semantic_search.py", "start_line": 70, "end_line": 149, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:\n            query: Texto da consulta", "mtime": 1755697115.1540978, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "2c2e60e4379a78b0c7e92e1e", "file_path": "embeddings/semantic_search.py", "start_line": 74, "end_line": 153, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n            \n        Returns:", "mtime": 1755697115.1540978, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "d5417cb34fa610b6f5431f88", "file_path": "embeddings/semantic_search.py", "start_line": 78, "end_line": 157, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n        \n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n            \n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return None\n            \n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []", "mtime": 1755697115.1540978, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return"]}
{"chunk_id": "fa4f48cfa341aa4c4641ae14", "file_path": "embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca sem√¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()", "mtime": 1755697115.1540978, "terms": ["except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model"]}
{"chunk_id": "6e7033b34fa098b4283eeabb", "file_path": "embeddings/semantic_search.py", "start_line": 143, "end_line": 222, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray], \n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n        \n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n            \n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or self.model is None:\n            return []\n        \n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n            \n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n            \n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n            \n            return similarities[:top_k]\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca sem√¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # Obt√©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor sele√ß√£o", "mtime": 1755697115.1540978, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "or", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele"]}
{"chunk_id": "3d7822c05f7559ee97512555", "file_path": "embeddings/semantic_search.py", "start_line": 182, "end_line": 261, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # Obt√©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor sele√ß√£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca sem√¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1755697115.1540978, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "756f028a2bc54bd7895c421b", "file_path": "embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # Obt√©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor sele√ß√£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca sem√¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk espec√≠fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em mem√≥ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))", "mtime": 1755697115.1540978, "terms": ["results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json"]}
{"chunk_id": "07fe09eea88b5a13b413f45e", "file_path": "embeddings/semantic_search.py", "start_line": 267, "end_line": 304, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk espec√≠fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em mem√≥ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "8bb83be47838f80ccd1adf20", "file_path": "embeddings/semantic_search.py", "start_line": 273, "end_line": 304, "content": "            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em mem√≥ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "0280ab6fa07d9e30c9481a80", "file_path": "embeddings/semantic_search.py", "start_line": 281, "end_line": 304, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "028c18221ff1be8b4fbe7719", "file_path": "embeddings/semantic_search.py", "start_line": 296, "end_line": 304, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1755697115.1540978, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "fbdb9b0c1f4af8520bffb15c", "file_path": "cache/search_cache.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nM√≥dulo para gerenciamento de cache de resultados de busca\n\"\"\"\nimport json\nimport hashlib\nimport time\nfrom typing import Any, Dict, Optional\n# Removendo imports n√£o utilizados que podem causar problemas\n\nclass SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"", "mtime": 1755690276.2485237, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gerenciamento", "de", "cache", "de", "resultados", "de", "busca", "import", "json", "import", "hashlib", "import", "time", "from", "typing", "import", "any", "dict", "optional", "removendo", "imports", "utilizados", "que", "podem", "causar", "problemas", "class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none"]}
{"chunk_id": "ca5360dc59a6469218343d38", "file_path": "cache/search_cache.py", "start_line": 11, "end_line": 90, "content": "class SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio", "mtime": 1755690276.2485237, "terms": ["class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio"]}
{"chunk_id": "35764bba5e8e74814b43b73f", "file_path": "cache/search_cache.py", "start_line": 14, "end_line": 93, "content": "    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:", "mtime": 1755690276.2485237, "terms": ["def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order"]}
{"chunk_id": "0bd9abd79b3070ece8471666", "file_path": "cache/search_cache.py", "start_line": 27, "end_line": 106, "content": "    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n", "mtime": 1755690276.2485237, "terms": ["def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "df919cc333a84981be2d9730", "file_path": "cache/search_cache.py", "start_line": 48, "end_line": 127, "content": "    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)", "mtime": 1755690276.2485237, "terms": ["def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key"]}
{"chunk_id": "a0dbc1de914801d595c0213e", "file_path": "cache/search_cache.py", "start_line": 69, "end_line": 148, "content": "            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(", "mtime": 1755690276.2485237, "terms": ["del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum"]}
{"chunk_id": "cf539096748fdafe5d215d8d", "file_path": "cache/search_cache.py", "start_line": 79, "end_line": 158, "content": "    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }", "mtime": 1755690276.2485237, "terms": ["def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds"]}
{"chunk_id": "4e9a2127a100d7e5cad1493d", "file_path": "cache/search_cache.py", "start_line": 107, "end_line": 162, "content": "    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "391196799063b14b58506b68", "file_path": "cache/search_cache.py", "start_line": 135, "end_line": 162, "content": "    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "c3bfd9110d6f7e9a55029513", "file_path": "cache/search_cache.py", "start_line": 137, "end_line": 162, "content": "        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "6b1c638eb644117f64d6e630", "file_path": "cache/search_cache.py", "start_line": 140, "end_line": 162, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1755690276.2485237, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "1972efb438b746e9d7fed6f9", "file_path": "cache/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Cache Module\n\"\"\"\nSistema de cache para embeddings e √≠ndices BM25.\n\"\"\"\n\nfrom .search_cache import *", "mtime": 1755701282.161089, "terms": ["mcp", "system", "cache", "module", "sistema", "de", "cache", "para", "embeddings", "ndices", "bm25", "from", "search_cache", "import"]}
{"chunk_id": "99799c722fddee579e00f3aa", "file_path": "utils/__init__.py", "start_line": 1, "end_line": 7, "content": "# MCP System - Utils Module\n\"\"\"\nUtilit√°rios do sistema MCP: embeddings, file watcher, etc.\n\"\"\"\n\nfrom .embeddings import *\nfrom .file_watcher import *", "mtime": 1755701275.657908, "terms": ["mcp", "system", "utils", "module", "utilit", "rios", "do", "sistema", "mcp", "embeddings", "file", "watcher", "etc", "from", "embeddings", "import", "from", "file_watcher", "import"]}
{"chunk_id": "9872699dc5eeca6eabb7622b", "file_path": "utils/embeddings.py", "start_line": 1, "end_line": 70, "content": "#!/usr/bin/env python3\n\"\"\"\nM√≥dulo para gera√ß√£o de embeddings usando Sentence Transformers\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Union\n\n\nclass EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gera", "de", "embeddings", "usando", "sentence", "transformers", "from", "sentence_transformers", "import", "sentencetransformer", "import", "numpy", "as", "np", "from", "typing", "import", "union", "class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "4b5bbedd0c1045be3de37c60", "file_path": "utils/embeddings.py", "start_line": 10, "end_line": 70, "content": "class EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "2003079116df9e8672c147d0", "file_path": "utils/embeddings.py", "start_line": 13, "end_line": 70, "content": "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "b960a1a51a7a855e3bd1e40a", "file_path": "utils/embeddings.py", "start_line": 24, "end_line": 70, "content": "    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "b4966b83236635bec7fdc9d3", "file_path": "utils/embeddings.py", "start_line": 36, "end_line": 70, "content": "    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "83881cb3a503c39e2e983f27", "file_path": "utils/embeddings.py", "start_line": 57, "end_line": 70, "content": "    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "0c8de0dce3e8419a412cf6cf", "file_path": "utils/embeddings.py", "start_line": 69, "end_line": 70, "content": "# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "42c55ee19958c9b9e0024656", "file_path": "utils/file_watcher.py", "start_line": 1, "end_line": 80, "content": "# src/utils/file_watcher.py\n\"\"\"\nSistema de monitoramento de arquivos para auto-indexa√ß√£o\nDetecta mudan√ßas e reindexar automaticamente arquivos modificados\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport time\nimport threading\nfrom pathlib import Path\nfrom typing import Set, Callable, Dict, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport hashlib\n\nimport sys\n\ntry:\n    from watchdog.observers import Observer\n    from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent\n    HAS_WATCHDOG = True\nexcept ImportError:\n    HAS_WATCHDOG = False\n    Observer = None\n    FileSystemEventHandler = None\n\n@dataclass\nclass IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False", "mtime": 1755697163.1334832, "terms": ["src", "utils", "file_watcher", "py", "sistema", "de", "monitoramento", "de", "arquivos", "para", "auto", "indexa", "detecta", "mudan", "as", "reindexar", "automaticamente", "arquivos", "modificados", "from", "__future__", "import", "annotations", "import", "os", "import", "time", "import", "threading", "from", "pathlib", "import", "path", "from", "typing", "import", "set", "callable", "dict", "optional", "from", "dataclasses", "import", "dataclass", "from", "concurrent", "futures", "import", "threadpoolexecutor", "import", "hashlib", "import", "sys", "try", "from", "watchdog", "observers", "import", "observer", "from", "watchdog", "events", "import", "filesystemeventhandler", "filemodifiedevent", "filecreatedevent", "filedeletedevent", "has_watchdog", "true", "except", "importerror", "has_watchdog", "false", "observer", "none", "filesystemeventhandler", "none", "dataclass", "class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false"]}
{"chunk_id": "aa9909d08cf52990eff03a5d", "file_path": "utils/file_watcher.py", "start_line": 29, "end_line": 108, "content": "class IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce", "mtime": 1755697163.1334832, "terms": ["class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce"]}
{"chunk_id": "0be2022196a58f0c5fd04023", "file_path": "utils/file_watcher.py", "start_line": 34, "end_line": 113, "content": "class FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:", "mtime": 1755697163.1334832, "terms": ["class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer"]}
{"chunk_id": "12e75180c957b7c2f5fef8ba", "file_path": "utils/file_watcher.py", "start_line": 40, "end_line": 119, "content": "    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks"]}
{"chunk_id": "51297785d4fa096d265474c8", "file_path": "utils/file_watcher.py", "start_line": 69, "end_line": 148, "content": "        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return", "mtime": 1755697163.1334832, "terms": ["if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return"]}
{"chunk_id": "44d4bcbb12417f0801f46869", "file_path": "utils/file_watcher.py", "start_line": 73, "end_line": 152, "content": "    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return", "mtime": 1755697163.1334832, "terms": ["def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return"]}
{"chunk_id": "3c5c11dd453fbff3b69edc0c", "file_path": "utils/file_watcher.py", "start_line": 93, "end_line": 172, "content": "    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n", "mtime": 1755697163.1334832, "terms": ["def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr"]}
{"chunk_id": "7144cd5e1bcf09e39d37a056", "file_path": "utils/file_watcher.py", "start_line": 111, "end_line": 190, "content": "    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')", "mtime": 1755697163.1334832, "terms": ["def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted"]}
{"chunk_id": "2a1c48109869ed62e5286d5d", "file_path": "utils/file_watcher.py", "start_line": 123, "end_line": 202, "content": "    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)", "mtime": 1755697163.1334832, "terms": ["def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr"]}
{"chunk_id": "32e77e46ee5d8fb55df10d4a", "file_path": "utils/file_watcher.py", "start_line": 137, "end_line": 216, "content": "            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados", "mtime": 1755697163.1334832, "terms": ["self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados"]}
{"chunk_id": "eb54ae8f8bc786fe26e0fb66", "file_path": "utils/file_watcher.py", "start_line": 145, "end_line": 224, "content": "    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)", "mtime": 1755697163.1334832, "terms": ["def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr"]}
{"chunk_id": "ad33f6150f3a41fcab30624c", "file_path": "utils/file_watcher.py", "start_line": 173, "end_line": 252, "content": "class WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:", "mtime": 1755697163.1334832, "terms": ["class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as"]}
{"chunk_id": "fc625ba8485aafd283f1b5d0", "file_path": "utils/file_watcher.py", "start_line": 176, "end_line": 255, "content": "    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict"]}
{"chunk_id": "2a69640d63e13e95c8af1b11", "file_path": "utils/file_watcher.py", "start_line": 180, "end_line": 259, "content": "    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,", "mtime": 1755697163.1334832, "terms": ["def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running"]}
{"chunk_id": "15f7d36faaaa26ac890ac2e9", "file_path": "utils/file_watcher.py", "start_line": 184, "end_line": 263, "content": "    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats", "mtime": 1755697163.1334832, "terms": ["def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats"]}
{"chunk_id": "62099b9d42213e18d8e32c15", "file_path": "utils/file_watcher.py", "start_line": 188, "end_line": 267, "content": "    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"", "mtime": 1755697163.1334832, "terms": ["def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher"]}
{"chunk_id": "153f060e00ab6a57c3bb7d51", "file_path": "utils/file_watcher.py", "start_line": 192, "end_line": 271, "content": "class FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    ", "mtime": 1755697163.1334832, "terms": ["class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as"]}
{"chunk_id": "098715c8cfe1d01f4af1d259", "file_path": "utils/file_watcher.py", "start_line": 195, "end_line": 274, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,", "mtime": 1755697163.1334832, "terms": ["def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none"]}
{"chunk_id": "713452a4d6b8feedfe7e83ee", "file_path": "utils/file_watcher.py", "start_line": 205, "end_line": 284, "content": "        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        ", "mtime": 1755697163.1334832, "terms": ["try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt"]}
{"chunk_id": "db4045659718608d0e04bfe4", "file_path": "utils/file_watcher.py", "start_line": 227, "end_line": 306, "content": "    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []", "mtime": 1755697163.1334832, "terms": ["def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files"]}
{"chunk_id": "11d8eb8c257da37186cd9428", "file_path": "utils/file_watcher.py", "start_line": 244, "end_line": 323, "content": "    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou", "mtime": 1755697163.1334832, "terms": ["def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou"]}
{"chunk_id": "779e690520b4aa3b67612843", "file_path": "utils/file_watcher.py", "start_line": 255, "end_line": 334, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        ", "mtime": 1755697163.1334832, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files"]}
{"chunk_id": "76c28e45e37dab615efe4502", "file_path": "utils/file_watcher.py", "start_line": 266, "end_line": 345, "content": "class SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:", "mtime": 1755697163.1334832, "terms": ["class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files"]}
{"chunk_id": "977993463a7f3436de756107", "file_path": "utils/file_watcher.py", "start_line": 272, "end_line": 351, "content": "    def __init__(self, \n                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)", "mtime": 1755697163.1334832, "terms": ["def", "__init__", "self", "watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files"]}
{"chunk_id": "1f4950c8c4d6e34c6cc4e3ee", "file_path": "utils/file_watcher.py", "start_line": 273, "end_line": 352, "content": "                 watch_path: str = \".\",\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)", "mtime": 1755697163.1334832, "terms": ["watch_path", "str", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files"]}
{"chunk_id": "2004bd7223f24cd8ee14745b", "file_path": "utils/file_watcher.py", "start_line": 296, "end_line": 375, "content": "    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        ", "mtime": 1755697163.1334832, "terms": ["def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start"]}
{"chunk_id": "9dea304d2a2f7b62b299c9bf", "file_path": "utils/file_watcher.py", "start_line": 304, "end_line": 383, "content": "    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():", "mtime": 1755697163.1334832, "terms": ["def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive"]}
{"chunk_id": "e13adf0ce8ad0d3942f67b1d", "file_path": "utils/file_watcher.py", "start_line": 337, "end_line": 406, "content": "    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "f9cf4c241ad14d69c9e4093a", "file_path": "utils/file_watcher.py", "start_line": 341, "end_line": 406, "content": "                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "2ceb50c653dcead02e012138", "file_path": "utils/file_watcher.py", "start_line": 363, "end_line": 406, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "7c997427a8ce953e10f449dd", "file_path": "utils/file_watcher.py", "start_line": 380, "end_line": 406, "content": "    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "4d811d48fc15a3e6884a9518", "file_path": "utils/file_watcher.py", "start_line": 387, "end_line": 406, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "100c5c904cdb420609c52c91", "file_path": "utils/file_watcher.py", "start_line": 398, "end_line": 406, "content": "def create_file_watcher(watch_path: str = \".\", **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1755697163.1334832, "terms": ["def", "create_file_watcher", "watch_path", "str", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "2627c0f97de40a354e7b7a7a", "file_path": "scripts/summarize_metrics.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nResumo de m√©tricas do MCP (context_pack) a partir de .mcp_index/metrics.csv\n\nCampos esperados no CSV:\n  ts, query, chunk_count, total_tokens, budget_tokens, budget_utilization, latency_ms\n\nUso b√°sico:\n  python summarize_metrics.py\n  python summarize_metrics.py --file .mcp_index/metrics.csv\n  python summarize_metrics.py --since 7\n  python summarize_metrics.py --filter \"minha funcao\"\n  python summarize_metrics.py --json\n\nSa√≠da:\n- Resumo geral (per√≠odo, N linhas, tokens m√©dios, p95, etc.)\n- Tabela di√°ria com avg/median/p95 de tokens e lat√™ncia\n\"\"\"\n\nimport os, sys, csv, argparse, datetime as dt, statistics as st, json\nfrom math import floor\nfrom typing import List, Dict, Any\n\ndef p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente √∫ltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' cont√©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao inv√©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV n√£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV est√° vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n", "mtime": 1755730890.8006165, "terms": ["usr", "bin", "env", "python3", "resumo", "de", "tricas", "do", "mcp", "context_pack", "partir", "de", "mcp_index", "metrics", "csv", "campos", "esperados", "no", "csv", "ts", "query", "chunk_count", "total_tokens", "budget_tokens", "budget_utilization", "latency_ms", "uso", "sico", "python", "summarize_metrics", "py", "python", "summarize_metrics", "py", "file", "mcp_index", "metrics", "csv", "python", "summarize_metrics", "py", "since", "python", "summarize_metrics", "py", "filter", "minha", "funcao", "python", "summarize_metrics", "py", "json", "sa", "da", "resumo", "geral", "per", "odo", "linhas", "tokens", "dios", "p95", "etc", "tabela", "di", "ria", "com", "avg", "median", "p95", "de", "tokens", "lat", "ncia", "import", "os", "sys", "csv", "argparse", "datetime", "as", "dt", "statistics", "as", "st", "json", "from", "math", "import", "floor", "from", "typing", "import", "list", "dict", "any", "def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily"]}
{"chunk_id": "ab112a745e12dab0b395b30b", "file_path": "scripts/summarize_metrics.py", "start_line": 24, "end_line": 103, "content": "def p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente √∫ltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' cont√©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao inv√©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV n√£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV est√° vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []", "mtime": 1755730890.8006165, "terms": ["def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat"]}
{"chunk_id": "f4bb10a1f776d0e67f78ae28", "file_path": "scripts/summarize_metrics.py", "start_line": 30, "end_line": 109, "content": "def coerce_int(s, default=0):\n    try:\n        return int(s)\n    except Exception:\n        try:\n            return int(float(s))\n        except Exception:\n            return default\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente √∫ltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' cont√©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao inv√©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV n√£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV est√° vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]", "mtime": 1755730890.8006165, "terms": ["def", "coerce_int", "default", "try", "return", "int", "except", "exception", "try", "return", "int", "float", "except", "exception", "return", "default", "def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day"]}
{"chunk_id": "df6df2fbf2ff7d6ad7fc0099", "file_path": "scripts/summarize_metrics.py", "start_line": 39, "end_line": 118, "content": "def parse_args():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP (CSV).\")\n    parser.add_argument(\"--file\", \"-f\", default=os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\"),\n                        help=\"Caminho do CSV (default: .mcp_index/metrics.csv ou $MCP_METRICS_FILE)\")\n    parser.add_argument(\"--since\", type=int, default=None,\n                        help=\"Somente √∫ltimos N dias (ex.: --since 7)\")\n    parser.add_argument(\"--filter\", type=str, default=None,\n                        help=\"Filtra linhas cujo 'query' cont√©m este texto (case-insensitive)\")\n    parser.add_argument(\"--json\", action=\"store_true\",\n                        help=\"Imprime JSON ao inv√©s de tabela\")\n    return parser.parse_args()\n\ndef load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV n√£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV est√° vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass", "mtime": 1755730890.8006165, "terms": ["def", "parse_args", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "csv", "parser", "add_argument", "file", "default", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "help", "caminho", "do", "csv", "default", "mcp_index", "metrics", "csv", "ou", "mcp_metrics_file", "parser", "add_argument", "since", "type", "int", "default", "none", "help", "somente", "ltimos", "dias", "ex", "since", "parser", "add_argument", "filter", "type", "str", "default", "none", "help", "filtra", "linhas", "cujo", "query", "cont", "este", "texto", "case", "insensitive", "parser", "add_argument", "json", "action", "store_true", "help", "imprime", "json", "ao", "inv", "de", "tabela", "return", "parser", "parse_args", "def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass"]}
{"chunk_id": "6d8efb53500fbf78a214911e", "file_path": "scripts/summarize_metrics.py", "start_line": 51, "end_line": 130, "content": "def load_rows(path: str) -> List[Dict[str, Any]]:\n    if not os.path.exists(path):\n        print(f\"[erro] CSV n√£o encontrado: {path}\", file=sys.stderr)\n        sys.exit(1)\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    if not rows:\n        print(\"[aviso] CSV est√° vazio.\", file=sys.stderr)\n    return rows\n\ndef within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n", "mtime": 1755730890.8006165, "terms": ["def", "load_rows", "path", "str", "list", "dict", "str", "any", "if", "not", "os", "path", "exists", "path", "print", "erro", "csv", "encontrado", "path", "file", "sys", "stderr", "sys", "exit", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "rows", "list", "reader", "if", "not", "rows", "print", "aviso", "csv", "est", "vazio", "file", "sys", "stderr", "return", "rows", "def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day"]}
{"chunk_id": "c164296d635332b1b2d5639b", "file_path": "scripts/summarize_metrics.py", "start_line": 62, "end_line": 141, "content": "def within_since(ts_iso: str, since_days: int) -> bool:\n    try:\n        # suporta 'YYYY-MM-DDTHH:MM:SS' (com/sem timezone)\n        d = dt.datetime.fromisoformat(ts_iso.replace(\"Z\",\"\"))\n    except Exception:\n        return True  # se n√£o der pra parsear, n√£o filtra\n    return (dt.datetime.utcnow() - d) <= dt.timedelta(days=since_days)\n\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},", "mtime": 1755730890.8006165, "terms": ["def", "within_since", "ts_iso", "str", "since_days", "int", "bool", "try", "suporta", "yyyy", "mm", "ddthh", "mm", "ss", "com", "sem", "timezone", "dt", "datetime", "fromisoformat", "ts_iso", "replace", "except", "exception", "return", "true", "se", "der", "pra", "parsear", "filtra", "return", "dt", "datetime", "utcnow", "dt", "timedelta", "days", "since_days", "def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end"]}
{"chunk_id": "3d007456816f195c051dc8e1", "file_path": "scripts/summarize_metrics.py", "start_line": 69, "end_line": 148, "content": "\ndef summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,", "mtime": 1755730890.8006165, "terms": ["def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else"]}
{"chunk_id": "e57ac7c53869f5b22c401d80", "file_path": "scripts/summarize_metrics.py", "start_line": 70, "end_line": 149, "content": "def summarize(rows: List[Dict[str, Any]], args):\n    # filtros\n    if args.since is not None:\n        rows = [r for r in rows if within_since(r.get(\"ts\",\"\"), args.since)]\n    if args.filter:\n        q = args.filter.lower()\n        rows = [r for r in rows if q in (r.get(\"query\",\"\").lower())]\n\n    if not rows:\n        return {\"overall\": {}, \"daily\": []}\n\n    # agrega por dia (YYYY-MM-DD)\n    by_day: Dict[str, List[Dict[str, Any]]] = {}\n    for r in rows:\n        ts = r.get(\"ts\",\"\")\n        day = ts[:10] if len(ts) >= 10 else \"unknown\"\n        by_day.setdefault(day, []).append(r)\n\n    # fun√ß√µes helpers\n    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,", "mtime": 1755730890.8006165, "terms": ["def", "summarize", "rows", "list", "dict", "str", "any", "args", "filtros", "if", "args", "since", "is", "not", "none", "rows", "for", "in", "rows", "if", "within_since", "get", "ts", "args", "since", "if", "args", "filter", "args", "filter", "lower", "rows", "for", "in", "rows", "if", "in", "get", "query", "lower", "if", "not", "rows", "return", "overall", "daily", "agrega", "por", "dia", "yyyy", "mm", "dd", "by_day", "dict", "str", "list", "dict", "str", "any", "for", "in", "rows", "ts", "get", "ts", "day", "ts", "if", "len", "ts", "else", "unknown", "by_day", "setdefault", "day", "append", "fun", "es", "helpers", "def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else"]}
{"chunk_id": "df4bc647c8cace9a5517460e", "file_path": "scripts/summarize_metrics.py", "start_line": 89, "end_line": 168, "content": "    def stats_for(values: List[float]) -> Dict[str, float]:\n        if not values:\n            return {\"avg\":0.0, \"med\":0.0, \"p95\":0.0, \"min\":0.0, \"max\":0.0}\n        return {\n            \"avg\": round(sum(values)/len(values), 2),\n            \"med\": round(st.median(values), 2),\n            \"p95\": round(p95(values), 2),\n            \"min\": round(min(values), 2),\n            \"max\": round(max(values), 2),\n        }\n\n    # di√°rio\n    daily = []\n    all_tokens = []\n    all_lat = []\n    all_chunks = []\n    period_start = None\n    period_end = None\n\n    for day in sorted(by_day.keys()):\n        rs = by_day[day]\n        toks = [coerce_int(r.get(\"total_tokens\",0)) for r in rs]\n        lat = [coerce_int(r.get(\"latency_ms\",0)) for r in rs]\n        chs = [coerce_int(r.get(\"chunk_count\",0)) for r in rs]\n        util = []\n        for r in rs:\n            try:\n                util.append(float(r.get(\"budget_utilization\", 0)))\n            except Exception:\n                pass\n\n        all_tokens.extend(toks)\n        all_lat.extend(lat)\n        all_chunks.extend(chs)\n\n        if period_start is None:\n            try:\n                period_start = day\n            except Exception:\n                pass\n        period_end = day\n\n        daily.append({\n            \"day\": day,\n            \"n\": len(rs),\n            \"tokens\": stats_for(toks),\n            \"latency_ms\": stats_for(lat),\n            \"chunk_count_avg\": round(sum(chs)/len(chs), 2) if chs else 0.0,\n            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,\n        },\n        \"latency_ms\": {\n            \"avg\": round(sum(all_lat)/len(all_lat), 2) if all_lat else 0.0,\n            \"med\": round(st.median(all_lat), 2) if all_lat else 0.0,\n            \"p95\": round(p95(all_lat), 2) if all_lat else 0.0,\n            \"min\": min(all_lat) if all_lat else 0,\n            \"max\": max(all_lat) if all_lat else 0,\n        },\n        \"chunk_count_avg\": round(sum(all_chunks)/len(all_chunks), 2) if all_chunks else 0.0,\n    }\n\n    return {\"overall\": overall, \"daily\": daily}\n\ndef print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:", "mtime": 1755730890.8006165, "terms": ["def", "stats_for", "values", "list", "float", "dict", "str", "float", "if", "not", "values", "return", "avg", "med", "p95", "min", "max", "return", "avg", "round", "sum", "values", "len", "values", "med", "round", "st", "median", "values", "p95", "round", "p95", "values", "min", "round", "min", "values", "max", "round", "max", "values", "di", "rio", "daily", "all_tokens", "all_lat", "all_chunks", "period_start", "none", "period_end", "none", "for", "day", "in", "sorted", "by_day", "keys", "rs", "by_day", "day", "toks", "coerce_int", "get", "total_tokens", "for", "in", "rs", "lat", "coerce_int", "get", "latency_ms", "for", "in", "rs", "chs", "coerce_int", "get", "chunk_count", "for", "in", "rs", "util", "for", "in", "rs", "try", "util", "append", "float", "get", "budget_utilization", "except", "exception", "pass", "all_tokens", "extend", "toks", "all_lat", "extend", "lat", "all_chunks", "extend", "chs", "if", "period_start", "is", "none", "try", "period_start", "day", "except", "exception", "pass", "period_end", "day", "daily", "append", "day", "day", "len", "rs", "tokens", "stats_for", "toks", "latency_ms", "stats_for", "lat", "chunk_count_avg", "round", "sum", "chs", "len", "chs", "if", "chs", "else", "budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else", "latency_ms", "avg", "round", "sum", "all_lat", "len", "all_lat", "if", "all_lat", "else", "med", "round", "st", "median", "all_lat", "if", "all_lat", "else", "p95", "round", "p95", "all_lat", "if", "all_lat", "else", "min", "min", "all_lat", "if", "all_lat", "else", "max", "max", "all_lat", "if", "all_lat", "else", "chunk_count_avg", "round", "sum", "all_chunks", "len", "all_chunks", "if", "all_chunks", "else", "return", "overall", "overall", "daily", "daily", "def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall"]}
{"chunk_id": "af0db25f4897325b04aed5c8", "file_path": "scripts/summarize_metrics.py", "start_line": 137, "end_line": 196, "content": "            \"budget_util_avg\": round(sum(util)/len(util), 3) if util else 0.0,\n        })\n\n    overall = {\n        \"period\": {\"start\": period_start, \"end\": period_end},\n        \"n\": len(rows),\n        \"tokens\": {\n            \"avg\": round(sum(all_tokens)/len(all_tokens), 2) if all_tokens else 0.0,\n            \"med\": round(st.median(all_tokens), 2) if all_tokens else 0.0,\n            \"p95\": round(p95(all_tokens), 2) if all_tokens else 0.0,\n            \"min\": min(all_tokens) if all_tokens else 0,\n            \"max\": max(all_tokens) if all_tokens else 0,\n            \"sum\": int(sum(all_tokens)) if all_tokens else 0,\n        },\n        \"latency_ms\": {\n            \"avg\": round(sum(all_lat)/len(all_lat), 2) if all_lat else 0.0,\n            \"med\": round(st.median(all_lat), 2) if all_lat else 0.0,\n            \"p95\": round(p95(all_lat), 2) if all_lat else 0.0,\n            \"min\": min(all_lat) if all_lat else 0,\n            \"max\": max(all_lat) if all_lat else 0,\n        },\n        \"chunk_count_avg\": round(sum(all_chunks)/len(all_chunks), 2) if all_chunks else 0.0,\n    }\n\n    return {\"overall\": overall, \"daily\": daily}\n\ndef print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:\n        print(\"sem dados ap√≥s filtros.\")\n        return\n\n    per = overall[\"period\"]\n    print(f\"Per√≠odo: {per.get('start','?')} ‚Üí {per.get('end','?')}\")\n    print(f\"N linhas: {overall['n']}\")\n    print(f\"Tokens  | avg: {overall['tokens']['avg']}, med: {overall['tokens']['med']}, p95: {overall['tokens']['p95']}, sum: {overall['tokens']['sum']}\")\n    print(f\"Lat(ms) | avg: {overall['latency_ms']['avg']}, med: {overall['latency_ms']['med']}, p95: {overall['latency_ms']['p95']}\")\n    print(f\"Chunk count m√©dio: {overall['chunk_count_avg']}\")\n    print(\"\\n=== DAILY ===\")\n    print(f\"{'day':<12} {'n':>4}  {'tok_avg':>8} {'tok_med':>8} {'tok_p95':>8}   {'lat_avg':>8} {'lat_p95':>8}   {'chunks_avg':>10} {'budget_u_avg':>12}\")\n    for d in daily:\n        print(f\"{d['day']:<12} {d['n']:>4}  \"\n              f\"{d['tokens']['avg']:>8} {d['tokens']['med']:>8} {d['tokens']['p95']:>8}   \"\n              f\"{d['latency_ms']['avg']:>8} {d['latency_ms']['p95']:>8}   \"\n              f\"{d['chunk_count_avg']:>10} {d['budget_util_avg']:>12}\")\n\ndef main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["budget_util_avg", "round", "sum", "util", "len", "util", "if", "util", "else", "overall", "period", "start", "period_start", "end", "period_end", "len", "rows", "tokens", "avg", "round", "sum", "all_tokens", "len", "all_tokens", "if", "all_tokens", "else", "med", "round", "st", "median", "all_tokens", "if", "all_tokens", "else", "p95", "round", "p95", "all_tokens", "if", "all_tokens", "else", "min", "min", "all_tokens", "if", "all_tokens", "else", "max", "max", "all_tokens", "if", "all_tokens", "else", "sum", "int", "sum", "all_tokens", "if", "all_tokens", "else", "latency_ms", "avg", "round", "sum", "all_lat", "len", "all_lat", "if", "all_lat", "else", "med", "round", "st", "median", "all_lat", "if", "all_lat", "else", "p95", "round", "p95", "all_lat", "if", "all_lat", "else", "min", "min", "all_lat", "if", "all_lat", "else", "max", "max", "all_lat", "if", "all_lat", "else", "chunk_count_avg", "round", "sum", "all_chunks", "len", "all_chunks", "if", "all_chunks", "else", "return", "overall", "overall", "daily", "daily", "def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall", "print", "sem", "dados", "ap", "filtros", "return", "per", "overall", "period", "print", "per", "odo", "per", "get", "start", "per", "get", "end", "print", "linhas", "overall", "print", "tokens", "avg", "overall", "tokens", "avg", "med", "overall", "tokens", "med", "p95", "overall", "tokens", "p95", "sum", "overall", "tokens", "sum", "print", "lat", "ms", "avg", "overall", "latency_ms", "avg", "med", "overall", "latency_ms", "med", "p95", "overall", "latency_ms", "p95", "print", "chunk", "count", "dio", "overall", "chunk_count_avg", "print", "daily", "print", "day", "tok_avg", "tok_med", "tok_p95", "lat_avg", "lat_p95", "chunks_avg", "budget_u_avg", "for", "in", "daily", "print", "day", "tokens", "avg", "tokens", "med", "tokens", "p95", "latency_ms", "avg", "latency_ms", "p95", "chunk_count_avg", "budget_util_avg", "def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "ce1b8335ec9e5c9ab0fc23e1", "file_path": "scripts/summarize_metrics.py", "start_line": 163, "end_line": 196, "content": "def print_table(summary: Dict[str, Any]):\n    overall = summary.get(\"overall\", {})\n    daily = summary.get(\"daily\", [])\n\n    print(\"\\n=== OVERALL ===\")\n    if not overall:\n        print(\"sem dados ap√≥s filtros.\")\n        return\n\n    per = overall[\"period\"]\n    print(f\"Per√≠odo: {per.get('start','?')} ‚Üí {per.get('end','?')}\")\n    print(f\"N linhas: {overall['n']}\")\n    print(f\"Tokens  | avg: {overall['tokens']['avg']}, med: {overall['tokens']['med']}, p95: {overall['tokens']['p95']}, sum: {overall['tokens']['sum']}\")\n    print(f\"Lat(ms) | avg: {overall['latency_ms']['avg']}, med: {overall['latency_ms']['med']}, p95: {overall['latency_ms']['p95']}\")\n    print(f\"Chunk count m√©dio: {overall['chunk_count_avg']}\")\n    print(\"\\n=== DAILY ===\")\n    print(f\"{'day':<12} {'n':>4}  {'tok_avg':>8} {'tok_med':>8} {'tok_p95':>8}   {'lat_avg':>8} {'lat_p95':>8}   {'chunks_avg':>10} {'budget_u_avg':>12}\")\n    for d in daily:\n        print(f\"{d['day']:<12} {d['n']:>4}  \"\n              f\"{d['tokens']['avg']:>8} {d['tokens']['med']:>8} {d['tokens']['p95']:>8}   \"\n              f\"{d['latency_ms']['avg']:>8} {d['latency_ms']['p95']:>8}   \"\n              f\"{d['chunk_count_avg']:>10} {d['budget_util_avg']:>12}\")\n\ndef main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["def", "print_table", "summary", "dict", "str", "any", "overall", "summary", "get", "overall", "daily", "summary", "get", "daily", "print", "overall", "if", "not", "overall", "print", "sem", "dados", "ap", "filtros", "return", "per", "overall", "period", "print", "per", "odo", "per", "get", "start", "per", "get", "end", "print", "linhas", "overall", "print", "tokens", "avg", "overall", "tokens", "avg", "med", "overall", "tokens", "med", "p95", "overall", "tokens", "p95", "sum", "overall", "tokens", "sum", "print", "lat", "ms", "avg", "overall", "latency_ms", "avg", "med", "overall", "latency_ms", "med", "p95", "overall", "latency_ms", "p95", "print", "chunk", "count", "dio", "overall", "chunk_count_avg", "print", "daily", "print", "day", "tok_avg", "tok_med", "tok_p95", "lat_avg", "lat_p95", "chunks_avg", "budget_u_avg", "for", "in", "daily", "print", "day", "tokens", "avg", "tokens", "med", "tokens", "p95", "latency_ms", "avg", "latency_ms", "p95", "chunk_count_avg", "budget_util_avg", "def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "5c25648a28b0bafcbc0cc442", "file_path": "scripts/summarize_metrics.py", "start_line": 186, "end_line": 196, "content": "def main():\n    args = parse_args()\n    rows = load_rows(args.file)\n    summary = summarize(rows, args)\n    if args.json:\n        print(json.dumps(summary, ensure_ascii=False, indent=2))\n    else:\n        print_table(summary)\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1755730890.8006165, "terms": ["def", "main", "args", "parse_args", "rows", "load_rows", "args", "file", "summary", "summarize", "rows", "args", "if", "args", "json", "print", "json", "dumps", "summary", "ensure_ascii", "false", "indent", "else", "print_table", "summary", "if", "__name__", "__main__", "main"]}
{"chunk_id": "edcf9674cd750796bdb752c5", "file_path": "scripts/mcp_client_stats.py", "start_line": 1, "end_line": 49, "content": "#!/usr/bin/env python3\n\"\"\"\nCliente MCP para obter estat√≠sticas do sistema\n\"\"\"\n\nimport json\nimport asyncio\nimport os\nimport time\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def get_stats():\n    \"\"\"Obt√©m estat√≠sticas do servidor MCP\"\"\"\n    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n    server_path = os.path.join(base_dir, 'mcp_server_enhanced.py')\n\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[\"-u\", server_path],\n        env={\"INDEX_DIR\": \".mcp_index\", \"INDEX_ROOT\": base_dir}\n    )\n\n    max_retries = 10\n    retry_delay = 1  # segundos\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            for attempt in range(max_retries):\n                try:\n                    tools = await session.list_tools()\n                    print(\"Ferramentas dispon√≠veis:\")\n                    for tool in tools:\n                        print(f\"  - {tool.name}: {tool.description}\")\n\n                    print(\"\\nExecutando comando get_stats...\")\n                    result = await session.call_tool(\"get_stats\", {})\n                    print(\"Resultado:\")\n                    print(json.dumps(result, indent=2, ensure_ascii=False))\n                    break\n                except Exception as e:\n                    print(f\"Tentativa {attempt+1} falhou: {e}\")\n                    if attempt == max_retries - 1:\n                        print(\"Falha ao executar comando ap√≥s v√°rias tentativas.\")\n                        break\n                    await asyncio.sleep(retry_delay)\n\nif __name__ == \"__main__\":\n    asyncio.run(get_stats())", "mtime": 1755727193.3613484, "terms": ["usr", "bin", "env", "python3", "cliente", "mcp", "para", "obter", "estat", "sticas", "do", "sistema", "import", "json", "import", "asyncio", "import", "os", "import", "time", "from", "mcp", "import", "clientsession", "stdioserverparameters", "from", "mcp", "client", "stdio", "import", "stdio_client", "async", "def", "get_stats", "obt", "estat", "sticas", "do", "servidor", "mcp", "base_dir", "os", "path", "abspath", "os", "path", "join", "os", "path", "dirname", "__file__", "server_path", "os", "path", "join", "base_dir", "mcp_server_enhanced", "py", "server_params", "stdioserverparameters", "command", "python", "args", "server_path", "env", "index_dir", "mcp_index", "index_root", "base_dir", "max_retries", "retry_delay", "segundos", "async", "with", "stdio_client", "server_params", "as", "read", "write", "async", "with", "clientsession", "read", "write", "as", "session", "for", "attempt", "in", "range", "max_retries", "try", "tools", "await", "session", "list_tools", "print", "ferramentas", "dispon", "veis", "for", "tool", "in", "tools", "print", "tool", "name", "tool", "description", "print", "nexecutando", "comando", "get_stats", "result", "await", "session", "call_tool", "get_stats", "print", "resultado", "print", "json", "dumps", "result", "indent", "ensure_ascii", "false", "break", "except", "exception", "as", "print", "tentativa", "attempt", "falhou", "if", "attempt", "max_retries", "print", "falha", "ao", "executar", "comando", "ap", "rias", "tentativas", "break", "await", "asyncio", "sleep", "retry_delay", "if", "__name__", "__main__", "asyncio", "run", "get_stats"]}
{"chunk_id": "0470f4a230ae57ec18418e49", "file_path": "scripts/get_stats.py", "start_line": 1, "end_line": 34, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para obter estat√≠sticas do sistema MCP\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\n\ndef get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estat√≠sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisi√ß√£o para o servidor MCP...\")\n    print(f\"Requisi√ß√£o: {request_str}\")\n    \n    # Aqui voc√™ precisaria se conectar ao servidor MCP\n    # Esta √© uma implementa√ß√£o simplificada\n    print(\"\\nPara executar este comando, voc√™ pode:\")\n    print(\"1. Usar a interface do VS Code com a extens√£o MCP\")\n    print(\"2. Enviar a requisi√ß√£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["usr", "bin", "env", "python3", "script", "para", "obter", "estat", "sticas", "do", "sistema", "mcp", "import", "json", "import", "subprocess", "import", "sys", "def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "2488ba58a4cb868d786ef089", "file_path": "scripts/get_stats.py", "start_line": 10, "end_line": 34, "content": "def get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estat√≠sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisi√ß√£o para o servidor MCP...\")\n    print(f\"Requisi√ß√£o: {request_str}\")\n    \n    # Aqui voc√™ precisaria se conectar ao servidor MCP\n    # Esta √© uma implementa√ß√£o simplificada\n    print(\"\\nPara executar este comando, voc√™ pode:\")\n    print(\"1. Usar a interface do VS Code com a extens√£o MCP\")\n    print(\"2. Enviar a requisi√ß√£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "27744efa5de470a8f5dfd7c0", "file_path": "reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diret√≥rio de √≠ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de m√©tricas (MCP_METRICS_FILE ou .mcp_index/metrics.csv).\n- (Opcional) Calcula baseline aproximada de tokens do reposit√≥rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# ---- Config m√©tricas (mesmo padr√£o do context_pack)\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", \".mcp_index/metrics.csv\")\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)", "mtime": 1755731436.0803573, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "config", "tricas", "mesmo", "padr", "do", "context_pack", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir"]}
{"chunk_id": "e7c774b75b4e9f5911cce40a", "file_path": "reindex.py", "start_line": 34, "end_line": 113, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1755731436.0803573, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "9ac068e6f70e05580089c4e7", "file_path": "reindex.py", "start_line": 43, "end_line": 122, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1755731436.0803573, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "39f9fe55db0355f67dfe83d9", "file_path": "reindex.py", "start_line": 47, "end_line": 126, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=\".mcp_index\", help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1755731436.0803573, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "b3aece1ad5078bf1465c4c3e", "file_path": "reindex.py", "start_line": 65, "end_line": 144, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = base_dir / args.index_dir\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"üìä Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1755731436.0803573, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "base_dir", "args", "index_dir", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "38ca7d55ab9ac0ff23c6946b", "file_path": "reindex.py", "start_line": 69, "end_line": 148, "content": "\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"üìä Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,", "mtime": 1755731436.0803573, "terms": ["importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed"]}
{"chunk_id": "aad8e65810028ee77501b919", "file_path": "reindex.py", "start_line": 137, "end_line": 163, "content": "    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # n√£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1755731436.0803573, "terms": ["loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
