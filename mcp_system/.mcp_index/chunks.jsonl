{"chunk_id": "53867eaffed5fefed0517c2e", "file_path": "main.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nCLI principal para an√°lise de extratos banc√°rios.\n\"\"\"\nimport sys\nfrom pathlib import Path\nimport click\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.panel import Panel\n\n# Adiciona o diret√≥rio raiz ao path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom src.application.use_cases import ExtractAnalyzer\nfrom src.domain.exceptions import DomainException\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nconsole = Console()\n\n\n@click.group()\ndef cli():\n    \"\"\"Sistema de An√°lise de Extratos Banc√°rios\"\"\"\n    pass\n\n\n@cli.command()\n@click.argument('file_path', type=click.Path(exists=True))\n@click.option('--output', '-o', help='Caminho para salvar o relat√≥rio')\n@click.option('--format', '-f', type=click.Choice(['text', 'markdown']), default='text', help='Formato do relat√≥rio')\ndef analyze(file_path, output, format):\n    \"\"\"Analisa um extrato banc√°rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]‚úì An√°lise conclu√≠da![/bold green]\\n\\n\"\n            f\"üìä Total de transa√ß√µes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"üí∞ Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"üí∏ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"üìà Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"‚ö†Ô∏è  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"üí° Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relat√≥rio completo\n        if output:\n            console.print(f\"[green]Relat√≥rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]Relat√≥rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:", "mtime": 1756147266.072644, "terms": ["usr", "bin", "env", "python3", "cli", "principal", "para", "an", "lise", "de", "extratos", "banc", "rios", "import", "sys", "from", "pathlib", "import", "path", "import", "click", "from", "rich", "console", "import", "console", "from", "rich", "table", "import", "table", "from", "rich", "panel", "import", "panel", "adiciona", "diret", "rio", "raiz", "ao", "path", "sys", "path", "insert", "str", "path", "__file__", "parent", "parent", "from", "src", "application", "use_cases", "import", "extractanalyzer", "from", "src", "domain", "exceptions", "import", "domainexception", "from", "src", "utils", "currency_utils", "import", "currencyutils", "console", "console", "click", "group", "def", "cli", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "pass", "cli", "command", "click", "argument", "file_path", "type", "click", "path", "exists", "true", "click", "option", "output", "help", "caminho", "para", "salvar", "relat", "rio", "click", "option", "format", "type", "click", "choice", "text", "markdown", "default", "text", "help", "formato", "do", "relat", "rio", "def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as"]}
{"chunk_id": "0bad8fef21a512c3f8890f26", "file_path": "main.py", "start_line": 24, "end_line": 103, "content": "def cli():\n    \"\"\"Sistema de An√°lise de Extratos Banc√°rios\"\"\"\n    pass\n\n\n@cli.command()\n@click.argument('file_path', type=click.Path(exists=True))\n@click.option('--output', '-o', help='Caminho para salvar o relat√≥rio')\n@click.option('--format', '-f', type=click.Choice(['text', 'markdown']), default='text', help='Formato do relat√≥rio')\ndef analyze(file_path, output, format):\n    \"\"\"Analisa um extrato banc√°rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]‚úì An√°lise conclu√≠da![/bold green]\\n\\n\"\n            f\"üìä Total de transa√ß√µes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"üí∞ Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"üí∏ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"üìà Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"‚ö†Ô∏è  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"üí° Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relat√≥rio completo\n        if output:\n            console.print(f\"[green]Relat√≥rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]Relat√≥rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de dom√≠nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instru√ß√µes de exemplo.\"\"\"\n    instructions = \"\"\"# Instru√ß√µes para uso do Sistema de An√°lise de Extratos Banc√°rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:", "mtime": 1756147266.072644, "terms": ["def", "cli", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "pass", "cli", "command", "click", "argument", "file_path", "type", "click", "path", "exists", "true", "click", "option", "output", "help", "caminho", "para", "salvar", "relat", "rio", "click", "option", "format", "type", "click", "choice", "text", "markdown", "default", "text", "help", "formato", "do", "relat", "rio", "def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas"]}
{"chunk_id": "8491bc1b2b7691aa3aa4e9b9", "file_path": "main.py", "start_line": 33, "end_line": 112, "content": "def analyze(file_path, output, format):\n    \"\"\"Analisa um extrato banc√°rio em PDF, Excel ou CSV.\"\"\"\n    try:\n        analyzer = ExtractAnalyzer()\n\n        # Se especificou formato markdown, troca o gerador\n        if format == 'markdown':\n            from src.infrastructure.reports.text_report import MarkdownReportGenerator\n            analyzer.use_case.report_generator = MarkdownReportGenerator()\n\n        result, report, statement = analyzer.analyze_file(file_path, output)\n        \n        # Formata valores com a moeda correta\n        currency_symbol = CurrencyUtils.get_currency_symbol(result.currency)\n        \n        # Mostra resumo no console\n        console.print(Panel.fit(\n            f\"[bold green]‚úì An√°lise conclu√≠da![/bold green]\\n\\n\"\n            f\"üìä Total de transa√ß√µes: {result.metadata.get('transaction_count', 0)}\\n\"\n            f\"üí∞ Receitas: {currency_symbol}{result.total_income:,.2f}\\n\"\n            f\"üí∏ Despesas: {currency_symbol}{result.total_expenses:,.2f}\\n\"\n            f\"üìà Saldo: {currency_symbol}{result.net_flow:,.2f}\"\n        ))\n        \n        # Mostra alertas, se houver\n        if result.alerts:\n            alert_table = Table(title=\"‚ö†Ô∏è  Alertas\", style=\"yellow\")\n            alert_table.add_column(\"Alerta\", style=\"yellow\")\n            for alert in result.alerts:\n                alert_table.add_row(alert)\n            console.print(alert_table)\n        \n        # Mostra insights, se houver\n        if result.insights:\n            insight_table = Table(title=\"üí° Insights\")\n            insight_table.add_column(\"Insight\", style=\"cyan\")\n            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relat√≥rio completo\n        if output:\n            console.print(f\"[green]Relat√≥rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]Relat√≥rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de dom√≠nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instru√ß√µes de exemplo.\"\"\"\n    instructions = \"\"\"# Instru√ß√µes para uso do Sistema de An√°lise de Extratos Banc√°rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas Obrigat√≥rias:\n- Data da transa√ß√£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- Descri√ß√£o da transa√ß√£o\n- Valor da transa√ß√£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo ap√≥s a transa√ß√£o\n- N√∫mero da conta", "mtime": 1756147266.072644, "terms": ["def", "analyze", "file_path", "output", "format", "analisa", "um", "extrato", "banc", "rio", "em", "pdf", "excel", "ou", "csv", "try", "analyzer", "extractanalyzer", "se", "especificou", "formato", "markdown", "troca", "gerador", "if", "format", "markdown", "from", "src", "infrastructure", "reports", "text_report", "import", "markdownreportgenerator", "analyzer", "use_case", "report_generator", "markdownreportgenerator", "result", "report", "statement", "analyzer", "analyze_file", "file_path", "output", "formata", "valores", "com", "moeda", "correta", "currency_symbol", "currencyutils", "get_currency_symbol", "result", "currency", "mostra", "resumo", "no", "console", "console", "print", "panel", "fit", "bold", "green", "an", "lise", "conclu", "da", "bold", "green", "total", "de", "transa", "es", "result", "metadata", "get", "transaction_count", "receitas", "currency_symbol", "result", "total_income", "despesas", "currency_symbol", "result", "total_expenses", "saldo", "currency_symbol", "result", "net_flow", "mostra", "alertas", "se", "houver", "if", "result", "alerts", "alert_table", "table", "title", "alertas", "style", "yellow", "alert_table", "add_column", "alerta", "style", "yellow", "for", "alert", "in", "result", "alerts", "alert_table", "add_row", "alert", "console", "print", "alert_table", "mostra", "insights", "se", "houver", "if", "result", "insights", "insight_table", "table", "title", "insights", "insight_table", "add_column", "insight", "style", "cyan", "for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta"]}
{"chunk_id": "971ab0d6d8d0380179867bec", "file_path": "main.py", "start_line": 69, "end_line": 148, "content": "            for insight in result.insights:\n                insight_table.add_row(insight)\n            console.print(insight_table)\n        \n        # Salva ou mostra o relat√≥rio completo\n        if output:\n            console.print(f\"[green]Relat√≥rio salvo em:[/green] {output}\")\n        else:\n            console.print(\"\\n[bold]Relat√≥rio completo:[/bold]\\n\")\n            console.print(report)\n            \n    except DomainException as e:\n        console.print(f\"[red]Erro de dom√≠nio:[/red] {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        console.print(f\"[red]Erro inesperado:[/red] {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\n@click.argument('output_path', type=click.Path())\ndef sample(output_path):\n    \"\"\"Cria um arquivo de instru√ß√µes de exemplo.\"\"\"\n    instructions = \"\"\"# Instru√ß√µes para uso do Sistema de An√°lise de Extratos Banc√°rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas Obrigat√≥rias:\n- Data da transa√ß√£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- Descri√ß√£o da transa√ß√£o\n- Valor da transa√ß√£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo ap√≥s a transa√ß√£o\n- N√∫mero da conta\n- Saldo inicial/final\n\n### Nomes de Colunas Aceitos:\n\n#### Para Data:\n- data\n- date\n- data transacao\n- transaction date\n\n#### Para Descri√ß√£o:\n- descricao\n- description\n- descri√ß√£o\n\n#### Para Valor:\n- valor\n- amount\n- value\n- montante\n\n#### Para Saldo:\n- saldo\n- balance\n- saldo ap√≥s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,Sal√°rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:", "mtime": 1756147266.072644, "terms": ["for", "insight", "in", "result", "insights", "insight_table", "add_row", "insight", "console", "print", "insight_table", "salva", "ou", "mostra", "relat", "rio", "completo", "if", "output", "console", "print", "green", "relat", "rio", "salvo", "em", "green", "output", "else", "console", "print", "bold", "relat", "rio", "completo", "bold", "console", "print", "report", "except", "domainexception", "as", "console", "print", "red", "erro", "de", "dom", "nio", "red", "str", "sys", "exit", "except", "exception", "as", "console", "print", "red", "erro", "inesperado", "red", "str", "sys", "exit", "cli", "command", "click", "argument", "output_path", "type", "click", "path", "def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta", "saldo", "inicial", "final", "nomes", "de", "colunas", "aceitos", "para", "data", "data", "date", "data", "transacao", "transaction", "date", "para", "descri", "descricao", "description", "descri", "para", "valor", "valor", "amount", "value", "montante", "para", "saldo", "saldo", "balance", "saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas"]}
{"chunk_id": "439c634265dc1b6aff52cfd2", "file_path": "main.py", "start_line": 90, "end_line": 169, "content": "def sample(output_path):\n    \"\"\"Cria um arquivo de instru√ß√µes de exemplo.\"\"\"\n    instructions = \"\"\"# Instru√ß√µes para uso do Sistema de An√°lise de Extratos Banc√°rios\n\n## Formatos Suportados\n\nO sistema suporta os seguintes formatos de extrato:\n1. PDF - Extratos em formato PDF\n2. Excel - Extratos em formato XLSX ou XLS\n3. CSV - Extratos em formato CSV\n\n## Estrutura Esperada para CSV\n\nPara que o sistema possa processar corretamente um arquivo CSV, ele deve conter as seguintes colunas:\n\n### Colunas Obrigat√≥rias:\n- Data da transa√ß√£o (formatos aceitos: DD/MM/YYYY, DD-MM-YYYY, YYYY-MM-DD)\n- Descri√ß√£o da transa√ß√£o\n- Valor da transa√ß√£o (positivo para receitas, negativo para despesas)\n\n### Colunas Opcionais:\n- Saldo ap√≥s a transa√ß√£o\n- N√∫mero da conta\n- Saldo inicial/final\n\n### Nomes de Colunas Aceitos:\n\n#### Para Data:\n- data\n- date\n- data transacao\n- transaction date\n\n#### Para Descri√ß√£o:\n- descricao\n- description\n- descri√ß√£o\n\n#### Para Valor:\n- valor\n- amount\n- value\n- montante\n\n#### Para Saldo:\n- saldo\n- balance\n- saldo ap√≥s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,Sal√°rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:\n\nO sistema detecta automaticamente a moeda do extrato:\n- EUR (Euro) - Padr√£o\n- USD (D√≥lar Americano)\n- BRL (Real Brasileiro)\n- GBP (Libra Esterlina)\n- JPY (Iene Japon√™s)\n- CHF (Franco Su√≠√ßo)\n- CAD (D√≥lar Canadense)\n- AUD (D√≥lar Australiano)\n\n## Executando a An√°lise:\n\nPara analisar um extrato, use o comando:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.pdf\npython main.py analyze caminho/para/seu/extrato.xlsx\npython main.py analyze caminho/para/seu/extrato.csv\n```\n", "mtime": 1756147266.072644, "terms": ["def", "sample", "output_path", "cria", "um", "arquivo", "de", "instru", "es", "de", "exemplo", "instructions", "instru", "es", "para", "uso", "do", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "formatos", "suportados", "sistema", "suporta", "os", "seguintes", "formatos", "de", "extrato", "pdf", "extratos", "em", "formato", "pdf", "excel", "extratos", "em", "formato", "xlsx", "ou", "xls", "csv", "extratos", "em", "formato", "csv", "estrutura", "esperada", "para", "csv", "para", "que", "sistema", "possa", "processar", "corretamente", "um", "arquivo", "csv", "ele", "deve", "conter", "as", "seguintes", "colunas", "colunas", "obrigat", "rias", "data", "da", "transa", "formatos", "aceitos", "dd", "mm", "yyyy", "dd", "mm", "yyyy", "yyyy", "mm", "dd", "descri", "da", "transa", "valor", "da", "transa", "positivo", "para", "receitas", "negativo", "para", "despesas", "colunas", "opcionais", "saldo", "ap", "transa", "mero", "da", "conta", "saldo", "inicial", "final", "nomes", "de", "colunas", "aceitos", "para", "data", "data", "date", "data", "transacao", "transaction", "date", "para", "descri", "descricao", "description", "descri", "para", "valor", "valor", "amount", "value", "montante", "para", "saldo", "saldo", "balance", "saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas", "sistema", "detecta", "automaticamente", "moeda", "do", "extrato", "eur", "euro", "padr", "usd", "lar", "americano", "brl", "real", "brasileiro", "gbp", "libra", "esterlina", "jpy", "iene", "japon", "chf", "franco", "su", "cad", "lar", "canadense", "aud", "lar", "australiano", "executando", "an", "lise", "para", "analisar", "um", "extrato", "use", "comando", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "pdf", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "xlsx", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv"]}
{"chunk_id": "b4d598870f59f1608de3732a", "file_path": "main.py", "start_line": 137, "end_line": 201, "content": "- saldo ap√≥s\n- balance after\n\n## Exemplo de Estrutura CSV:\n\ndata,descricao,valor,saldo\n01/01/2023,Sal√°rio Janeiro,2500.00,2500.00\n02/01/2023,Supermercado,-150.50,2349.50\n03/01/2023,Conta de Luz,-80.00,2269.50\n05/01/2023,Restaurante,-65.75,2203.75\n\n## Moedas Suportadas:\n\nO sistema detecta automaticamente a moeda do extrato:\n- EUR (Euro) - Padr√£o\n- USD (D√≥lar Americano)\n- BRL (Real Brasileiro)\n- GBP (Libra Esterlina)\n- JPY (Iene Japon√™s)\n- CHF (Franco Su√≠√ßo)\n- CAD (D√≥lar Canadense)\n- AUD (D√≥lar Australiano)\n\n## Executando a An√°lise:\n\nPara analisar um extrato, use o comando:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.pdf\npython main.py analyze caminho/para/seu/extrato.xlsx\npython main.py analyze caminho/para/seu/extrato.csv\n```\n\nPara salvar o relat√≥rio em um arquivo:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.csv --output relatorio.txt\n```\n\nPara gerar um relat√≥rio em formato Markdown:\n\n```bash\npython main.py analyze caminho/para/seu/extrato.csv --format markdown --output relatorio.md\n```\n\"\"\"\n    \n    try:\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(instructions)\n        console.print(f\"[green]‚úì[/green] Arquivo de instru√ß√µes criado em: {output_path}\")\n    except Exception as e:\n        console.print(f\"[red]‚úó[/red] Erro ao criar arquivo de instru√ß√µes: {str(e)}\")\n        sys.exit(1)\n\n\n@cli.command()\ndef version():\n    \"\"\"Mostra a vers√£o do sistema.\"\"\"\n    console.print(\"[bold blue]Sistema de An√°lise de Extratos Banc√°rios[/bold blue]\")\n    console.print(\"Vers√£o: 1.0.0\")\n    console.print(\"Formatos suportados: PDF, Excel, CSV\")\n\n\nif __name__ == '__main__':\n    cli()", "mtime": 1756147266.072644, "terms": ["saldo", "ap", "balance", "after", "exemplo", "de", "estrutura", "csv", "data", "descricao", "valor", "saldo", "sal", "rio", "janeiro", "supermercado", "conta", "de", "luz", "restaurante", "moedas", "suportadas", "sistema", "detecta", "automaticamente", "moeda", "do", "extrato", "eur", "euro", "padr", "usd", "lar", "americano", "brl", "real", "brasileiro", "gbp", "libra", "esterlina", "jpy", "iene", "japon", "chf", "franco", "su", "cad", "lar", "canadense", "aud", "lar", "australiano", "executando", "an", "lise", "para", "analisar", "um", "extrato", "use", "comando", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "pdf", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "xlsx", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "para", "salvar", "relat", "rio", "em", "um", "arquivo", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "output", "relatorio", "txt", "para", "gerar", "um", "relat", "rio", "em", "formato", "markdown", "bash", "python", "main", "py", "analyze", "caminho", "para", "seu", "extrato", "csv", "format", "markdown", "output", "relatorio", "md", "try", "with", "open", "output_path", "encoding", "utf", "as", "write", "instructions", "console", "print", "green", "green", "arquivo", "de", "instru", "es", "criado", "em", "output_path", "except", "exception", "as", "console", "print", "red", "red", "erro", "ao", "criar", "arquivo", "de", "instru", "es", "str", "sys", "exit", "cli", "command", "def", "version", "mostra", "vers", "do", "sistema", "console", "print", "bold", "blue", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "bold", "blue", "console", "print", "vers", "console", "print", "formatos", "suportados", "pdf", "excel", "csv", "if", "__name__", "__main__", "cli"]}
{"chunk_id": "66f3f21ef64861b50fccee07", "file_path": "main.py", "start_line": 193, "end_line": 201, "content": "def version():\n    \"\"\"Mostra a vers√£o do sistema.\"\"\"\n    console.print(\"[bold blue]Sistema de An√°lise de Extratos Banc√°rios[/bold blue]\")\n    console.print(\"Vers√£o: 1.0.0\")\n    console.print(\"Formatos suportados: PDF, Excel, CSV\")\n\n\nif __name__ == '__main__':\n    cli()", "mtime": 1756147266.072644, "terms": ["def", "version", "mostra", "vers", "do", "sistema", "console", "print", "bold", "blue", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "bold", "blue", "console", "print", "vers", "console", "print", "formatos", "suportados", "pdf", "excel", "csv", "if", "__name__", "__main__", "cli"]}
{"chunk_id": "9fd3d99e3242e92e7fa99c64", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 1, "end_line": 80, "content": "# mcp_server_enhanced.py\n\"\"\"\nServidor MCP melhorado com busca sem√¢ntica e auto-indexa√ß√£o\nUsando FastMCP para API simplificada com decorators\n\"\"\"\n\nimport os\nimport sys\nfrom typing import Any, Dict, List\nimport pathlib\nimport threading\nimport csv\nimport datetime as dt\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from mcp.server.fastmcp import FastMCP\n    HAS_MCP = True\n    HAS_FASTMCP = True\nexcept ImportError:\n    try:\n        # Fallback para vers√µes mais antigas\n        from mcp.server import Server\n        from mcp import types\n        HAS_MCP = True\n        HAS_FASTMCP = False\n    except ImportError:\n        sys.stderr.write(\"[mcp_server_enhanced] ERROR: MCP SDK n√£o encontrado. Instale `mcp`.\\n\")\n        raise\n\n# Importa funcionalidades melhoradas\ntry:\n    from .code_indexer_enhanced import (\n        EnhancedCodeIndexer,\n        enhanced_search_code,\n        enhanced_build_context_pack,\n        enhanced_index_repo_paths,\n        BaseCodeIndexer,\n        search_code,\n        build_context_pack,\n        index_repo_paths\n    )\n    HAS_ENHANCED = True\n    sys.stderr.write(\"[mcp_server_enhanced] ‚úÖ Funcionalidades melhoradas carregadas\\n\")\nexcept ImportError as e:\n    sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Funcionalidades melhoradas n√£o dispon√≠veis: {e}\\n\")\n    sys.stderr.write(\"[mcp_server_enhanced] üîÑ Usando vers√£o base integrada\\n\")\n\n    # Fallback para vers√£o base integrada\n    try:\n        from .code_indexer_enhanced import (\n            BaseCodeIndexer,\n            search_code,\n            build_context_pack,\n            index_repo_paths\n        )\n        HAS_ENHANCED = False\n    except ImportError as e2:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ùå Erro cr√≠tico: {e2}\\n\")\n        raise\n\nHAS_ENHANCED_FEATURES = HAS_ENHANCED\n\n# Config / inst√¢ncias - agora usando caminhos relativos √† pasta mcp_system\nINDEX_DIR = os.environ.get(\"INDEX_DIR\", str(CURRENT_DIR / \".mcp_index\"))\nINDEX_ROOT = os.environ.get(\"INDEX_ROOT\", str(CURRENT_DIR.parent))\n\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXA√á√ÉO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:", "mtime": 1756161371.7068532, "terms": ["mcp_server_enhanced", "py", "servidor", "mcp", "melhorado", "com", "busca", "sem", "ntica", "auto", "indexa", "usando", "fastmcp", "para", "api", "simplificada", "com", "decorators", "import", "os", "import", "sys", "from", "typing", "import", "any", "dict", "list", "import", "pathlib", "import", "threading", "import", "csv", "import", "datetime", "as", "dt", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "mcp", "server", "fastmcp", "import", "fastmcp", "has_mcp", "true", "has_fastmcp", "true", "except", "importerror", "try", "fallback", "para", "vers", "es", "mais", "antigas", "from", "mcp", "server", "import", "server", "from", "mcp", "import", "types", "has_mcp", "true", "has_fastmcp", "false", "except", "importerror", "sys", "stderr", "write", "mcp_server_enhanced", "error", "mcp", "sdk", "encontrado", "instale", "mcp", "raise", "importa", "funcionalidades", "melhoradas", "try", "from", "code_indexer_enhanced", "import", "enhancedcodeindexer", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "true", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "carregadas", "except", "importerror", "as", "sys", "stderr", "write", "mcp_server_enhanced", "funcionalidades", "melhoradas", "dispon", "veis", "sys", "stderr", "write", "mcp_server_enhanced", "usando", "vers", "base", "integrada", "fallback", "para", "vers", "base", "integrada", "try", "from", "code_indexer_enhanced", "import", "basecodeindexer", "search_code", "build_context_pack", "index_repo_paths", "has_enhanced", "false", "except", "importerror", "as", "e2", "sys", "stderr", "write", "mcp_server_enhanced", "erro", "cr", "tico", "e2", "raise", "has_enhanced_features", "has_enhanced", "config", "inst", "ncias", "agora", "usando", "caminhos", "relativos", "pasta", "mcp_system", "index_dir", "os", "environ", "get", "index_dir", "str", "current_dir", "mcp_index", "index_root", "os", "environ", "get", "index_root", "str", "current_dir", "parent", "if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool"]}
{"chunk_id": "64a71319d8df2510c04747c2", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 69, "end_line": 148, "content": "\nif HAS_ENHANCED_FEATURES:\n    _indexer = EnhancedCodeIndexer(\n        index_dir=INDEX_DIR,\n        repo_root=INDEX_ROOT\n    )\nelse:\n    _indexer = BaseCodeIndexer(index_dir=INDEX_DIR)\n\n# ===== CONFIG DE AUTO-INDEXA√á√ÉO NO START =====\n\ndef _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] üöÄ Iniciando indexa√ß√£o autom√°tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        # Executa indexa√ß√£o inicial e captura resultado\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            result = index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] ‚úÖ Indexa√ß√£o inicial conclu√≠da\\n\")\n\n        # Registrar m√©tricas da indexa√ß√£o inicial em metrics_index.csv\n        try:\n            index_dir = getattr(_indexer, 'index_dir', None)\n            if not index_dir and hasattr(_indexer, 'base_indexer'):\n                index_dir = getattr(_indexer.base_indexer, 'index_dir', None)\n            index_dir_str = str(index_dir) if index_dir else str(CURRENT_DIR / \".mcp_index\")\n\n            metrics_dir = pathlib.Path(index_dir_str)\n            metrics_dir.mkdir(parents=True, exist_ok=True)\n            metrics_path = metrics_dir / \"metrics_index.csv\"\n\n            row = {\n                \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n                \"op\": \"initial_index\",\n                \"path\": os.pathsep.join(AUTO_INDEX_PATHS),\n                \"index_dir\": index_dir_str,\n                \"files_indexed\": int(result.get(\"files_indexed\", 0)) if isinstance(result, dict) else 0,\n                \"chunks\": int(result.get(\"chunks\", 0)) if isinstance(result, dict) else 0,\n                \"recursive\": bool(AUTO_INDEX_RECURSIVE),\n                \"include_globs\": \"\",\n                \"exclude_globs\": \"\",\n                \"elapsed_s\": float(result.get(\"elapsed_s\", 0.0)) if isinstance(result, dict) else 0.0,\n            }\n\n            file_exists = metrics_path.exists()\n            with open(metrics_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n                w = csv.DictWriter(f, fieldnames=list(row.keys()))\n                if not file_exists:\n                    w.writeheader()", "mtime": 1756161371.7068532, "terms": ["if", "has_enhanced_features", "_indexer", "enhancedcodeindexer", "index_dir", "index_dir", "repo_root", "index_root", "else", "_indexer", "basecodeindexer", "index_dir", "index_dir", "config", "de", "auto", "indexa", "no", "start", "def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "executa", "indexa", "inicial", "captura", "resultado", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "result", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "registrar", "tricas", "da", "indexa", "inicial", "em", "metrics_index", "csv", "try", "index_dir", "getattr", "_indexer", "index_dir", "none", "if", "not", "index_dir", "and", "hasattr", "_indexer", "base_indexer", "index_dir", "getattr", "_indexer", "base_indexer", "index_dir", "none", "index_dir_str", "str", "index_dir", "if", "index_dir", "else", "str", "current_dir", "mcp_index", "metrics_dir", "pathlib", "path", "index_dir_str", "metrics_dir", "mkdir", "parents", "true", "exist_ok", "true", "metrics_path", "metrics_dir", "metrics_index", "csv", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "initial_index", "path", "os", "pathsep", "join", "auto_index_paths", "index_dir", "index_dir_str", "files_indexed", "int", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "chunks", "int", "result", "get", "chunks", "if", "isinstance", "result", "dict", "else", "recursive", "bool", "auto_index_recursive", "include_globs", "exclude_globs", "elapsed_s", "float", "result", "get", "elapsed_s", "if", "isinstance", "result", "dict", "else", "file_exists", "metrics_path", "exists", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "list", "row", "keys", "if", "not", "file_exists", "writeheader"]}
{"chunk_id": "fea5a61b7672356019ab7b9c", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 80, "end_line": 159, "content": "def _truthy(env_val: str, default: bool = False) -> bool:\n    if env_val is None:\n        return default\n    return str(env_val).strip().lower() in {\"1\", \"true\", \"yes\", \"on\"}\n\nAUTO_INDEX_ON_START = _truthy(os.environ.get(\"AUTO_INDEX_ON_START\", \"1\"), True)\nAUTO_INDEX_PATHS = [p.strip() for p in os.environ.get(\"AUTO_INDEX_PATHS\", \".\").split(os.pathsep) if p.strip()]\nAUTO_INDEX_RECURSIVE = _truthy(os.environ.get(\"AUTO_INDEX_RECURSIVE\", \"1\"), True)\nAUTO_ENABLE_SEMANTIC = _truthy(os.environ.get(\"AUTO_ENABLE_SEMANTIC\", \"1\"), True)\nAUTO_START_WATCHER = _truthy(os.environ.get(\"AUTO_START_WATCHER\", \"1\"), True)\n\n\ndef _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] üöÄ Iniciando indexa√ß√£o autom√°tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        # Executa indexa√ß√£o inicial e captura resultado\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            result = index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] ‚úÖ Indexa√ß√£o inicial conclu√≠da\\n\")\n\n        # Registrar m√©tricas da indexa√ß√£o inicial em metrics_index.csv\n        try:\n            index_dir = getattr(_indexer, 'index_dir', None)\n            if not index_dir and hasattr(_indexer, 'base_indexer'):\n                index_dir = getattr(_indexer.base_indexer, 'index_dir', None)\n            index_dir_str = str(index_dir) if index_dir else str(CURRENT_DIR / \".mcp_index\")\n\n            metrics_dir = pathlib.Path(index_dir_str)\n            metrics_dir.mkdir(parents=True, exist_ok=True)\n            metrics_path = metrics_dir / \"metrics_index.csv\"\n\n            row = {\n                \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n                \"op\": \"initial_index\",\n                \"path\": os.pathsep.join(AUTO_INDEX_PATHS),\n                \"index_dir\": index_dir_str,\n                \"files_indexed\": int(result.get(\"files_indexed\", 0)) if isinstance(result, dict) else 0,\n                \"chunks\": int(result.get(\"chunks\", 0)) if isinstance(result, dict) else 0,\n                \"recursive\": bool(AUTO_INDEX_RECURSIVE),\n                \"include_globs\": \"\",\n                \"exclude_globs\": \"\",\n                \"elapsed_s\": float(result.get(\"elapsed_s\", 0.0)) if isinstance(result, dict) else 0.0,\n            }\n\n            file_exists = metrics_path.exists()\n            with open(metrics_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n                w = csv.DictWriter(f, fieldnames=list(row.keys()))\n                if not file_exists:\n                    w.writeheader()\n                w.writerow(row)\n        except Exception as e:\n            sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha ao logar m√©tricas de indexa√ß√£o inicial: {e}\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha na indexa√ß√£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")", "mtime": 1756161371.7068532, "terms": ["def", "_truthy", "env_val", "str", "default", "bool", "false", "bool", "if", "env_val", "is", "none", "return", "default", "return", "str", "env_val", "strip", "lower", "in", "true", "yes", "on", "auto_index_on_start", "_truthy", "os", "environ", "get", "auto_index_on_start", "true", "auto_index_paths", "strip", "for", "in", "os", "environ", "get", "auto_index_paths", "split", "os", "pathsep", "if", "strip", "auto_index_recursive", "_truthy", "os", "environ", "get", "auto_index_recursive", "true", "auto_enable_semantic", "_truthy", "os", "environ", "get", "auto_enable_semantic", "true", "auto_start_watcher", "_truthy", "os", "environ", "get", "auto_start_watcher", "true", "def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "executa", "indexa", "inicial", "captura", "resultado", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "result", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "registrar", "tricas", "da", "indexa", "inicial", "em", "metrics_index", "csv", "try", "index_dir", "getattr", "_indexer", "index_dir", "none", "if", "not", "index_dir", "and", "hasattr", "_indexer", "base_indexer", "index_dir", "getattr", "_indexer", "base_indexer", "index_dir", "none", "index_dir_str", "str", "index_dir", "if", "index_dir", "else", "str", "current_dir", "mcp_index", "metrics_dir", "pathlib", "path", "index_dir_str", "metrics_dir", "mkdir", "parents", "true", "exist_ok", "true", "metrics_path", "metrics_dir", "metrics_index", "csv", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "initial_index", "path", "os", "pathsep", "join", "auto_index_paths", "index_dir", "index_dir_str", "files_indexed", "int", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "chunks", "int", "result", "get", "chunks", "if", "isinstance", "result", "dict", "else", "recursive", "bool", "auto_index_recursive", "include_globs", "exclude_globs", "elapsed_s", "float", "result", "get", "elapsed_s", "if", "isinstance", "result", "dict", "else", "file_exists", "metrics_path", "exists", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "list", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "ao", "logar", "tricas", "de", "indexa", "inicial", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher"]}
{"chunk_id": "44f67f029c075a9e0c2fcd77", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 92, "end_line": 171, "content": "def _initial_index():\n    if not AUTO_INDEX_ON_START:\n        return\n    try:\n        sys.stderr.write(\"[mcp_server_enhanced] üöÄ Iniciando indexa√ß√£o autom√°tica inicial...\\n\")\n        abs_paths: List[str] = []\n        for p in AUTO_INDEX_PATHS:\n            abs_paths.append(p if os.path.isabs(p) else os.path.join(INDEX_ROOT, p))\n\n        # Executa indexa√ß√£o inicial e captura resultado\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE,\n                enable_semantic=AUTO_ENABLE_SEMANTIC,\n                exclude_globs=[]\n            )\n            if AUTO_START_WATCHER and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n        else:\n            result = index_repo_paths(\n                _indexer,\n                abs_paths,\n                recursive=AUTO_INDEX_RECURSIVE\n            )\n        sys.stderr.write(\"[mcp_server_enhanced] ‚úÖ Indexa√ß√£o inicial conclu√≠da\\n\")\n\n        # Registrar m√©tricas da indexa√ß√£o inicial em metrics_index.csv\n        try:\n            index_dir = getattr(_indexer, 'index_dir', None)\n            if not index_dir and hasattr(_indexer, 'base_indexer'):\n                index_dir = getattr(_indexer.base_indexer, 'index_dir', None)\n            index_dir_str = str(index_dir) if index_dir else str(CURRENT_DIR / \".mcp_index\")\n\n            metrics_dir = pathlib.Path(index_dir_str)\n            metrics_dir.mkdir(parents=True, exist_ok=True)\n            metrics_path = metrics_dir / \"metrics_index.csv\"\n\n            row = {\n                \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n                \"op\": \"initial_index\",\n                \"path\": os.pathsep.join(AUTO_INDEX_PATHS),\n                \"index_dir\": index_dir_str,\n                \"files_indexed\": int(result.get(\"files_indexed\", 0)) if isinstance(result, dict) else 0,\n                \"chunks\": int(result.get(\"chunks\", 0)) if isinstance(result, dict) else 0,\n                \"recursive\": bool(AUTO_INDEX_RECURSIVE),\n                \"include_globs\": \"\",\n                \"exclude_globs\": \"\",\n                \"elapsed_s\": float(result.get(\"elapsed_s\", 0.0)) if isinstance(result, dict) else 0.0,\n            }\n\n            file_exists = metrics_path.exists()\n            with open(metrics_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n                w = csv.DictWriter(f, fieldnames=list(row.keys()))\n                if not file_exists:\n                    w.writeheader()\n                w.writerow(row)\n        except Exception as e:\n            sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha ao logar m√©tricas de indexa√ß√£o inicial: {e}\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha na indexa√ß√£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,", "mtime": 1756161371.7068532, "terms": ["def", "_initial_index", "if", "not", "auto_index_on_start", "return", "try", "sys", "stderr", "write", "mcp_server_enhanced", "iniciando", "indexa", "autom", "tica", "inicial", "abs_paths", "list", "str", "for", "in", "auto_index_paths", "abs_paths", "append", "if", "os", "path", "isabs", "else", "os", "path", "join", "index_root", "executa", "indexa", "inicial", "captura", "resultado", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "enable_semantic", "auto_enable_semantic", "exclude_globs", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "else", "result", "index_repo_paths", "_indexer", "abs_paths", "recursive", "auto_index_recursive", "sys", "stderr", "write", "mcp_server_enhanced", "indexa", "inicial", "conclu", "da", "registrar", "tricas", "da", "indexa", "inicial", "em", "metrics_index", "csv", "try", "index_dir", "getattr", "_indexer", "index_dir", "none", "if", "not", "index_dir", "and", "hasattr", "_indexer", "base_indexer", "index_dir", "getattr", "_indexer", "base_indexer", "index_dir", "none", "index_dir_str", "str", "index_dir", "if", "index_dir", "else", "str", "current_dir", "mcp_index", "metrics_dir", "pathlib", "path", "index_dir_str", "metrics_dir", "mkdir", "parents", "true", "exist_ok", "true", "metrics_path", "metrics_dir", "metrics_index", "csv", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "initial_index", "path", "os", "pathsep", "join", "auto_index_paths", "index_dir", "index_dir_str", "files_indexed", "int", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "chunks", "int", "result", "get", "chunks", "if", "isinstance", "result", "dict", "else", "recursive", "bool", "auto_index_recursive", "include_globs", "exclude_globs", "elapsed_s", "float", "result", "get", "elapsed_s", "if", "isinstance", "result", "dict", "else", "file_exists", "metrics_path", "exists", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "list", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "ao", "logar", "tricas", "de", "indexa", "inicial", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic"]}
{"chunk_id": "43efe1f1841ec3ac3af49da1", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 137, "end_line": 216, "content": "                \"chunks\": int(result.get(\"chunks\", 0)) if isinstance(result, dict) else 0,\n                \"recursive\": bool(AUTO_INDEX_RECURSIVE),\n                \"include_globs\": \"\",\n                \"exclude_globs\": \"\",\n                \"elapsed_s\": float(result.get(\"elapsed_s\", 0.0)) if isinstance(result, dict) else 0.0,\n            }\n\n            file_exists = metrics_path.exists()\n            with open(metrics_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n                w = csv.DictWriter(f, fieldnames=list(row.keys()))\n                if not file_exists:\n                    w.writeheader()\n                w.writerow(row)\n        except Exception as e:\n            sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha ao logar m√©tricas de indexa√ß√£o inicial: {e}\\n\")\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è Falha na indexa√ß√£o inicial: {e}\\n\")\n\n# ===== HANDLERS IMPLEMENTATION =====\n\ndef _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n", "mtime": 1756161371.7068532, "terms": ["chunks", "int", "result", "get", "chunks", "if", "isinstance", "result", "dict", "else", "recursive", "bool", "auto_index_recursive", "include_globs", "exclude_globs", "elapsed_s", "float", "result", "get", "elapsed_s", "if", "isinstance", "result", "dict", "else", "file_exists", "metrics_path", "exists", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "list", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "ao", "logar", "tricas", "de", "indexa", "inicial", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "falha", "na", "indexa", "inicial", "handlers", "implementation", "def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str"]}
{"chunk_id": "83e17554eb03b54e6defad23", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 157, "end_line": 236, "content": "def _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs):\n    \"\"\"Handler para indexar um caminho\"\"\"\n    sys.stderr.write(f\"üîç [index_path] {path} (recursive={recursive}, semantic={enable_semantic}, watcher={auto_start_watcher})\\n\")\n\n    try:\n        # Converte path relativo para absoluto\n        if not os.path.isabs(path):\n            path = os.path.join(INDEX_ROOT, path)\n\n        if HAS_ENHANCED_FEATURES:\n            result = enhanced_index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive,\n                enable_semantic=enable_semantic,\n                exclude_globs=exclude_globs or []\n            )\n\n            if auto_start_watcher and hasattr(_indexer, 'start_auto_indexing'):\n                _indexer.start_auto_indexing()\n                result['auto_indexing'] = 'started'\n        else:\n            result = index_repo_paths(\n                _indexer,\n                [path],\n                recursive=recursive\n            )\n\n        return result\n\n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [index_path] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        ", "mtime": 1756161371.7068532, "terms": ["def", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "handler", "para", "indexar", "um", "caminho", "sys", "stderr", "write", "index_path", "path", "recursive", "recursive", "semantic", "enable_semantic", "watcher", "auto_start_watcher", "try", "converte", "path", "relativo", "para", "absoluto", "if", "not", "os", "path", "isabs", "path", "path", "os", "path", "join", "index_root", "path", "if", "has_enhanced_features", "result", "enhanced_index_repo_paths", "_indexer", "path", "recursive", "recursive", "enable_semantic", "enable_semantic", "exclude_globs", "exclude_globs", "or", "if", "auto_start_watcher", "and", "hasattr", "_indexer", "start_auto_indexing", "_indexer", "start_auto_indexing", "result", "auto_indexing", "started", "else", "result", "index_repo_paths", "_indexer", "path", "recursive", "recursive", "return", "result", "except", "exception", "as", "sys", "stderr", "write", "index_path", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget"]}
{"chunk_id": "efa6261dd8a9cee87cdfe2bf", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 191, "end_line": 270, "content": "def _handle_search_code(query, limit, semantic_weight, use_mmr):\n    \"\"\"Handler para buscar c√≥digo\"\"\"\n    sys.stderr.write(f\"üîç [search_code] '{query}' (limit={limit}, weight={semantic_weight}, mmr={use_mmr})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            results = enhanced_search_code(\n                _indexer, \n                query, \n                limit=limit,\n                semantic_weight=semantic_weight,\n                use_mmr=use_mmr\n            )\n        else:\n            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:", "mtime": 1756161371.7068532, "terms": ["def", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "handler", "para", "buscar", "digo", "sys", "stderr", "write", "search_code", "query", "limit", "limit", "weight", "semantic_weight", "mmr", "use_mmr", "try", "if", "has_enhanced_features", "results", "enhanced_search_code", "_indexer", "query", "limit", "limit", "semantic_weight", "semantic_weight", "use_mmr", "use_mmr", "else", "results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as"]}
{"chunk_id": "1e294a43a42ea3b66c5bbbd3", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 205, "end_line": 284, "content": "            results = search_code(_indexer, query, limit=limit)\n        \n        return {\n            'status': 'success',\n            'results': results,\n            'count': len(results)\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [search_code] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP", "mtime": 1756161371.7068532, "terms": ["results", "search_code", "_indexer", "query", "limit", "limit", "return", "status", "success", "results", "results", "count", "len", "results", "except", "exception", "as", "sys", "stderr", "write", "search_code", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp"]}
{"chunk_id": "41237b55af9470c13a0ccf29", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 217, "end_line": 296, "content": "def _handle_context_pack(query, token_budget, max_chunks, strategy):\n    \"\"\"Handler para criar pacote de contexto\"\"\"\n    sys.stderr.write(f\"üì¶ [context_pack] '{query}' (budget={token_budget}, chunks={max_chunks}, strategy={strategy})\\n\")\n    \n    try:\n        if HAS_ENHANCED_FEATURES:\n            context_data = enhanced_build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n        else:\n            context_data = build_context_pack(\n                _indexer, \n                query, \n                token_budget=token_budget\n            )\n        \n        return {\n            'status': 'success',\n            'context_data': context_data,\n            'total_tokens': sum(chunk.get('estimated_tokens', 0) for chunk in context_data.get('chunks', []))\n        }\n        \n    except Exception as e:\n        sys.stderr.write(f\"‚ùå [context_pack] Erro: {str(e)}\\n\")\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"", "mtime": 1756161371.7068532, "terms": ["def", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "handler", "para", "criar", "pacote", "de", "contexto", "sys", "stderr", "write", "context_pack", "query", "budget", "token_budget", "chunks", "max_chunks", "strategy", "strategy", "try", "if", "has_enhanced_features", "context_data", "enhanced_build_context_pack", "_indexer", "query", "token_budget", "token_budget", "max_chunks", "max_chunks", "strategy", "strategy", "else", "context_data", "build_context_pack", "_indexer", "query", "token_budget", "token_budget", "return", "status", "success", "context_data", "context_data", "total_tokens", "sum", "chunk", "get", "estimated_tokens", "for", "chunk", "in", "context_data", "get", "chunks", "except", "exception", "as", "sys", "stderr", "write", "context_pack", "erro", "str", "return", "status", "error", "error", "str", "def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache"]}
{"chunk_id": "0d7cdbf48e0fab818ceedd92", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 247, "end_line": 326, "content": "def _handle_auto_index(action, paths, recursive):\n    \"\"\"Handler para controle de auto-indexa√ß√£o\"\"\"\n    sys.stderr.write(f\"üëÅÔ∏è  [auto_index] {action} (paths={paths}, recursive={recursive})\\n\")\n    \n    try:\n        if not HAS_ENHANCED_FEATURES or not hasattr(_indexer, 'start_auto_indexing'):\n            return {\n                'status': 'not_available',\n                'message': 'Auto-indexa√ß√£o n√£o dispon√≠vel. Instale watchdog e use EnhancedCodeIndexer.'\n            }\n        \n        if action == 'start':\n            _indexer.start_auto_indexing()\n            return {'status': 'started', 'paths': paths}\n        elif action == 'stop':\n            _indexer.stop_auto_indexing()\n            return {'status': 'stopped'}\n        elif action == 'status':\n            is_running = _indexer.is_auto_indexing_running()\n            return {'status': 'running' if is_running else 'stopped', 'is_running': is_running}\n        else:\n            return {'status': 'error', 'error': f'A√ß√£o inv√°lida: {action}'}\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background para n√£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()", "mtime": 1756161371.7068532, "terms": ["def", "_handle_auto_index", "action", "paths", "recursive", "handler", "para", "controle", "de", "auto", "indexa", "sys", "stderr", "write", "auto_index", "action", "paths", "paths", "recursive", "recursive", "try", "if", "not", "has_enhanced_features", "or", "not", "hasattr", "_indexer", "start_auto_indexing", "return", "status", "not_available", "message", "auto", "indexa", "dispon", "vel", "instale", "watchdog", "use", "enhancedcodeindexer", "if", "action", "start", "_indexer", "start_auto_indexing", "return", "status", "started", "paths", "paths", "elif", "action", "stop", "_indexer", "stop_auto_indexing", "return", "status", "stopped", "elif", "action", "status", "is_running", "_indexer", "is_auto_indexing_running", "return", "status", "running", "if", "is_running", "else", "stopped", "is_running", "is_running", "else", "return", "status", "error", "error", "inv", "lida", "action", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start"]}
{"chunk_id": "04894c1f345bfb82542be0ab", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 273, "end_line": 352, "content": "def _handle_get_stats():\n    \"\"\"Handler para obter estat√≠sticas\"\"\"\n    sys.stderr.write(\"üìä [get_stats] Coletando estat√≠sticas...\\n\")\n    \n    try:\n        stats = _indexer.get_stats()\n        \n        # Adiciona informa√ß√µes sobre capacidades\n        stats['capabilities'] = {\n            'semantic_search': HAS_ENHANCED_FEATURES,\n            'auto_indexing': HAS_ENHANCED_FEATURES,\n            'fastmcp': HAS_FASTMCP\n        }\n        \n        return {\n            'status': 'success',\n            'stats': stats\n        }\n        \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\ndef _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background para n√£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,", "mtime": 1756161371.7068532, "terms": ["def", "_handle_get_stats", "handler", "para", "obter", "estat", "sticas", "sys", "stderr", "write", "get_stats", "coletando", "estat", "sticas", "try", "stats", "_indexer", "get_stats", "adiciona", "informa", "es", "sobre", "capacidades", "stats", "capabilities", "semantic_search", "has_enhanced_features", "auto_indexing", "has_enhanced_features", "fastmcp", "has_fastmcp", "return", "status", "success", "stats", "stats", "except", "exception", "as", "return", "status", "error", "error", "str", "def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int"]}
{"chunk_id": "2720c1ebf36e72e6dcd7ee90", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 295, "end_line": 374, "content": "def _handle_cache_management(action, cache_type):\n    \"\"\"Handler para gest√£o de cache\"\"\"\n    sys.stderr.write(f\"üóÑÔ∏è  [cache_management] {action} ({cache_type})\\n\")\n    \n    try:\n        if action == 'clear':\n            if cache_type == 'embeddings' and hasattr(_indexer, 'semantic_engine'):\n                _indexer.semantic_engine.clear_cache()\n                return {'status': 'success', 'message': 'Cache de embeddings limpo'}\n            elif cache_type == 'all':\n                if hasattr(_indexer, 'semantic_engine'):\n                    _indexer.semantic_engine.clear_cache()\n                # Aqui voc√™ pode adicionar limpeza de outros caches se existirem\n                return {'status': 'success', 'message': 'Todos os caches limpos'}\n        elif action == 'status':\n            result = {'status': 'success'}\n            if hasattr(_indexer, 'semantic_engine'):\n                result['embeddings_cache'] = _indexer.semantic_engine.get_cache_stats()\n            return result\n            \n    except Exception as e:\n        return {'status': 'error', 'error': str(e)}\n\n# ===== SERVIDOR MCP COM FASTMCP =====\n\nif HAS_FASTMCP:\n    # Cria servidor FastMCP\n    mcp = FastMCP(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background para n√£o bloquear o initialize\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    # ===== TOOLS B√ÅSICAS =====\n\n    @mcp.tool()\n    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)", "mtime": 1756161371.7068532, "terms": ["def", "_handle_cache_management", "action", "cache_type", "handler", "para", "gest", "de", "cache", "sys", "stderr", "write", "cache_management", "action", "cache_type", "try", "if", "action", "clear", "if", "cache_type", "embeddings", "and", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "return", "status", "success", "message", "cache", "de", "embeddings", "limpo", "elif", "cache_type", "all", "if", "hasattr", "_indexer", "semantic_engine", "_indexer", "semantic_engine", "clear_cache", "aqui", "voc", "pode", "adicionar", "limpeza", "de", "outros", "caches", "se", "existirem", "return", "status", "success", "message", "todos", "os", "caches", "limpos", "elif", "action", "status", "result", "status", "success", "if", "hasattr", "_indexer", "semantic_engine", "result", "embeddings_cache", "_indexer", "semantic_engine", "get_cache_stats", "return", "result", "except", "exception", "as", "return", "status", "error", "error", "str", "servidor", "mcp", "com", "fastmcp", "if", "has_fastmcp", "cria", "servidor", "fastmcp", "mcp", "fastmcp", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "para", "bloquear", "initialize", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "tools", "sicas", "mcp", "tool", "def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true"]}
{"chunk_id": "bcfc5bfb59aafdc9c70c628d", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 333, "end_line": 412, "content": "    def index_path(path: str = \".\",\n                   recursive: bool = True,\n                   enable_semantic: bool = True,\n                   auto_start_watcher: bool = False,\n                   exclude_globs: list = None) -> dict:\n        \"\"\"Indexa arquivos de c√≥digo no caminho especificado\"\"\"\n        return _handle_index_path(path, recursive, enable_semantic, auto_start_watcher, exclude_globs)\n\n    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",", "mtime": 1756161371.7068532, "terms": ["def", "index_path", "path", "str", "recursive", "bool", "true", "enable_semantic", "bool", "true", "auto_start_watcher", "bool", "false", "exclude_globs", "list", "none", "dict", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "return", "_handle_index_path", "path", "recursive", "enable_semantic", "auto_start_watcher", "exclude_globs", "mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object"]}
{"chunk_id": "397eb8dc3fc28d62c0d5bb32", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 341, "end_line": 420, "content": "    @mcp.tool()\n    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }", "mtime": 1756161371.7068532, "terms": ["mcp", "tool", "def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query"]}
{"chunk_id": "671d19ab79d05b436109e73c", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 342, "end_line": 421, "content": "    def search_code(query: str,\n                    limit: int = 10,\n                    semantic_weight: float = 0.3,\n                    use_mmr: bool = True) -> dict:\n        \"\"\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\"\"\"\n        return _handle_search_code(query, limit, semantic_weight, use_mmr)\n\n    @mcp.tool()\n    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),", "mtime": 1756161371.7068532, "terms": ["def", "search_code", "query", "str", "limit", "int", "semantic_weight", "float", "use_mmr", "bool", "true", "dict", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "return", "_handle_search_code", "query", "limit", "semantic_weight", "use_mmr", "mcp", "tool", "def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query"]}
{"chunk_id": "6726490c09828c808541b9a1", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 350, "end_line": 429, "content": "    def context_pack(query: str,\n                     token_budget: int = 8000,\n                     max_chunks: int = 20,\n                     strategy: str = \"mmr\") -> dict:\n        \"\"\"Cria pacote de contexto otimizado para LLMs\"\"\"\n        return _handle_context_pack(query, token_budget, max_chunks, strategy)\n\n    @mcp.tool()\n    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},", "mtime": 1756161371.7068532, "terms": ["def", "context_pack", "query", "str", "token_budget", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "return", "_handle_context_pack", "query", "token_budget", "max_chunks", "strategy", "mcp", "tool", "def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default"]}
{"chunk_id": "970d601b28399cbaa5c235cd", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 358, "end_line": 437, "content": "    def auto_index(action: str = \"status\", paths: list = None, recursive: bool = True) -> dict:\n        \"\"\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\"\"\"\n        return _handle_auto_index(action, paths, recursive)\n\n    @mcp.tool()\n    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",", "mtime": 1756161371.7068532, "terms": ["def", "auto_index", "action", "str", "status", "paths", "list", "none", "recursive", "bool", "true", "dict", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "return", "_handle_auto_index", "action", "paths", "recursive", "mcp", "tool", "def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index"]}
{"chunk_id": "20a5c6f9eb8bdf42e0a8a59d", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 363, "end_line": 442, "content": "    def get_stats() -> dict:\n        \"\"\"Obt√©m estat√≠sticas do indexador\"\"\"\n        return _handle_get_stats()\n\n    @mcp.tool()\n    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},", "mtime": 1756161371.7068532, "terms": ["def", "get_stats", "dict", "obt", "estat", "sticas", "do", "indexador", "return", "_handle_get_stats", "mcp", "tool", "def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status"]}
{"chunk_id": "58837e50821cdc04256f1054", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 368, "end_line": 447, "content": "    def cache_management(action: str = \"status\", cache_type: str = \"all\") -> dict:\n        \"\"\"Gerencia caches (clear/status)\"\"\"\n        return _handle_cache_management(action, cache_type)\n\n    # Removido helper duplicado e chamada imediata; j√° iniciamos a thread acima\n    # def _start_initial_indexing_thread():\n    #     thread = threading.Thread(target=_initial_index, daemon=True)\n    #     thread.start()\n    # _start_initial_indexing_thread()\n\nelse:\n    # Fallback para MCP tradicional\n    from mcp.server.stdio import stdio_server\n    from mcp.types import Tool, TextContent\n\n    server = Server(name=\"code-indexer-enhanced\")\n\n    # Dispara indexa√ß√£o inicial em background\n    try:\n        threading.Thread(target=_initial_index, daemon=True).start()\n    except Exception as e:\n        sys.stderr.write(f\"[mcp_server_enhanced] ‚ö†Ô∏è N√£o foi poss√≠vel iniciar thread de indexa√ß√£o inicial: {e}\\n\")\n\n    @server.list_tools()\n    async def list_tools():\n        return [\n            Tool(\n                name=\"index_path\",\n                description=\"Indexa arquivos de c√≥digo no caminho especificado\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": {\"type\": \"string\", \"default\": \".\"},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True},\n                        \"enable_semantic\": {\"type\": \"boolean\", \"default\": True},\n                        \"auto_start_watcher\": {\"type\": \"boolean\", \"default\": False},\n                        \"exclude_globs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": []}\n                    }\n                }\n            ),\n            Tool(\n                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),", "mtime": 1756161371.7068532, "terms": ["def", "cache_management", "action", "str", "status", "cache_type", "str", "all", "dict", "gerencia", "caches", "clear", "status", "return", "_handle_cache_management", "action", "cache_type", "removido", "helper", "duplicado", "chamada", "imediata", "iniciamos", "thread", "acima", "def", "_start_initial_indexing_thread", "thread", "threading", "thread", "target", "_initial_index", "daemon", "true", "thread", "start", "_start_initial_indexing_thread", "else", "fallback", "para", "mcp", "tradicional", "from", "mcp", "server", "stdio", "import", "stdio_server", "from", "mcp", "types", "import", "tool", "textcontent", "server", "server", "name", "code", "indexer", "enhanced", "dispara", "indexa", "inicial", "em", "background", "try", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "except", "exception", "as", "sys", "stderr", "write", "mcp_server_enhanced", "foi", "poss", "vel", "iniciar", "thread", "de", "indexa", "inicial", "server", "list_tools", "async", "def", "list_tools", "return", "tool", "name", "index_path", "description", "indexa", "arquivos", "de", "digo", "no", "caminho", "especificado", "inputschema", "type", "object", "properties", "path", "type", "string", "default", "recursive", "type", "boolean", "default", "true", "enable_semantic", "type", "boolean", "default", "true", "auto_start_watcher", "type", "boolean", "default", "false", "exclude_globs", "type", "array", "items", "type", "string", "default", "tool", "name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true"]}
{"chunk_id": "89c63392be0c427a44ebbd01", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 409, "end_line": 488, "content": "                name=\"search_code\",\n                description=\"Busca c√≥digo usando busca h√≠brida (BM25 + sem√¢ntica)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"limit\": {\"type\": \"integer\", \"default\": 10},\n                        \"semantic_weight\": {\"type\": \"number\", \"default\": 0.3},\n                        \"use_mmr\": {\"type\": \"boolean\", \"default\": True}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"context_pack\",\n                description=\"Cria pacote de contexto otimizado para LLMs\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"query\": {\"type\": \"string\"},\n                        \"token_budget\": {\"type\": \"integer\", \"default\": 8000},\n                        \"max_chunks\": {\"type\": \"integer\", \"default\": 20},\n                        \"strategy\": {\"type\": \"string\", \"default\": \"mmr\"}\n                    },\n                    \"required\": [\"query\"]\n                }\n            ),\n            Tool(\n                name=\"auto_index\",\n                description=\"Controla sistema de auto-indexa√ß√£o (start/stop/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"paths\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"default\": None},\n                        \"recursive\": {\"type\": \"boolean\", \"default\": True}\n                    }\n                }\n            ),\n            Tool(\n                name=\"get_stats\",\n                description=\"Obt√©m estat√≠sticas do indexador\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {}\n                }\n            ),\n            Tool(\n                name=\"cache_management\",\n                description=\"Gerencia caches (clear/status)\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"action\": {\"type\": \"string\", \"default\": \"status\"},\n                        \"cache_type\": {\"type\": \"string\", \"default\": \"all\"}\n                    }\n                }\n            )\n        ]\n\n    @server.call_tool()\n    async def call_tool(name: str, arguments: Dict[str, Any] = None):\n        arguments = arguments or {}\n        try:\n            if name == \"index_path\":\n                return _handle_index_path(\n                    arguments.get(\"path\", \".\"),\n                    arguments.get(\"recursive\", True),\n                    arguments.get(\"enable_semantic\", True),\n                    arguments.get(\"auto_start_watcher\", False),\n                    arguments.get(\"exclude_globs\", None)\n                )\n            elif name == \"search_code\":\n                return _handle_search_code(\n                    arguments.get(\"query\"),\n                    arguments.get(\"limit\", 10),\n                    arguments.get(\"semantic_weight\", 0.3),\n                    arguments.get(\"use_mmr\", True)\n                )\n            elif name == \"context_pack\":", "mtime": 1756161371.7068532, "terms": ["name", "search_code", "description", "busca", "digo", "usando", "busca", "brida", "bm25", "sem", "ntica", "inputschema", "type", "object", "properties", "query", "type", "string", "limit", "type", "integer", "default", "semantic_weight", "type", "number", "default", "use_mmr", "type", "boolean", "default", "true", "required", "query", "tool", "name", "context_pack", "description", "cria", "pacote", "de", "contexto", "otimizado", "para", "llms", "inputschema", "type", "object", "properties", "query", "type", "string", "token_budget", "type", "integer", "default", "max_chunks", "type", "integer", "default", "strategy", "type", "string", "default", "mmr", "required", "query", "tool", "name", "auto_index", "description", "controla", "sistema", "de", "auto", "indexa", "start", "stop", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "paths", "type", "array", "items", "type", "string", "default", "none", "recursive", "type", "boolean", "default", "true", "tool", "name", "get_stats", "description", "obt", "estat", "sticas", "do", "indexador", "inputschema", "type", "object", "properties", "tool", "name", "cache_management", "description", "gerencia", "caches", "clear", "status", "inputschema", "type", "object", "properties", "action", "type", "string", "default", "status", "cache_type", "type", "string", "default", "all", "server", "call_tool", "async", "def", "call_tool", "name", "str", "arguments", "dict", "str", "any", "none", "arguments", "arguments", "or", "try", "if", "name", "index_path", "return", "_handle_index_path", "arguments", "get", "path", "arguments", "get", "recursive", "true", "arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "none", "elif", "name", "search_code", "return", "_handle_search_code", "arguments", "get", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack"]}
{"chunk_id": "3b347f4f588fe3ff1f1466f8", "file_path": "mcp_system/mcp_server_enhanced.py", "start_line": 477, "end_line": 526, "content": "                    arguments.get(\"enable_semantic\", True),\n                    arguments.get(\"auto_start_watcher\", False),\n                    arguments.get(\"exclude_globs\", None)\n                )\n            elif name == \"search_code\":\n                return _handle_search_code(\n                    arguments.get(\"query\"),\n                    arguments.get(\"limit\", 10),\n                    arguments.get(\"semantic_weight\", 0.3),\n                    arguments.get(\"use_mmr\", True)\n                )\n            elif name == \"context_pack\":\n                return _handle_context_pack(\n                    arguments.get(\"query\"),\n                    arguments.get(\"token_budget\", 8000),\n                    arguments.get(\"max_chunks\", 20),\n                    arguments.get(\"strategy\", \"mmr\")\n                )\n            elif name == \"auto_index\":\n                return _handle_auto_index(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"paths\", None),\n                    arguments.get(\"recursive\", True)\n                )\n            elif name == \"get_stats\":\n                return _handle_get_stats()\n            elif name == \"cache_management\":\n                return _handle_cache_management(\n                    arguments.get(\"action\", \"status\"),\n                    arguments.get(\"cache_type\", \"all\")\n                )\n            else:\n                return {\"status\": \"error\", \"error\": f\"Ferramenta desconhecida: {name}\"}\n        except Exception as e:\n            return {\"status\": \"error\", \"error\": str(e)}\n\n    # Iniciar servidor\n    async def main():\n        # Removido: j√° iniciamos a thread antes\n        # threading.Thread(target=_initial_index, daemon=True).start()\n        async with stdio_server() as (read_stream, write_stream):\n            await server.run(read_stream, write_stream, server_name=\"code-indexer-enhanced\")\n\n    if __name__ == \"__main__\":\n        import asyncio\n        asyncio.run(main())\n\nif HAS_FASTMCP and __name__ == \"__main__\":\n    # Executa o servidor FastMCP quando dispon√≠vel\n    mcp.run()", "mtime": 1756161371.7068532, "terms": ["arguments", "get", "enable_semantic", "true", "arguments", "get", "auto_start_watcher", "false", "arguments", "get", "exclude_globs", "none", "elif", "name", "search_code", "return", "_handle_search_code", "arguments", "get", "query", "arguments", "get", "limit", "arguments", "get", "semantic_weight", "arguments", "get", "use_mmr", "true", "elif", "name", "context_pack", "return", "_handle_context_pack", "arguments", "get", "query", "arguments", "get", "token_budget", "arguments", "get", "max_chunks", "arguments", "get", "strategy", "mmr", "elif", "name", "auto_index", "return", "_handle_auto_index", "arguments", "get", "action", "status", "arguments", "get", "paths", "none", "arguments", "get", "recursive", "true", "elif", "name", "get_stats", "return", "_handle_get_stats", "elif", "name", "cache_management", "return", "_handle_cache_management", "arguments", "get", "action", "status", "arguments", "get", "cache_type", "all", "else", "return", "status", "error", "error", "ferramenta", "desconhecida", "name", "except", "exception", "as", "return", "status", "error", "error", "str", "iniciar", "servidor", "async", "def", "main", "removido", "iniciamos", "thread", "antes", "threading", "thread", "target", "_initial_index", "daemon", "true", "start", "async", "with", "stdio_server", "as", "read_stream", "write_stream", "await", "server", "run", "read_stream", "write_stream", "server_name", "code", "indexer", "enhanced", "if", "__name__", "__main__", "import", "asyncio", "asyncio", "run", "main", "if", "has_fastmcp", "and", "__name__", "__main__", "executa", "servidor", "fastmcp", "quando", "dispon", "vel", "mcp", "run"]}
{"chunk_id": "cf68b6e8100962416706bc9e", "file_path": "mcp_system/__init__.py", "start_line": 1, "end_line": 45, "content": "# __init__.py\n\"\"\"\nMCP System - Model Context Protocol\nSistema avan√ßado de indexa√ß√£o e busca de c√≥digo para desenvolvimento assistido por IA.\n\"\"\"\n\n# Vers√£o do pacote\n__version__ = \"1.0.0\"\n\n# Importa√ß√µes principais para facilitar o uso do pacote\nfrom .code_indexer_enhanced import (\n    BaseCodeIndexer,\n    EnhancedCodeIndexer,\n    search_code,\n    build_context_pack,\n    index_repo_paths,\n    enhanced_search_code,\n    enhanced_build_context_pack,\n    enhanced_index_repo_paths\n)\n\n# Verificar se recursos avan√ßados est√£o dispon√≠veis\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\n    HAS_ENHANCED_FEATURES = True\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n\n# Expor funcionalidades principais\n__all__ = [\n    \"BaseCodeIndexer\",\n    \"EnhancedCodeIndexer\",\n    \"search_code\",\n    \"build_context_pack\",\n    \"index_repo_paths\",\n    \"enhanced_search_code\",\n    \"enhanced_build_context_pack\",\n    \"enhanced_index_repo_paths\",\n    \"HAS_ENHANCED_FEATURES\"\n]\n\n# Se recursos avan√ßados estiverem dispon√≠veis, export√°-los tamb√©m\nif HAS_ENHANCED_FEATURES:\n    __all__.extend([\"SemanticSearchEngine\", \"create_file_watcher\"])", "mtime": 1756157484.5288744, "terms": ["__init__", "py", "mcp", "system", "model", "context", "protocol", "sistema", "avan", "ado", "de", "indexa", "busca", "de", "digo", "para", "desenvolvimento", "assistido", "por", "ia", "vers", "do", "pacote", "__version__", "importa", "es", "principais", "para", "facilitar", "uso", "do", "pacote", "from", "code_indexer_enhanced", "import", "basecodeindexer", "enhancedcodeindexer", "search_code", "build_context_pack", "index_repo_paths", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "verificar", "se", "recursos", "avan", "ados", "est", "dispon", "veis", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "has_enhanced_features", "true", "except", "importerror", "has_enhanced_features", "false", "expor", "funcionalidades", "principais", "__all__", "basecodeindexer", "enhancedcodeindexer", "search_code", "build_context_pack", "index_repo_paths", "enhanced_search_code", "enhanced_build_context_pack", "enhanced_index_repo_paths", "has_enhanced_features", "se", "recursos", "avan", "ados", "estiverem", "dispon", "veis", "export", "los", "tamb", "if", "has_enhanced_features", "__all__", "extend", "semanticsearchengine", "create_file_watcher"]}
{"chunk_id": "b9b4b67d106d06d1540ae814", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 1, "end_line": 80, "content": "# code_indexer_enhanced.py\n# Sistema MCP melhorado com busca h√≠brida, auto-indexa√ß√£o e cache inteligente\nfrom __future__ import annotations\nimport os, re, json, math, time, hashlib, threading, csv, datetime as dt\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nimport pathlib\n\n#logs para m√©tricas\n\n# Diret√≥rio atual deste m√≥dulo para resolu√ß√µes relativas ao pacote mcp_system\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\n# CSV padr√£o para m√©tricas de context_pack (consultas)\n# Mant√©m compatibilidade via env MCP_METRICS_FILE, mas por padr√£o separa em metrics_context.csv\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", str(CURRENT_DIR / \".mcp_index/metrics_context.csv\"))\n\ndef _log_metrics(row: dict):\n    \"\"\"Append de uma linha de m√©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avan√ßados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILIT√ÅRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data", "mtime": 1756161401.0090349, "terms": ["code_indexer_enhanced", "py", "sistema", "mcp", "melhorado", "com", "busca", "brida", "auto", "indexa", "cache", "inteligente", "from", "__future__", "import", "annotations", "import", "os", "re", "json", "math", "time", "hashlib", "threading", "csv", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "import", "pathlib", "logs", "para", "tricas", "diret", "rio", "atual", "deste", "dulo", "para", "resolu", "es", "relativas", "ao", "pacote", "mcp_system", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "csv", "padr", "para", "tricas", "de", "context_pack", "consultas", "mant", "compatibilidade", "via", "env", "mcp_metrics_file", "mas", "por", "padr", "separa", "em", "metrics_context", "csv", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "str", "current_dir", "mcp_index", "metrics_context", "csv", "def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data"]}
{"chunk_id": "030b325f1597c67f97efc954", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 18, "end_line": 97, "content": "def _log_metrics(row: dict):\n    \"\"\"Append de uma linha de m√©tricas em CSV.\"\"\"\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\n# Tenta importar recursos avan√ßados\nHAS_ENHANCED_FEATURES = True\ntry:\n    from .embeddings.semantic_search import SemanticSearchEngine\n    from .utils.file_watcher import create_file_watcher\nexcept ImportError:\n    HAS_ENHANCED_FEATURES = False\n    SemanticSearchEngine = None\n    create_file_watcher = None\n\n# ========== CONSTANTES E UTILIT√ÅRIOS BASE ==========\n\nLANG_EXTS = {\n    \".py\", \".pyi\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".java\", \".go\", \".rb\", \".php\",\n    \".c\", \".cpp\", \".h\", \".hpp\", \".cs\", \".rs\", \".m\", \".mm\", \".swift\", \".kt\", \".kts\",\n    \".sql\", \".sh\", \".bash\", \".zsh\", \".ps1\", \".psm1\",\n}\n\nDEFAULT_INCLUDE = [\n    \"**/*.py\",\"**/*.js\",\"**/*.ts\",\"**/*.tsx\",\"**/*.jsx\",\"**/*.java\",\n    \"**/*.go\",\"**/*.rb\",\"**/*.php\",\"**/*.c\",\"**/*.cpp\",\"**/*.cs\",\n    \"**/*.rs\",\"**/*.swift\",\"**/*.kt\",\"**/*.kts\",\"**/*.sql\",\"**/*.sh\"\n]\nDEFAULT_EXCLUDE = [\n    \"**/.git/**\",\"**/node_modules/**\",\"**/dist/**\",\"**/build/**\",\n    \"**/.venv/**\",\"**/__pycache__/**\"\n]\n\nTOKEN_PATTERN = re.compile(r\"[A-Za-z_][A-Za-z_0-9]{1,}|[A-Za-z]{2,}\")\n\ndef now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():", "mtime": 1756161401.0090349, "terms": ["def", "_log_metrics", "row", "dict", "append", "de", "uma", "linha", "de", "tricas", "em", "csv", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "tenta", "importar", "recursos", "avan", "ados", "has_enhanced_features", "true", "try", "from", "embeddings", "semantic_search", "import", "semanticsearchengine", "from", "utils", "file_watcher", "import", "create_file_watcher", "except", "importerror", "has_enhanced_features", "false", "semanticsearchengine", "none", "create_file_watcher", "none", "constantes", "utilit", "rios", "base", "lang_exts", "py", "pyi", "js", "jsx", "ts", "tsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "mm", "swift", "kt", "kts", "sql", "sh", "bash", "zsh", "ps1", "psm1", "default_include", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "cs", "rs", "swift", "kt", "kts", "sql", "sh", "default_exclude", "git", "node_modules", "dist", "build", "venv", "__pycache__", "token_pattern", "re", "compile", "za", "z_", "za", "z_0", "za", "def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists"]}
{"chunk_id": "f1946acf81164cd6027db21c", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 60, "end_line": 139, "content": "def now_ts() -> float:\n    return time.time()\n\ndef tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n", "mtime": 1756161401.0090349, "terms": ["def", "now_ts", "float", "return", "time", "time", "def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none"]}
{"chunk_id": "a94c47a3eccace5a0a3ad660", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 63, "end_line": 142, "content": "def tokenize(text: str) -> List[str]:\n    return [t.lower() for t in TOKEN_PATTERN.findall(text)]\n\ndef est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.", "mtime": 1756161401.0090349, "terms": ["def", "tokenize", "text", "str", "list", "str", "return", "lower", "for", "in", "token_pattern", "findall", "text", "def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap"]}
{"chunk_id": "58548bc37e309ab49b6e8095", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 66, "end_line": 145, "content": "def est_tokens(text: str) -> int:\n    # rough heuristic: ~4 chars per token\n    return max(1, int(len(text) / 4))\n\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()", "mtime": 1756161401.0090349, "terms": ["def", "est_tokens", "text", "str", "int", "rough", "heuristic", "chars", "per", "token", "return", "max", "int", "len", "text", "def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines"]}
{"chunk_id": "45ea917f2a3fbb18f54a5acb", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 69, "end_line": 148, "content": "\ndef hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):", "mtime": 1756161401.0090349, "terms": ["def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines"]}
{"chunk_id": "5a606fa24e91122d264222d6", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 70, "end_line": 149, "content": "def hash_id(s: str) -> str:\n    return hashlib.blake2s(s.encode(\"utf-8\"), digest_size=12).hexdigest()\n\n# ========== INDEXADOR BASE ==========\n\nclass BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):", "mtime": 1756161401.0090349, "terms": ["def", "hash_id", "str", "str", "return", "hashlib", "blake2s", "encode", "utf", "digest_size", "hexdigest", "indexador", "base", "class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class"]}
{"chunk_id": "4175ee69864180ee113e1a39", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 75, "end_line": 154, "content": "class BaseCodeIndexer:\n    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)", "mtime": 1756161401.0090349, "terms": ["class", "basecodeindexer", "def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add"]}
{"chunk_id": "80e52d6bb036a2d3b9212dd9", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 76, "end_line": 155, "content": "    def __init__(self, index_dir: str = str(CURRENT_DIR / \".mcp_index\"), repo_root: str = str(CURRENT_DIR.parent)) -> None:\n        self.index_dir = Path(index_dir)\n        self.index_dir.mkdir(parents=True, exist_ok=True)\n        self.repo_root = Path(repo_root)\n        self.chunks: Dict[str, Dict] = {}             # chunk_id -> data\n        self.inverted: Dict[str, Dict[str, int]] = {} # term -> {chunk_id: tf}\n        self.doc_len: Dict[str, int] = {}             # chunk_id -> token count\n        self.file_mtime: Dict[str, float] = {}        # file_path -> mtime\n        self._load()\n\n    # ---------- persistence ----------\n    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap", "mtime": 1756161401.0090349, "terms": ["def", "__init__", "self", "index_dir", "str", "str", "current_dir", "mcp_index", "repo_root", "str", "str", "current_dir", "parent", "none", "self", "index_dir", "path", "index_dir", "self", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "repo_root", "path", "repo_root", "self", "chunks", "dict", "str", "dict", "chunk_id", "data", "self", "inverted", "dict", "str", "dict", "str", "int", "term", "chunk_id", "tf", "self", "doc_len", "dict", "str", "int", "chunk_id", "token", "count", "self", "file_mtime", "dict", "str", "float", "file_path", "mtime", "self", "_load", "persistence", "def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap"]}
{"chunk_id": "6fb004afe845668378cf7c10", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 87, "end_line": 166, "content": "    def _paths(self):\n        return (\n            self.index_dir / \"chunks.jsonl\",\n            self.index_dir / \"inverted.json\",\n            self.index_dir / \"doclen.json\",\n            self.index_dir / \"file_mtime.json\",\n        )\n\n    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:", "mtime": 1756161401.0090349, "terms": ["def", "_paths", "self", "return", "self", "index_dir", "chunks", "jsonl", "self", "index_dir", "inverted", "json", "self", "index_dir", "doclen", "json", "self", "index_dir", "file_mtime", "json", "def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int"]}
{"chunk_id": "299fbfa39193585f09eb98f1", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 95, "end_line": 174, "content": "    def _load(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        if chunks_p.exists():\n            with chunks_p.open(\"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    obj = json.loads(line)\n                    self.chunks[obj[\"chunk_id\"]] = obj\n        if inv_p.exists():\n            self.inverted = json.loads(inv_p.read_text(encoding=\"utf-8\"))\n        if dl_p.exists():\n            self.doc_len = {k:int(v) for k,v in json.loads(dl_p.read_text(encoding=\"utf-8\")).items()}\n        if mt_p.exists():\n            self.file_mtime = {k:float(v) for k,v in json.loads(mt_p.read_text(encoding=\"utf-8\")).items()}\n\n    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:", "mtime": 1756161401.0090349, "terms": ["def", "_load", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "if", "chunks_p", "exists", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "line", "in", "obj", "json", "loads", "line", "self", "chunks", "obj", "chunk_id", "obj", "if", "inv_p", "exists", "self", "inverted", "json", "loads", "inv_p", "read_text", "encoding", "utf", "if", "dl_p", "exists", "self", "doc_len", "int", "for", "in", "json", "loads", "dl_p", "read_text", "encoding", "utf", "items", "if", "mt_p", "exists", "self", "file_mtime", "float", "for", "in", "json", "loads", "mt_p", "read_text", "encoding", "utf", "items", "def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception"]}
{"chunk_id": "715de96be7fe21ba361bb8db", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 109, "end_line": 188, "content": "    def _save(self):\n        chunks_p, inv_p, dl_p, mt_p = self._paths()\n        with chunks_p.open(\"w\", encoding=\"utf-8\") as f:\n            for c in self.chunks.values():\n                f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n        inv_p.write_text(json.dumps(self.inverted), encoding=\"utf-8\")\n        dl_p.write_text(json.dumps(self.doc_len), encoding=\"utf-8\")\n        mt_p.write_text(json.dumps(self.file_mtime), encoding=\"utf-8\")\n\n    # ---------- m√©tricas e estat√≠sticas ----------\n    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,", "mtime": 1756161401.0090349, "terms": ["def", "_save", "self", "chunks_p", "inv_p", "dl_p", "mt_p", "self", "_paths", "with", "chunks_p", "open", "encoding", "utf", "as", "for", "in", "self", "chunks", "values", "write", "json", "dumps", "ensure_ascii", "false", "inv_p", "write_text", "json", "dumps", "self", "inverted", "encoding", "utf", "dl_p", "write_text", "json", "dumps", "self", "doc_len", "encoding", "utf", "mt_p", "write_text", "json", "dumps", "self", "file_mtime", "encoding", "utf", "tricas", "estat", "sticas", "def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid"]}
{"chunk_id": "1ff7a5178250dfa80e91d738", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 119, "end_line": 198, "content": "    def get_stats(self) -> Dict[str, Any]:\n        try:\n            index_size_bytes = sum(len(json.dumps(c)) for c in self.chunks.values())\n        except Exception:\n            index_size_bytes = 0\n        return {\n            \"total_chunks\": len(self.chunks),\n            \"total_files\": len(set(c.get(\"file_path\") for c in self.chunks.values())),\n            \"index_size_mb\": round(index_size_bytes / (1024 * 1024), 3),\n        }\n\n    # ---------- chunking ----------\n    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)", "mtime": 1756161401.0090349, "terms": ["def", "get_stats", "self", "dict", "str", "any", "try", "index_size_bytes", "sum", "len", "json", "dumps", "for", "in", "self", "chunks", "values", "except", "exception", "index_size_bytes", "return", "total_chunks", "len", "self", "chunks", "total_files", "len", "set", "get", "file_path", "for", "in", "self", "chunks", "values", "index_size_mb", "round", "index_size_bytes", "chunking", "def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks"]}
{"chunk_id": "8b55d17b4e25e0bd5eef9900", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 131, "end_line": 210, "content": "    def _read_text(self, path: Path) -> Optional[str]:\n        try:\n            return path.read_text(encoding=\"utf-8\")\n        except Exception:\n            try:\n                return path.read_text(errors=\"ignore\")\n            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted", "mtime": 1756161401.0090349, "terms": ["def", "_read_text", "self", "path", "path", "optional", "str", "try", "return", "path", "read_text", "encoding", "utf", "except", "exception", "try", "return", "path", "read_text", "errors", "ignore", "except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted"]}
{"chunk_id": "043077180b0bf4d78d20b588", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 137, "end_line": 216, "content": "            except Exception:\n                return None\n\n    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,", "mtime": 1756161401.0090349, "terms": ["except", "exception", "return", "none", "def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer"]}
{"chunk_id": "9dc868d17d0d4537ff0b1886", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 140, "end_line": 219, "content": "    def _split_chunks(self, path: Path, text: str, max_lines: int = 80, overlap: int = 12) -> List[Tuple[int,int,str]]:\n        \"\"\"\n        Heuristic chunking: break at def/class for Python; else line windows with overlap.\n        Returns list of (start_line, end_line, content)\n        \"\"\"\n        lines = text.splitlines()\n        boundaries = set()\n        if path.suffix == \".py\":\n            for i, ln in enumerate(lines):\n                if ln.lstrip().startswith(\"def \") or ln.lstrip().startswith(\"class \"):\n                    boundaries.add(i)\n        boundaries.add(0)\n        i = 0\n        while i < len(lines):\n            boundaries.add(i)\n            i += max_lines - overlap\n        sorted_bounds = sorted(boundaries)\n        chunks = []\n        for i in range(len(sorted_bounds)):\n            start = sorted_bounds[i]\n            end = min(len(lines), start + max_lines)\n            content = \"\\n\".join(lines[start:end])\n            if content.strip():\n                chunks.append((start+1, end, content))\n        return chunks\n\n    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,", "mtime": 1756161401.0090349, "terms": ["def", "_split_chunks", "self", "path", "path", "text", "str", "max_lines", "int", "overlap", "int", "list", "tuple", "int", "int", "str", "heuristic", "chunking", "break", "at", "def", "class", "for", "python", "else", "line", "windows", "with", "overlap", "returns", "list", "of", "start_line", "end_line", "content", "lines", "text", "splitlines", "boundaries", "set", "if", "path", "suffix", "py", "for", "ln", "in", "enumerate", "lines", "if", "ln", "lstrip", "startswith", "def", "or", "ln", "lstrip", "startswith", "class", "boundaries", "add", "boundaries", "add", "while", "len", "lines", "boundaries", "add", "max_lines", "overlap", "sorted_bounds", "sorted", "boundaries", "chunks", "for", "in", "range", "len", "sorted_bounds", "start", "sorted_bounds", "end", "min", "len", "lines", "start", "max_lines", "content", "join", "lines", "start", "end", "if", "content", "strip", "chunks", "append", "start", "end", "content", "return", "chunks", "def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "ff7f4aac1341e6fbce6b27ed", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 166, "end_line": 245, "content": "    def _index_file(self, file_path: Path) -> Tuple[int,int]:\n        text = self._read_text(file_path)\n        if text is None:\n            return (0,0)\n        mtime = file_path.stat().st_mtime\n        # Salvar path relativo ao repo_root para estabilidade\n        try:\n            rel_path = file_path.resolve().relative_to(self.repo_root.resolve())\n        except Exception:\n            rel_path = file_path.name\n        self.file_mtime[str(rel_path)] = mtime\n\n        chunks = self._split_chunks(file_path, text)\n        new_chunks = 0\n        new_tokens = 0\n        for (start, end, content) in chunks:\n            chunk_key = f\"{rel_path}|{start}|{end}|{mtime}\"\n            cid = hash_id(chunk_key)\n            toks = tokenize(content)\n            if not toks:\n                continue\n            self.chunks[cid] = {\n                \"chunk_id\": cid,\n                \"file_path\": str(rel_path),\n                \"start_line\": start,\n                \"end_line\": end,\n                \"content\": content,\n                \"mtime\": mtime,\n                \"terms\": toks[:2000],\n            }\n            self.doc_len[cid] = len(toks)\n            new_chunks += 1\n            new_tokens += len(toks)\n        self._rebuild_inverted()\n        return (new_chunks, new_tokens)\n\n    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue", "mtime": 1756161401.0090349, "terms": ["def", "_index_file", "self", "file_path", "path", "tuple", "int", "int", "text", "self", "_read_text", "file_path", "if", "text", "is", "none", "return", "mtime", "file_path", "stat", "st_mtime", "salvar", "path", "relativo", "ao", "repo_root", "para", "estabilidade", "try", "rel_path", "file_path", "resolve", "relative_to", "self", "repo_root", "resolve", "except", "exception", "rel_path", "file_path", "name", "self", "file_mtime", "str", "rel_path", "mtime", "chunks", "self", "_split_chunks", "file_path", "text", "new_chunks", "new_tokens", "for", "start", "end", "content", "in", "chunks", "chunk_key", "rel_path", "start", "end", "mtime", "cid", "hash_id", "chunk_key", "toks", "tokenize", "content", "if", "not", "toks", "continue", "self", "chunks", "cid", "chunk_id", "cid", "file_path", "str", "rel_path", "start_line", "start", "end_line", "end", "content", "content", "mtime", "mtime", "terms", "toks", "self", "doc_len", "cid", "len", "toks", "new_chunks", "new_tokens", "len", "toks", "self", "_rebuild_inverted", "return", "new_chunks", "new_tokens", "def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue"]}
{"chunk_id": "ed15b15ceeec54f8d5837797", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 202, "end_line": 281, "content": "    def _rebuild_inverted(self):\n        inverted: Dict[str, Dict[str,int]] = {}\n        for cid, c in self.chunks.items():\n            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)", "mtime": 1756161401.0090349, "terms": ["def", "_rebuild_inverted", "self", "inverted", "dict", "str", "dict", "str", "int", "for", "cid", "in", "self", "chunks", "items", "seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid"]}
{"chunk_id": "883d857ce7594c2c4bcc084c", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 205, "end_line": 284, "content": "            seen = {}\n            for t in c[\"terms\"]:\n                seen[t] = seen.get(t, 0) + 1\n            for t, tf in seen.items():\n                inverted.setdefault(t, {})[cid] = tf\n        self.inverted = inverted\n        self._save()\n\n# ========== FUN√á√ïES DE INDEXA√á√ÉO ==========\n\ndef index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)", "mtime": 1756161401.0090349, "terms": ["seen", "for", "in", "terms", "seen", "seen", "get", "for", "tf", "in", "seen", "items", "inverted", "setdefault", "cid", "tf", "self", "inverted", "inverted", "self", "_save", "fun", "es", "de", "indexa", "def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now"]}
{"chunk_id": "7871b9baedcc0fe678a6235c", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 215, "end_line": 294, "content": "def index_repo_paths(\n    indexer: BaseCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None\n) -> Dict[str,int]:\n    include = include_globs or DEFAULT_INCLUDE\n    exclude = set(exclude_globs or DEFAULT_EXCLUDE)\n    files_indexed = 0\n    total_chunks = 0\n\n    for p in paths:\n        pth = Path(p)\n        if not pth.is_absolute():\n            pth = (indexer.repo_root / pth).resolve()\n        if pth.is_file():\n            if any(pth.match(gl) for gl in include) and not any(pth.match(gl) for gl in exclude):\n                c, _ = indexer._index_file(pth)\n                files_indexed += 1 if c > 0 else 0\n                total_chunks += c\n        elif pth.is_dir():\n            base = pth\n            # Aten√ß√£o: rglob/glob invertidos ‚Äî para recursivo usar rglob\n            for gl in include:\n                iter_paths = base.rglob(gl) if recursive else base.glob(gl)\n                for fp in iter_paths:\n                    if any(fp.match(eg) for eg in exclude):\n                        continue\n                    if not fp.is_file():\n                        continue\n                    c, _ = indexer._index_file(fp)\n                    files_indexed += 1 if c > 0 else 0\n                    total_chunks += c\n        else:\n            # caminho inexistente: ignora\n            pass\n\n    indexer._save()\n    return {\"files_indexed\": files_indexed, \"chunks\": total_chunks}\n\n# ========== FUN√á√ïES DE BUSCA BM25 ==========\n\ndef _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}", "mtime": 1756161401.0090349, "terms": ["def", "index_repo_paths", "indexer", "basecodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "dict", "str", "int", "include", "include_globs", "or", "default_include", "exclude", "set", "exclude_globs", "or", "default_exclude", "files_indexed", "total_chunks", "for", "in", "paths", "pth", "path", "if", "not", "pth", "is_absolute", "pth", "indexer", "repo_root", "pth", "resolve", "if", "pth", "is_file", "if", "any", "pth", "match", "gl", "for", "gl", "in", "include", "and", "not", "any", "pth", "match", "gl", "for", "gl", "in", "exclude", "indexer", "_index_file", "pth", "files_indexed", "if", "else", "total_chunks", "elif", "pth", "is_dir", "base", "pth", "aten", "rglob", "glob", "invertidos", "para", "recursivo", "usar", "rglob", "for", "gl", "in", "include", "iter_paths", "base", "rglob", "gl", "if", "recursive", "else", "base", "glob", "gl", "for", "fp", "in", "iter_paths", "if", "any", "fp", "match", "eg", "for", "eg", "in", "exclude", "continue", "if", "not", "fp", "is_file", "continue", "indexer", "_index_file", "fp", "files_indexed", "if", "else", "total_chunks", "else", "caminho", "inexistente", "ignora", "pass", "indexer", "_save", "return", "files_indexed", "files_indexed", "chunks", "total_chunks", "fun", "es", "de", "busca", "bm25", "def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores"]}
{"chunk_id": "7b3d8e9f8b72ea1ed7e95896", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 258, "end_line": 337, "content": "def _bm25_scores(indexer: BaseCodeIndexer, query_tokens: List[str], k1: float = 1.5, b: float = 0.75) -> Dict[str, float]:\n    N = max(1, len(indexer.chunks))\n    avgdl = max(1, sum(indexer.doc_len.values()) / len(indexer.doc_len)) if indexer.doc_len else 1\n    scores: Dict[str, float] = {}\n\n    # df / idf\n    unique_q = set(query_tokens)\n    df = {t: len(indexer.inverted.get(t, {})) for t in unique_q}\n    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1) for t in unique_q}\n\n    for t in query_tokens:\n        postings = indexer.inverted.get(t, {})\n        for cid, tf in postings.items():\n            dl = indexer.doc_len.get(cid, 1)\n            denom = tf + k1 * (1 - b + b * (dl / avgdl))\n            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))", "mtime": 1756161401.0090349, "terms": ["def", "_bm25_scores", "indexer", "basecodeindexer", "query_tokens", "list", "str", "k1", "float", "float", "dict", "str", "float", "max", "len", "indexer", "chunks", "avgdl", "max", "sum", "indexer", "doc_len", "values", "len", "indexer", "doc_len", "if", "indexer", "doc_len", "else", "scores", "dict", "str", "float", "df", "idf", "unique_q", "set", "query_tokens", "df", "len", "indexer", "inverted", "get", "for", "in", "unique_q", "idf", "math", "log", "df", "get", "df", "get", "for", "in", "unique_q", "for", "in", "query_tokens", "postings", "indexer", "inverted", "get", "for", "cid", "tf", "in", "postings", "items", "dl", "indexer", "doc_len", "get", "cid", "denom", "tf", "k1", "dl", "avgdl", "idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys"]}
{"chunk_id": "3998568bd730dab7883386ce", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 273, "end_line": 352, "content": "            s = idf[t] * (tf * (k1 + 1)) / denom\n            scores[cid] = scores.get(cid, 0.0) + s\n    return scores\n\ndef _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]", "mtime": 1756161401.0090349, "terms": ["idf", "tf", "k1", "denom", "scores", "cid", "scores", "get", "cid", "return", "scores", "def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines"]}
{"chunk_id": "6f818627fbe2fab6e8514462", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 277, "end_line": 356, "content": "def _recency_boost(indexer: BaseCodeIndexer, cids: List[str], half_life_days: float = 30.0) -> Dict[str, float]:\n    now = now_ts()\n    boosts = {}\n    for cid in cids:\n        c = indexer.chunks.get(cid)\n        if not c:\n            continue\n        age_days = max(0.0, (now - float(c.get(\"mtime\", now))) / 86400.0)\n        # meia-vida: 30 dias => score cai pela metade a cada 30 dias\n        boosts[cid] = 0.5 ** (age_days / half_life_days)\n    return boosts\n\ndef _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],", "mtime": 1756161401.0090349, "terms": ["def", "_recency_boost", "indexer", "basecodeindexer", "cids", "list", "str", "half_life_days", "float", "dict", "str", "float", "now", "now_ts", "boosts", "for", "cid", "in", "cids", "indexer", "chunks", "get", "cid", "if", "not", "continue", "age_days", "max", "now", "float", "get", "mtime", "now", "meia", "vida", "dias", "score", "cai", "pela", "metade", "cada", "dias", "boosts", "cid", "age_days", "half_life_days", "return", "boosts", "def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line"]}
{"chunk_id": "6ee9545d97cf74777fb18c08", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 289, "end_line": 368, "content": "def _normalize(scores: Dict[str, float]) -> Dict[str, float]:\n    if not scores:\n        return {}\n    mx = max(scores.values())\n    if mx <= 0:\n        return {k: 0.0 for k in scores}\n    return {k: v / mx for k, v in scores.items()}\n\ndef _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:", "mtime": 1756161401.0090349, "terms": ["def", "_normalize", "scores", "dict", "str", "float", "dict", "str", "float", "if", "not", "scores", "return", "mx", "max", "scores", "values", "if", "mx", "return", "for", "in", "scores", "return", "mx", "for", "in", "scores", "items", "def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str"]}
{"chunk_id": "3048c83e1aa610e3db07e1e7", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 297, "end_line": 376, "content": "def _similarity(a_tokens: List[str], b_tokens: List[str]) -> float:\n    if not a_tokens or not b_tokens:\n        return 0.0\n    sa, sb = set(a_tokens), set(b_tokens)\n    inter = len(sa & sb)\n    uni = len(sa | sb)\n    return inter / max(1, uni)\n\ndef _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n", "mtime": 1756161401.0090349, "terms": ["def", "_similarity", "a_tokens", "list", "str", "b_tokens", "list", "str", "float", "if", "not", "a_tokens", "or", "not", "b_tokens", "return", "sa", "sb", "set", "a_tokens", "set", "b_tokens", "inter", "len", "sa", "sb", "uni", "len", "sa", "sb", "return", "inter", "max", "uni", "def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens"]}
{"chunk_id": "9c3cd8177d9ed8901e9abbb7", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 305, "end_line": 384, "content": "def _mmr_select(indexer: BaseCodeIndexer, candidates: List[str], query_tokens: List[str], k: int = 10, lambda_diverse: float = 0.7) -> List[str]:\n    selected: List[str] = []\n    candidates = list(candidates)\n    while candidates and len(selected) < k:\n        best_cid = None\n        best_score = -1e9\n        for cid in list(candidates):\n            rel = _similarity(query_tokens, indexer.chunks[cid][\"terms\"])\n            if not selected:\n                div = 0.0\n            else:\n                div = max(_similarity(indexer.chunks[cid][\"terms\"], indexer.chunks[sc][\"terms\"]) for sc in selected)\n            mmr = lambda_diverse * rel - (1 - lambda_diverse) * div\n            if mmr > best_score:\n                best_score = mmr\n                best_cid = cid\n        if best_cid is None:\n            break\n        selected.append(best_cid)\n        candidates.remove(best_cid)\n    return selected\n\ndef search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n", "mtime": 1756161401.0090349, "terms": ["def", "_mmr_select", "indexer", "basecodeindexer", "candidates", "list", "str", "query_tokens", "list", "str", "int", "lambda_diverse", "float", "list", "str", "selected", "list", "str", "candidates", "list", "candidates", "while", "candidates", "and", "len", "selected", "best_cid", "none", "best_score", "e9", "for", "cid", "in", "list", "candidates", "rel", "_similarity", "query_tokens", "indexer", "chunks", "cid", "terms", "if", "not", "selected", "div", "else", "div", "max", "_similarity", "indexer", "chunks", "cid", "terms", "indexer", "chunks", "sc", "terms", "for", "sc", "in", "selected", "mmr", "lambda_diverse", "rel", "lambda_diverse", "div", "if", "mmr", "best_score", "best_score", "mmr", "best_cid", "cid", "if", "best_cid", "is", "none", "break", "selected", "append", "best_cid", "candidates", "remove", "best_cid", "return", "selected", "def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break"]}
{"chunk_id": "4f1eff9549b0e9f57078703d", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 327, "end_line": 406, "content": "def search_code(indexer: BaseCodeIndexer, query: str, top_k: int = 30, filters: Optional[Dict] = None) -> List[Dict]:\n    q_tokens = tokenize(query)\n    if not q_tokens:\n        return []\n\n    bm25 = _bm25_scores(indexer, q_tokens)\n    if not bm25:\n        return []\n\n    bm25_norm = _normalize(bm25)\n    rec = _recency_boost(indexer, list(bm25.keys()))\n    rec_norm = _normalize(rec)\n\n    # combina√ß√£o simples (80‚Äì90% lexical, 10‚Äì20% recency)\n    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",", "mtime": 1756161401.0090349, "terms": ["def", "search_code", "indexer", "basecodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "list", "dict", "q_tokens", "tokenize", "query", "if", "not", "q_tokens", "return", "bm25", "_bm25_scores", "indexer", "q_tokens", "if", "not", "bm25", "return", "bm25_norm", "_normalize", "bm25", "rec", "_recency_boost", "indexer", "list", "bm25", "keys", "rec_norm", "_normalize", "rec", "combina", "simples", "lexical", "recency", "combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end"]}
{"chunk_id": "684985621783d7f8eb0d5596", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 341, "end_line": 420, "content": "    combined = {cid: 0.85 * bm25_norm.get(cid, 0.0) + 0.15 * rec_norm.get(cid, 0.0) for cid in bm25}\n\n    # pega mais candidatos primeiro, depois diversifica\n    ranked = sorted(combined.items(), key=lambda kv: kv[1], reverse=True)[:max(1, top_k * 3)]\n    candidate_ids = [cid for cid, _ in ranked]\n\n    selected = _mmr_select(indexer, candidate_ids, q_tokens, k=min(top_k, len(candidate_ids)), lambda_diverse=0.7)\n\n    results = []\n    for cid in selected:\n        c = indexer.chunks[cid]\n        preview = c[\"content\"].splitlines()[:12]\n        results.append({\n            \"chunk_id\": cid,\n            \"file_path\": c[\"file_path\"],\n            \"start_line\": c[\"start_line\"],\n            \"end_line\": c[\"end_line\"],\n            \"score\": combined.get(cid, 0.0),\n            \"preview\": \"\\n\".join(preview),\n        })\n    return results\n\n# ========== FUN√á√ïES DE CONTEXTO ==========\n\ndef get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":", "mtime": 1756161401.0090349, "terms": ["combined", "cid", "bm25_norm", "get", "cid", "rec_norm", "get", "cid", "for", "cid", "in", "bm25", "pega", "mais", "candidatos", "primeiro", "depois", "diversifica", "ranked", "sorted", "combined", "items", "key", "lambda", "kv", "kv", "reverse", "true", "max", "top_k", "candidate_ids", "cid", "for", "cid", "in", "ranked", "selected", "_mmr_select", "indexer", "candidate_ids", "q_tokens", "min", "top_k", "len", "candidate_ids", "lambda_diverse", "results", "for", "cid", "in", "selected", "indexer", "chunks", "cid", "preview", "content", "splitlines", "results", "append", "chunk_id", "cid", "file_path", "file_path", "start_line", "start_line", "end_line", "end_line", "score", "combined", "get", "cid", "preview", "join", "preview", "return", "results", "fun", "es", "de", "contexto", "def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr"]}
{"chunk_id": "09b22af4b5d9a9cffb8e76c1", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 365, "end_line": 444, "content": "def get_chunk_by_id(indexer: BaseCodeIndexer, chunk_id: str) -> Optional[Dict]:\n    return indexer.chunks.get(chunk_id)\n\ndef _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:", "mtime": 1756161401.0090349, "terms": ["def", "get_chunk_by_id", "indexer", "basecodeindexer", "chunk_id", "str", "optional", "dict", "return", "indexer", "chunks", "get", "chunk_id", "def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks"]}
{"chunk_id": "7f6c42d471f5fadd2bd642a3", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 368, "end_line": 447, "content": "def _summarize_chunk(c: Dict, query_tokens: List[str], max_lines: int = 18) -> str:\n    \"\"\"\n    Sumariza o chunk pegando linhas que contenham termos da query.\n    Se nada casar, usa as primeiras `max_lines` linhas.\n    \"\"\"\n    lines = c[\"content\"].splitlines()\n    header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n    qset = set(query_tokens)\n\n    picked = []\n    for ln in lines:\n        tks = tokenize(ln)\n        if set(tks) & qset:\n            picked.append(ln)\n        if len(picked) >= max_lines:\n            break\n\n    if not picked:\n        picked = lines[:max_lines]\n\n    summary = header + \"\\n\" + \"\\n\".join(picked)\n    return summary\n\ndef build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento", "mtime": 1756161401.0090349, "terms": ["def", "_summarize_chunk", "dict", "query_tokens", "list", "str", "max_lines", "int", "str", "sumariza", "chunk", "pegando", "linhas", "que", "contenham", "termos", "da", "query", "se", "nada", "casar", "usa", "as", "primeiras", "max_lines", "linhas", "lines", "content", "splitlines", "header", "file_path", "start_line", "end_line", "qset", "set", "query_tokens", "picked", "for", "ln", "in", "lines", "tks", "tokenize", "ln", "if", "set", "tks", "qset", "picked", "append", "ln", "if", "len", "picked", "max_lines", "break", "if", "not", "picked", "picked", "lines", "max_lines", "summary", "header", "join", "picked", "return", "summary", "def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento"]}
{"chunk_id": "1b0cbd5b3460ab54eda52bcc", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 391, "end_line": 470, "content": "def build_context_pack(\n    indexer: BaseCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"\n    Retorna um \"pacote de contexto\" pronto para inje√ß√£o no prompt:\n      {\n        \"query\": ...,\n        \"total_tokens\": ...,\n        \"chunks\": [\n          {\n            \"chunk_id\": ...,\n            \"header\": \"path:start-end\",\n            \"summary\": \"<linhas mais relevantes>\",\n            \"content_snippet\": \"<trecho comprimido dentro do or√ßamento>\"\n          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por itera√ß√£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG M√âTRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),", "mtime": 1756161401.0090349, "terms": ["def", "build_context_pack", "indexer", "basecodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "retorna", "um", "pacote", "de", "contexto", "pronto", "para", "inje", "no", "prompt", "query", "total_tokens", "chunks", "chunk_id", "header", "path", "start", "end", "summary", "linhas", "mais", "relevantes", "content_snippet", "trecho", "comprimido", "dentro", "do", "or", "amento", "t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds"]}
{"chunk_id": "3ad06843ab764a22d3a04f25", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 409, "end_line": 488, "content": "          },\n          ...\n        ]\n      }\n    \"\"\"\n    t0 = time.perf_counter()  # <-- start para medir lat√™ncia\n\n    q_tokens = tokenize(query)\n    search_res = search_code(indexer, query, top_k=max_chunks * 3)\n    ordered_ids = [r[\"chunk_id\"] for r in search_res]\n\n    if strategy == \"mmr\":\n        ordered_ids = _mmr_select(\n            indexer,\n            ordered_ids,\n            q_tokens,\n            k=min(max_chunks, len(ordered_ids)),\n            lambda_diverse=0.7,\n        )\n    else:\n        ordered_ids = ordered_ids[:max_chunks]\n\n    pack = {\"query\": query, \"total_tokens\": 0, \"chunks\": []}\n    remaining = max(1, budget_tokens)\n\n    for cid in ordered_ids:\n        c = indexer.chunks[cid]\n        header = f\"{c['file_path']}:{c['start_line']}-{c['end_line']}\"\n        summary = _summarize_chunk(c, q_tokens, max_lines=18)\n\n        # snippet inicial = summary (j√° inclui header + linhas relevantes)\n        snippet = summary\n        est = est_tokens(snippet)\n\n        # se n√£o cabe e j√° temos algo no pack, tenta o pr√≥ximo\n        if est > remaining and pack[\"chunks\"]:\n            continue\n\n        # tentativa de poda para caber no or√ßamento\n        while est > remaining and \"\\n\" in snippet:\n            snippet = \"\\n\".join(snippet.splitlines()[:-3])  # corta 3 linhas por itera√ß√£o\n            est = est_tokens(snippet)\n            if len(snippet) < 40:\n                break\n\n        pack[\"chunks\"].append({\n            \"chunk_id\": cid,\n            \"header\": header,\n            \"summary\": summary,\n            \"content_snippet\": snippet,\n        })\n        pack[\"total_tokens\"] += est\n        remaining = max(0, remaining - est)\n\n        if len(pack[\"chunks\"]) >= max_chunks or remaining <= 0:\n            break\n\n    # --- LOG M√âTRICAS CSV ---\n    try:\n        latency_ms = int((time.perf_counter() - t0) * 1000)\n        _log_metrics({\n            \"ts\": dt.datetime.utcnow().isoformat(timespec=\"seconds\"),\n            \"query\": query[:160],\n            \"chunk_count\": len(pack[\"chunks\"]),\n            \"total_tokens\": pack[\"total_tokens\"],\n            \"budget_tokens\": budget_tokens,\n            \"budget_utilization\": round(pack[\"total_tokens\"] / max(1, budget_tokens), 3),\n            \"latency_ms\": latency_ms,\n        })\n    except Exception:\n        pass  # n√£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de c√≥digo melhorado que combina:", "mtime": 1756161401.0090349, "terms": ["t0", "time", "perf_counter", "start", "para", "medir", "lat", "ncia", "q_tokens", "tokenize", "query", "search_res", "search_code", "indexer", "query", "top_k", "max_chunks", "ordered_ids", "chunk_id", "for", "in", "search_res", "if", "strategy", "mmr", "ordered_ids", "_mmr_select", "indexer", "ordered_ids", "q_tokens", "min", "max_chunks", "len", "ordered_ids", "lambda_diverse", "else", "ordered_ids", "ordered_ids", "max_chunks", "pack", "query", "query", "total_tokens", "chunks", "remaining", "max", "budget_tokens", "for", "cid", "in", "ordered_ids", "indexer", "chunks", "cid", "header", "file_path", "start_line", "end_line", "summary", "_summarize_chunk", "q_tokens", "max_lines", "snippet", "inicial", "summary", "inclui", "header", "linhas", "relevantes", "snippet", "summary", "est", "est_tokens", "snippet", "se", "cabe", "temos", "algo", "no", "pack", "tenta", "pr", "ximo", "if", "est", "remaining", "and", "pack", "chunks", "continue", "tentativa", "de", "poda", "para", "caber", "no", "or", "amento", "while", "est", "remaining", "and", "in", "snippet", "snippet", "join", "snippet", "splitlines", "corta", "linhas", "por", "itera", "est", "est_tokens", "snippet", "if", "len", "snippet", "break", "pack", "chunks", "append", "chunk_id", "cid", "header", "header", "summary", "summary", "content_snippet", "snippet", "pack", "total_tokens", "est", "remaining", "max", "remaining", "est", "if", "len", "pack", "chunks", "max_chunks", "or", "remaining", "break", "log", "tricas", "csv", "try", "latency_ms", "int", "time", "perf_counter", "t0", "_log_metrics", "ts", "dt", "datetime", "utcnow", "isoformat", "timespec", "seconds", "query", "query", "chunk_count", "len", "pack", "chunks", "total_tokens", "pack", "total_tokens", "budget_tokens", "budget_tokens", "budget_utilization", "round", "pack", "total_tokens", "max", "budget_tokens", "latency_ms", "latency_ms", "except", "exception", "pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina"]}
{"chunk_id": "035437ca0f292db168b38d15", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 477, "end_line": 556, "content": "        })\n    except Exception:\n        pass  # n√£o quebra o fluxo se log falhar\n\n    return pack\n\n\n# ========== INDEXADOR MELHORADO ==========\n\nclass EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de c√≥digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca sem√¢ntica com embeddings\n    - Auto-indexa√ß√£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n", "mtime": 1756161401.0090349, "terms": ["except", "exception", "pass", "quebra", "fluxo", "se", "log", "falhar", "return", "pack", "indexador", "melhorado", "class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result"]}
{"chunk_id": "a86f44e21cbdd1979a4e6e0d", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 486, "end_line": 565, "content": "class EnhancedCodeIndexer:\n    \"\"\"\n    Indexador de c√≥digo melhorado que combina:\n    - BM25 search (do indexador original)\n    - Busca sem√¢ntica com embeddings\n    - Auto-indexa√ß√£o com file watcher\n    - Cache inteligente\n    \"\"\"\n    \n    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,", "mtime": 1756161401.0090349, "terms": ["class", "enhancedcodeindexer", "indexador", "de", "digo", "melhorado", "que", "combina", "bm25", "search", "do", "indexador", "original", "busca", "sem", "ntica", "com", "embeddings", "auto", "indexa", "com", "file", "watcher", "cache", "inteligente", "def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "b392c691510245b6465728be", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 495, "end_line": 574, "content": "    def __init__(self, \n                 index_dir: str = \".mcp_index\", \n                 repo_root: str = \".\",\n                 enable_semantic: bool = True,\n                 enable_auto_indexing: bool = True,\n                 semantic_weight: float = 0.3):\n        \n        # Inicializa indexador base\n        self.base_indexer = BaseCodeIndexer(index_dir=index_dir, repo_root=repo_root)\n        \n        # Configura√ß√µes\n        self.index_dir = Path(index_dir)\n        self.repo_root = Path(repo_root)\n        self.enable_semantic = enable_semantic and HAS_ENHANCED_FEATURES\n        self.enable_auto_indexing = enable_auto_indexing and HAS_ENHANCED_FEATURES\n        self.semantic_weight = semantic_weight\n        \n        # Inicializa componentes opcionais\n        self.semantic_engine = None\n        self.file_watcher = None\n        \n        if self.enable_semantic:\n            try:\n                self.semantic_engine = SemanticSearchEngine(cache_dir=str(self.index_dir))\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar busca sem√¢ntica: {e}\\n\")\n                self.enable_semantic = False\n\n        if self.enable_auto_indexing:\n            try:\n                self.file_watcher = create_file_watcher(\n                    watch_path=str(self.repo_root),\n                    indexer_callback=self._auto_index_callback,\n                    debounce_seconds=2.0\n                )\n                # Mensagem ser√° enviada pelo mcp_server_enhanced.py\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao inicializar auto-indexa√ß√£o: {e}\\n\")\n                self.enable_auto_indexing = False\n\n        # Lock para thread safety\n        self._lock = threading.RLock()\n\n    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir", "mtime": 1756161401.0090349, "terms": ["def", "__init__", "self", "index_dir", "str", "mcp_index", "repo_root", "str", "enable_semantic", "bool", "true", "enable_auto_indexing", "bool", "true", "semantic_weight", "float", "inicializa", "indexador", "base", "self", "base_indexer", "basecodeindexer", "index_dir", "index_dir", "repo_root", "repo_root", "configura", "es", "self", "index_dir", "path", "index_dir", "self", "repo_root", "path", "repo_root", "self", "enable_semantic", "enable_semantic", "and", "has_enhanced_features", "self", "enable_auto_indexing", "enable_auto_indexing", "and", "has_enhanced_features", "self", "semantic_weight", "semantic_weight", "inicializa", "componentes", "opcionais", "self", "semantic_engine", "none", "self", "file_watcher", "none", "if", "self", "enable_semantic", "try", "self", "semantic_engine", "semanticsearchengine", "cache_dir", "str", "self", "index_dir", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "busca", "sem", "ntica", "self", "enable_semantic", "false", "if", "self", "enable_auto_indexing", "try", "self", "file_watcher", "create_file_watcher", "watch_path", "str", "self", "repo_root", "indexer_callback", "self", "_auto_index_callback", "debounce_seconds", "mensagem", "ser", "enviada", "pelo", "mcp_server_enhanced", "py", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "inicializar", "auto", "indexa", "self", "enable_auto_indexing", "false", "lock", "para", "thread", "safety", "self", "_lock", "threading", "rlock", "def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir"]}
{"chunk_id": "36e88df865fe5e3c0f3872e0", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 541, "end_line": 620, "content": "    def _auto_index_callback(self, changed_files: List[Path]) -> Dict[str, Any]:\n        \"\"\"Callback para reindexa√ß√£o autom√°tica de arquivos modificados\"\"\"\n        with self._lock:\n            try:\n                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o", "mtime": 1756161401.0090349, "terms": ["def", "_auto_index_callback", "self", "changed_files", "list", "path", "dict", "str", "any", "callback", "para", "reindexa", "autom", "tica", "de", "arquivos", "modificados", "with", "self", "_lock", "try", "converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr"]}
{"chunk_id": "060bb54840f47578c45d5026", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 545, "end_line": 624, "content": "                # Converte paths para strings\n                file_paths = [str(f) for f in changed_files]\n\n                # Reindexar usando indexador base\n                result = self.index_files(file_paths, show_progress=False)\n\n                # Mensagens de debug via stderr para n√£o interferir com protocolo MCP\n                import sys\n                sys.stderr.write(f\"üîÑ Auto-reindexa√ß√£o: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n\n                return result\n\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na auto-indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n\n    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:", "mtime": 1756161401.0090349, "terms": ["converte", "paths", "para", "strings", "file_paths", "str", "for", "in", "changed_files", "reindexar", "usando", "indexador", "base", "result", "self", "index_files", "file_paths", "show_progress", "false", "mensagens", "de", "debug", "via", "stderr", "para", "interferir", "com", "protocolo", "mcp", "import", "sys", "sys", "stderr", "write", "auto", "reindexa", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "auto", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns"]}
{"chunk_id": "f3aee47369e90f2b148221b3", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 562, "end_line": 641, "content": "    def index_files(self,\n                   paths: List[str],\n                   recursive: bool = True,\n                   include_globs: List[str] = None,\n                   exclude_globs: List[str] = None,\n                   show_progress: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Indexa arquivos usando o indexador base\n        \n        Args:\n            paths: Lista de caminhos para indexar\n            recursive: Se deve indexar recursivamente\n            include_globs: Padr√µes de arquivos para incluir\n            exclude_globs: Padr√µes de arquivos para excluir\n            show_progress: Se deve mostrar progresso\n            \n        Returns:\n            Dicion√°rio com estat√≠sticas da indexa√ß√£o\n        \"\"\"\n        with self._lock:\n            try:\n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"üìÅ Indexando {len(paths)} caminhos...\\n\")\n                \n                # Usa fun√ß√£o de indexa√ß√£o base\n                result = index_repo_paths(\n                    self.base_indexer,\n                    paths=paths,\n                    recursive=recursive,\n                    include_globs=include_globs,\n                    exclude_globs=exclude_globs\n                )\n                \n                if show_progress:\n                    import sys\n                    sys.stderr.write(f\"‚úÖ Indexa√ß√£o conclu√≠da: {result.get('files_indexed', 0)} arquivos, {result.get('chunks', 0)} chunks\\n\")\n                \n                return result\n                \n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ùå Erro na indexa√ß√£o: {e}\\n\")\n                return {\"files_indexed\": 0, \"chunks\": 0, \"error\": str(e)}\n    \n    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25", "mtime": 1756161401.0090349, "terms": ["def", "index_files", "self", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "show_progress", "bool", "true", "dict", "str", "any", "indexa", "arquivos", "usando", "indexador", "base", "args", "paths", "lista", "de", "caminhos", "para", "indexar", "recursive", "se", "deve", "indexar", "recursivamente", "include_globs", "padr", "es", "de", "arquivos", "para", "incluir", "exclude_globs", "padr", "es", "de", "arquivos", "para", "excluir", "show_progress", "se", "deve", "mostrar", "progresso", "returns", "dicion", "rio", "com", "estat", "sticas", "da", "indexa", "with", "self", "_lock", "try", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexando", "len", "paths", "caminhos", "usa", "fun", "de", "indexa", "base", "result", "index_repo_paths", "self", "base_indexer", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "if", "show_progress", "import", "sys", "sys", "stderr", "write", "indexa", "conclu", "da", "result", "get", "files_indexed", "arquivos", "result", "get", "chunks", "chunks", "return", "result", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "indexa", "return", "files_indexed", "chunks", "error", "str", "def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25"]}
{"chunk_id": "1371d99e98eb5f5792c9a727", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 607, "end_line": 686, "content": "    def search_code(self, \n                   query: str, \n                   top_k: int = 30, \n                   use_semantic: Optional[bool] = None,\n                   semantic_weight: Optional[float] = None,\n                   use_mmr: bool = True,\n                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca h√≠brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ö†Ô∏è  Erro na busca sem√¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )", "mtime": 1756161401.0090349, "terms": ["def", "search_code", "self", "query", "str", "top_k", "int", "use_semantic", "optional", "bool", "none", "semantic_weight", "optional", "float", "none", "use_mmr", "bool", "true", "filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic"]}
{"chunk_id": "62315c700fdb2f8cdc221aa8", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 613, "end_line": 692, "content": "                   filters: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"\n        Busca h√≠brida combinando BM25 e busca sem√¢ntica\n        \n        Args:\n            query: Consulta de busca\n            top_k: N√∫mero m√°ximo de resultados\n            use_semantic: Se True, usa busca h√≠brida. Se None, usa configura√ß√£o padr√£o\n            semantic_weight: Peso da similaridade sem√¢ntica (sobrescreve padr√£o)\n            filters: Filtros adicionais\n            \n        Returns:\n            Lista de resultados ordenados por relev√¢ncia\n        \"\"\"\n        # Define se usar busca sem√¢ntica\n        use_semantic = use_semantic if use_semantic is not None else self.enable_semantic\n        semantic_weight = semantic_weight if semantic_weight is not None else self.semantic_weight\n        \n        # Busca BM25 base\n        bm25_results = []\n        try:\n            # Usa fun√ß√£o de busca base integrada\n            bm25_results = search_code(self.base_indexer, query, top_k=top_k * 2, filters=filters)\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca BM25: {e}\\n\")\n            return []\n        \n        # Se busca sem√¢ntica n√£o habilitada, retorna apenas BM25\n        if not use_semantic or not self.semantic_engine:\n            return bm25_results[:top_k]\n        \n        # Busca h√≠brida\n        try:\n            semantic_results = self.semantic_engine.hybrid_search(\n                query=query,\n                bm25_results=bm25_results,\n                chunk_data=self.base_indexer.chunks,\n                semantic_weight=semantic_weight,\n                top_k=top_k,\n                use_mmr=use_mmr\n            )\n            return semantic_results\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ö†Ô∏è  Erro na busca sem√¢ntica, usando apenas BM25: {e}\\n\")\n            return bm25_results[:top_k]\n    \n    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa fun√ß√£o de constru√ß√£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela fun√ß√£o base\n            mock_results = []\n            for result in search_results:", "mtime": 1756161401.0090349, "terms": ["filters", "optional", "dict", "none", "list", "dict", "busca", "brida", "combinando", "bm25", "busca", "sem", "ntica", "args", "query", "consulta", "de", "busca", "top_k", "mero", "ximo", "de", "resultados", "use_semantic", "se", "true", "usa", "busca", "brida", "se", "none", "usa", "configura", "padr", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "sobrescreve", "padr", "filters", "filtros", "adicionais", "returns", "lista", "de", "resultados", "ordenados", "por", "relev", "ncia", "define", "se", "usar", "busca", "sem", "ntica", "use_semantic", "use_semantic", "if", "use_semantic", "is", "not", "none", "else", "self", "enable_semantic", "semantic_weight", "semantic_weight", "if", "semantic_weight", "is", "not", "none", "else", "self", "semantic_weight", "busca", "bm25", "base", "bm25_results", "try", "usa", "fun", "de", "busca", "base", "integrada", "bm25_results", "search_code", "self", "base_indexer", "query", "top_k", "top_k", "filters", "filters", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "bm25", "return", "se", "busca", "sem", "ntica", "habilitada", "retorna", "apenas", "bm25", "if", "not", "use_semantic", "or", "not", "self", "semantic_engine", "return", "bm25_results", "top_k", "busca", "brida", "try", "semantic_results", "self", "semantic_engine", "hybrid_search", "query", "query", "bm25_results", "bm25_results", "chunk_data", "self", "base_indexer", "chunks", "semantic_weight", "semantic_weight", "top_k", "top_k", "use_mmr", "use_mmr", "return", "semantic_results", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "usando", "apenas", "bm25", "return", "bm25_results", "top_k", "def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results"]}
{"chunk_id": "05bd32c7104cead91a05ccc5", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 662, "end_line": 741, "content": "    def build_context_pack(self, \n                          query: str,\n                          budget_tokens: int = 3000,\n                          max_chunks: int = 10,\n                          strategy: str = \"mmr\",\n                          use_semantic: Optional[bool] = None) -> Dict:\n        \"\"\"\n        Constr√≥i pacote de contexto otimizado\n        \n        Args:\n            query: Consulta de busca\n            budget_tokens: Or√ßamento m√°ximo de tokens\n            max_chunks: N√∫mero m√°ximo de chunks\n            strategy: Estrat√©gia de sele√ß√£o (\"mmr\" ou \"topk\")\n            use_semantic: Se usar busca sem√¢ntica\n            \n        Returns:\n            Pacote de contexto formatado\n        \"\"\"\n        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa fun√ß√£o de constru√ß√£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela fun√ß√£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # Constr√≥i pack usando fun√ß√£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informa√ß√µes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    ", "mtime": 1756161401.0090349, "terms": ["def", "build_context_pack", "self", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "use_semantic", "optional", "bool", "none", "dict", "constr", "pacote", "de", "contexto", "otimizado", "args", "query", "consulta", "de", "busca", "budget_tokens", "or", "amento", "ximo", "de", "tokens", "max_chunks", "mero", "ximo", "de", "chunks", "strategy", "estrat", "gia", "de", "sele", "mmr", "ou", "topk", "use_semantic", "se", "usar", "busca", "sem", "ntica", "returns", "pacote", "de", "contexto", "formatado", "busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false"]}
{"chunk_id": "74ebd5795af49ec64d0d88be", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 681, "end_line": 760, "content": "        # Busca chunks relevantes\n        search_results = self.search_code(\n            query=query, \n            top_k=max_chunks * 3,  # Busca mais para melhor sele√ß√£o\n            use_semantic=use_semantic\n        )\n        \n        # Usa fun√ß√£o de constru√ß√£o de contexto base integrada\n        try:\n            # Simula resultado de busca no formato esperado pela fun√ß√£o base\n            mock_results = []\n            for result in search_results:\n                mock_results.append({\n                    'chunk_id': result['chunk_id'],\n                    'score': result['score']\n                })\n            \n            # Constr√≥i pack usando fun√ß√£o base\n            pack = build_context_pack(\n                self.base_indexer,\n                query=query,\n                budget_tokens=budget_tokens,\n                max_chunks=max_chunks,\n                strategy=strategy\n            )\n            \n            # Adiciona informa√ß√µes sobre tipo de busca usada\n            pack['search_type'] = 'hybrid' if (use_semantic and self.enable_semantic) else 'bm25'\n            pack['semantic_enabled'] = self.enable_semantic\n            pack['auto_indexing_enabled'] = self.enable_auto_indexing\n            \n            return pack\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao construir contexto: {e}\\n\")\n            return {\"query\": query, \"total_tokens\": 0, \"chunks\": [], \"error\": str(e)}\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,", "mtime": 1756161401.0090349, "terms": ["busca", "chunks", "relevantes", "search_results", "self", "search_code", "query", "query", "top_k", "max_chunks", "busca", "mais", "para", "melhor", "sele", "use_semantic", "use_semantic", "usa", "fun", "de", "constru", "de", "contexto", "base", "integrada", "try", "simula", "resultado", "de", "busca", "no", "formato", "esperado", "pela", "fun", "base", "mock_results", "for", "result", "in", "search_results", "mock_results", "append", "chunk_id", "result", "chunk_id", "score", "result", "score", "constr", "pack", "usando", "fun", "base", "pack", "build_context_pack", "self", "base_indexer", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "adiciona", "informa", "es", "sobre", "tipo", "de", "busca", "usada", "pack", "search_type", "hybrid", "if", "use_semantic", "and", "self", "enable_semantic", "else", "bm25", "pack", "semantic_enabled", "self", "enable_semantic", "pack", "auto_indexing_enabled", "self", "enable_auto_indexing", "return", "pack", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "construir", "contexto", "return", "query", "query", "total_tokens", "chunks", "error", "str", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none"]}
{"chunk_id": "6d3ec61ec0f5e975c189a6f8", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 719, "end_line": 798, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do indexador\"\"\"\n        return {\n            \"total_chunks\": len(self.base_indexer.chunks),\n            \"total_files\": len(set(c[\"file_path\"] for c in self.base_indexer.chunks.values())),\n            \"index_size_mb\": sum(len(json.dumps(c)) for c in self.base_indexer.chunks.values()) / (1024 * 1024),\n            \"semantic_enabled\": self.enable_semantic,\n            \"auto_indexing_enabled\": self.enable_auto_indexing,\n            \"has_enhanced_features\": HAS_ENHANCED_FEATURES\n        }\n    \n    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O par√¢metro enable_semantic n√£o √© usado diretamente na indexa√ß√£o,\n    # mas √© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,", "mtime": 1756161401.0090349, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "indexador", "return", "total_chunks", "len", "self", "base_indexer", "chunks", "total_files", "len", "set", "file_path", "for", "in", "self", "base_indexer", "chunks", "values", "index_size_mb", "sum", "len", "json", "dumps", "for", "in", "self", "base_indexer", "chunks", "values", "semantic_enabled", "self", "enable_semantic", "auto_indexing_enabled", "self", "enable_auto_indexing", "has_enhanced_features", "has_enhanced_features", "def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks"]}
{"chunk_id": "d2c581c8abb7e78bb9ccb22f", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 730, "end_line": 803, "content": "    def start_auto_indexing(self) -> bool:\n        \"\"\"Inicia o file watcher para auto-indexa√ß√£o\"\"\"\n        if not self.enable_auto_indexing or not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.start()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao iniciar auto-indexa√ß√£o: {e}\\n\")\n            return False\n    \n    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O par√¢metro enable_semantic n√£o √© usado diretamente na indexa√ß√£o,\n    # mas √© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["def", "start_auto_indexing", "self", "bool", "inicia", "file", "watcher", "para", "auto", "indexa", "if", "not", "self", "enable_auto_indexing", "or", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "start", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "iniciar", "auto", "indexa", "return", "false", "def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "817ebd311162f52aa712d03a", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 742, "end_line": 803, "content": "    def stop_auto_indexing(self) -> bool:\n        \"\"\"Para o file watcher\"\"\"\n        if not self.file_watcher:\n            return False\n        try:\n            self.file_watcher.stop()\n            return True\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O par√¢metro enable_semantic n√£o √© usado diretamente na indexa√ß√£o,\n    # mas √© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["def", "stop_auto_indexing", "self", "bool", "para", "file", "watcher", "if", "not", "self", "file_watcher", "return", "false", "try", "self", "file_watcher", "stop", "return", "true", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "7161ac399149f834c463604f", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 749, "end_line": 803, "content": "        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao parar auto-indexa√ß√£o: {e}\\n\")\n            return False\n\n# ========== FUN√á√ïES P√öBLICAS PARA COMPATIBILIDADE ==========\n\ndef enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O par√¢metro enable_semantic n√£o √© usado diretamente na indexa√ß√£o,\n    # mas √© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "parar", "auto", "indexa", "return", "false", "fun", "es", "blicas", "para", "compatibilidade", "def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "efb1cb574e06f2184a5b90af", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 756, "end_line": 803, "content": "def enhanced_index_repo_paths(\n    indexer: EnhancedCodeIndexer,\n    paths: List[str],\n    recursive: bool = True,\n    include_globs: List[str] = None,\n    exclude_globs: List[str] = None,\n    enable_semantic: bool = True  # Adicionado para compatibilidade com o servidor\n) -> Dict[str,int]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # O par√¢metro enable_semantic n√£o √© usado diretamente na indexa√ß√£o,\n    # mas √© mantido para compatibilidade com a API do servidor\n    return indexer.index_files(\n        paths=paths,\n        recursive=recursive,\n        include_globs=include_globs,\n        exclude_globs=exclude_globs\n    )\n\ndef enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["def", "enhanced_index_repo_paths", "indexer", "enhancedcodeindexer", "paths", "list", "str", "recursive", "bool", "true", "include_globs", "list", "str", "none", "exclude_globs", "list", "str", "none", "enable_semantic", "bool", "true", "adicionado", "para", "compatibilidade", "com", "servidor", "dict", "str", "int", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "par", "metro", "enable_semantic", "usado", "diretamente", "na", "indexa", "mas", "mantido", "para", "compatibilidade", "com", "api", "do", "servidor", "return", "indexer", "index_files", "paths", "paths", "recursive", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "b58b56b6ef32210b54c14e88", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 774, "end_line": 803, "content": "def enhanced_search_code(\n    indexer: EnhancedCodeIndexer, \n    query: str, \n    top_k: int = 30, \n    filters: Optional[Dict] = None,\n    semantic_weight: float = None,\n    use_mmr: bool = True\n) -> List[Dict]:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    # Os par√¢metros semantic_weight e use_mmr s√£o mantidos para compatibilidade\n    # mas podem n√£o ser usados dependendo da implementa√ß√£o do indexer\n    return indexer.search_code(query=query, top_k=top_k, filters=filters)\n\ndef enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["def", "enhanced_search_code", "indexer", "enhancedcodeindexer", "query", "str", "top_k", "int", "filters", "optional", "dict", "none", "semantic_weight", "float", "none", "use_mmr", "bool", "true", "list", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "os", "par", "metros", "semantic_weight", "use_mmr", "mantidos", "para", "compatibilidade", "mas", "podem", "ser", "usados", "dependendo", "da", "implementa", "do", "indexer", "return", "indexer", "search_code", "query", "query", "top_k", "top_k", "filters", "filters", "def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "17beb2e060cab0fb2e0d359d", "file_path": "mcp_system/code_indexer_enhanced.py", "start_line": 787, "end_line": 803, "content": "def enhanced_build_context_pack(\n    indexer: EnhancedCodeIndexer,\n    query: str,\n    budget_tokens: int = 3000,\n    max_chunks: int = 10,\n    strategy: str = \"mmr\"\n) -> Dict:\n    \"\"\"Wrapper para compatibilidade com API antiga\"\"\"\n    return indexer.build_context_pack(\n        query=query,\n        budget_tokens=budget_tokens,\n        max_chunks=max_chunks,\n        strategy=strategy\n    )\n\n# Alias para compatibilidade com c√≥digo existente\nCodeIndexer = BaseCodeIndexer", "mtime": 1756161401.0090349, "terms": ["def", "enhanced_build_context_pack", "indexer", "enhancedcodeindexer", "query", "str", "budget_tokens", "int", "max_chunks", "int", "strategy", "str", "mmr", "dict", "wrapper", "para", "compatibilidade", "com", "api", "antiga", "return", "indexer", "build_context_pack", "query", "query", "budget_tokens", "budget_tokens", "max_chunks", "max_chunks", "strategy", "strategy", "alias", "para", "compatibilidade", "com", "digo", "existente", "codeindexer", "basecodeindexer"]}
{"chunk_id": "786b6b6d37a9eaaf05749891", "file_path": "mcp_system/reindex.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nReindexador simples para o MCP Code Indexer.\n\n- Remove (opcional) o diret√≥rio de √≠ndice.\n- Reindexa arquivos conforme include/exclude globs.\n- Mede tempo e grava linha no CSV de m√©tricas (MCP_METRICS_FILE ou .mcp_index/metrics_index.csv).\n- (Opcional) Calcula baseline aproximada de tokens do reposit√≥rio.\n\nExemplos:\n  python reindex.py --clean\n  python reindex.py --path src --include \"**/*.py\" --exclude \"**/tests/**\"\n  python reindex.py --baseline-estimate\n  MCP_METRICS_FILE=\".mcp_index/metrics_index.csv\" python reindex.py --clean\n\nRequer:\n  - code_indexer_enhanced.py com CodeIndexer e index_repo_paths\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport json\nimport shutil\nimport time\nimport argparse\nimport datetime as dt\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport pathlib\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\n# ---- Config m√©tricas (mesmo padr√£o do context_pack) - agora usando caminho relativo √† pasta mcp_system\nMETRICS_PATH = os.environ.get(\"MCP_METRICS_FILE\", str(CURRENT_DIR / \".mcp_index/metrics.csv\"))\n\ndef _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n", "mtime": 1756161419.1409552, "terms": ["usr", "bin", "env", "python3", "reindexador", "simples", "para", "mcp", "code", "indexer", "remove", "opcional", "diret", "rio", "de", "ndice", "reindexa", "arquivos", "conforme", "include", "exclude", "globs", "mede", "tempo", "grava", "linha", "no", "csv", "de", "tricas", "mcp_metrics_file", "ou", "mcp_index", "metrics_index", "csv", "opcional", "calcula", "baseline", "aproximada", "de", "tokens", "do", "reposit", "rio", "exemplos", "python", "reindex", "py", "clean", "python", "reindex", "py", "path", "src", "include", "py", "exclude", "tests", "python", "reindex", "py", "baseline", "estimate", "mcp_metrics_file", "mcp_index", "metrics_index", "csv", "python", "reindex", "py", "clean", "requer", "code_indexer_enhanced", "py", "com", "codeindexer", "index_repo_paths", "import", "os", "import", "sys", "import", "csv", "import", "json", "import", "shutil", "import", "time", "import", "argparse", "import", "datetime", "as", "dt", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "dict", "any", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "config", "tricas", "mesmo", "padr", "do", "context_pack", "agora", "usando", "caminho", "relativo", "pasta", "mcp_system", "metrics_path", "os", "environ", "get", "mcp_metrics_file", "str", "current_dir", "mcp_index", "metrics", "csv", "def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise"]}
{"chunk_id": "b3af9a038d8a451f49f61594", "file_path": "mcp_system/reindex.py", "start_line": 38, "end_line": 117, "content": "def _log_metrics(row: Dict[str, Any]) -> None:\n    os.makedirs(os.path.dirname(METRICS_PATH), exist_ok=True)\n    file_exists = os.path.exists(METRICS_PATH)\n    with open(METRICS_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.DictWriter(f, fieldnames=row.keys())\n        if not file_exists:\n            w.writeheader()\n        w.writerow(row)\n\ndef _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():", "mtime": 1756161419.1409552, "terms": ["def", "_log_metrics", "row", "dict", "str", "any", "none", "os", "makedirs", "os", "path", "dirname", "metrics_path", "exist_ok", "true", "file_exists", "os", "path", "exists", "metrics_path", "with", "open", "metrics_path", "newline", "encoding", "utf", "as", "csv", "dictwriter", "fieldnames", "row", "keys", "if", "not", "file_exists", "writeheader", "writerow", "row", "def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists"]}
{"chunk_id": "47da96a6c6806a5a1221c183", "file_path": "mcp_system/reindex.py", "start_line": 47, "end_line": 126, "content": "def _est_tokens_from_len(n_chars: int) -> int:\n    # heur√≠stica compat√≠vel com o indexador (~4 chars por token)\n    return max(1, n_chars // 4)\n\ndef parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1", "mtime": 1756161419.1409552, "terms": ["def", "_est_tokens_from_len", "n_chars", "int", "int", "heur", "stica", "compat", "vel", "com", "indexador", "chars", "por", "token", "return", "max", "n_chars", "def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks"]}
{"chunk_id": "8171a96c659d6b80e27ac0d5", "file_path": "mcp_system/reindex.py", "start_line": 51, "end_line": 130, "content": "def parse_args():\n    p = argparse.ArgumentParser(description=\"Reindexa o projeto para o MCP Code Indexer.\")\n    p.add_argument(\"--path\", default=\".\", help=\"Pasta ou arquivo inicial (default: .)\")\n    p.add_argument(\"--index-dir\", default=str(CURRENT_DIR / \".mcp_index\"), help=\"Diret√≥rio de √≠ndice (default: .mcp_index)\")\n    p.add_argument(\"--clean\", action=\"store_true\", help=\"Remove o √≠ndice antes de reindexar\")\n    p.add_argument(\"--recursive\", action=\"store_true\", default=True, help=\"Busca recursiva (default: True)\")\n    p.add_argument(\"--no-recursive\", dest=\"recursive\", action=\"store_false\", help=\"Desabilita busca recursiva\")\n    p.add_argument(\"--include\", action=\"append\", default=None,\n                   help='Glob de inclus√£o (pode repetir). Ex: --include \"**/*.py\"')\n    p.add_argument(\"--exclude\", action=\"append\", default=None,\n                   help='Glob de exclus√£o (pode repetir). Ex: --exclude \"**/tests/**\"')\n    p.add_argument(\"--baseline-estimate\", action=\"store_true\",\n                   help=\"Ap√≥s indexar, calcula baseline aproximada de tokens do repo\")\n    p.add_argument(\"--topn-baseline\", type=int, default=0,\n                   help=\"Se >0, considera apenas os N maiores chunks na baseline (0 = todos)\")\n    p.add_argument(\"--quiet\", action=\"store_true\", help=\"Menos sa√≠da no stdout\")\n    return p.parse_args()\n\ndef main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue", "mtime": 1756161419.1409552, "terms": ["def", "parse_args", "argparse", "argumentparser", "description", "reindexa", "projeto", "para", "mcp", "code", "indexer", "add_argument", "path", "default", "help", "pasta", "ou", "arquivo", "inicial", "default", "add_argument", "index", "dir", "default", "str", "current_dir", "mcp_index", "help", "diret", "rio", "de", "ndice", "default", "mcp_index", "add_argument", "clean", "action", "store_true", "help", "remove", "ndice", "antes", "de", "reindexar", "add_argument", "recursive", "action", "store_true", "default", "true", "help", "busca", "recursiva", "default", "true", "add_argument", "no", "recursive", "dest", "recursive", "action", "store_false", "help", "desabilita", "busca", "recursiva", "add_argument", "include", "action", "append", "default", "none", "help", "glob", "de", "inclus", "pode", "repetir", "ex", "include", "py", "add_argument", "exclude", "action", "append", "default", "none", "help", "glob", "de", "exclus", "pode", "repetir", "ex", "exclude", "tests", "add_argument", "baseline", "estimate", "action", "store_true", "help", "ap", "indexar", "calcula", "baseline", "aproximada", "de", "tokens", "do", "repo", "add_argument", "topn", "baseline", "type", "int", "default", "help", "se", "considera", "apenas", "os", "maiores", "chunks", "na", "baseline", "todos", "add_argument", "quiet", "action", "store_true", "help", "menos", "sa", "da", "no", "stdout", "return", "parse_args", "def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue"]}
{"chunk_id": "ba091a310e294cad294b691f", "file_path": "mcp_system/reindex.py", "start_line": 69, "end_line": 148, "content": "def main():\n    args = parse_args()\n    base_dir = Path.cwd()\n    index_dir = Path(args.index_dir)  # Agora usa o caminho absoluto\n\n    # Importa o indexador do seu projeto\n    try:\n        from code_indexer_enhanced import CodeIndexer, index_repo_paths  # type: ignore\n    except Exception as e:\n        print(\"[erro] N√£o foi poss√≠vel importar CodeIndexer/index_repo_paths de code_indexer_enhanced.py\", file=sys.stderr)\n        raise\n\n    if args.clean and index_dir.exists():\n        if not args.quiet:\n            print(f\"üîÑ Limpando √≠ndice anterior em: {index_dir}\")\n        shutil.rmtree(index_dir)\n\n    index_dir.mkdir(parents=True, exist_ok=True)\n\n    # Instancia o indexador com o mesmo diret√≥rio de √≠ndice\n    try:\n        indexer = CodeIndexer(index_dir=str(index_dir), repo_root=str(base_dir))\n    except TypeError:\n        # compat: se sua assinatura for diferente\n        indexer = CodeIndexer(index_dir=str(index_dir))\n\n    include_globs = args.include\n    exclude_globs = args.exclude\n\n    t0 = time.perf_counter()\n    res = index_repo_paths(\n        indexer,\n        paths=[args.path],\n        recursive=bool(args.recursive),\n        include_globs=include_globs,\n        exclude_globs=exclude_globs,\n    )\n    elapsed = round(time.perf_counter() - t0, 3)\n\n    files_indexed = int(res.get(\"files_indexed\", 0))\n    chunks = int(res.get(\"chunks\", 0))\n\n    baseline_tokens = None\n    if args.baseline_estimate:\n        # L√™ o √≠ndice persistido e soma tokens estimados por chunk\n        chunks_file = index_dir / \"chunks.jsonl\"\n        total_chars = 0\n        n_chunks = 0\n        if chunks_file.exists():\n            with chunks_file.open(\"r\", encoding=\"utf-8\") as f:\n                # se --topn-baseline > 0, carregamos tudo para ordenar; sen√£o, stream\n                if args.topn_baseline and args.topn_baseline > 0:\n                    all_chunks: List[Dict[str, Any]] = [json.loads(line) for line in f if line.strip()]\n                    all_chunks.sort(key=lambda c: len(c.get(\"content\", \"\")), reverse=True)\n                    all_chunks = all_chunks[:args.topn_baseline]\n                    for c in all_chunks:\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n                else:\n                    for line in f:\n                        if not line.strip():\n                            continue\n                        c = json.loads(line)\n                        total_chars += len(c.get(\"content\", \"\"))\n                        n_chunks += 1\n        baseline_tokens = _est_tokens_from_len(total_chars)\n        if not args.quiet:\n            print(f\"üìä Baseline (aprox.) de tokens do repo: {baseline_tokens} (chunks: {n_chunks})\")\n\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,", "mtime": 1756161419.1409552, "terms": ["def", "main", "args", "parse_args", "base_dir", "path", "cwd", "index_dir", "path", "args", "index_dir", "agora", "usa", "caminho", "absoluto", "importa", "indexador", "do", "seu", "projeto", "try", "from", "code_indexer_enhanced", "import", "codeindexer", "index_repo_paths", "type", "ignore", "except", "exception", "as", "print", "erro", "foi", "poss", "vel", "importar", "codeindexer", "index_repo_paths", "de", "code_indexer_enhanced", "py", "file", "sys", "stderr", "raise", "if", "args", "clean", "and", "index_dir", "exists", "if", "not", "args", "quiet", "print", "limpando", "ndice", "anterior", "em", "index_dir", "shutil", "rmtree", "index_dir", "index_dir", "mkdir", "parents", "true", "exist_ok", "true", "instancia", "indexador", "com", "mesmo", "diret", "rio", "de", "ndice", "try", "indexer", "codeindexer", "index_dir", "str", "index_dir", "repo_root", "str", "base_dir", "except", "typeerror", "compat", "se", "sua", "assinatura", "for", "diferente", "indexer", "codeindexer", "index_dir", "str", "index_dir", "include_globs", "args", "include", "exclude_globs", "args", "exclude", "t0", "time", "perf_counter", "res", "index_repo_paths", "indexer", "paths", "args", "path", "recursive", "bool", "args", "recursive", "include_globs", "include_globs", "exclude_globs", "exclude_globs", "elapsed", "round", "time", "perf_counter", "t0", "files_indexed", "int", "res", "get", "files_indexed", "chunks", "int", "res", "get", "chunks", "baseline_tokens", "none", "if", "args", "baseline_estimate", "ndice", "persistido", "soma", "tokens", "estimados", "por", "chunk", "chunks_file", "index_dir", "chunks", "jsonl", "total_chars", "n_chunks", "if", "chunks_file", "exists", "with", "chunks_file", "open", "encoding", "utf", "as", "se", "topn", "baseline", "carregamos", "tudo", "para", "ordenar", "sen", "stream", "if", "args", "topn_baseline", "and", "args", "topn_baseline", "all_chunks", "list", "dict", "str", "any", "json", "loads", "line", "for", "line", "in", "if", "line", "strip", "all_chunks", "sort", "key", "lambda", "len", "get", "content", "reverse", "true", "all_chunks", "all_chunks", "args", "topn_baseline", "for", "in", "all_chunks", "total_chars", "len", "get", "content", "n_chunks", "else", "for", "line", "in", "if", "not", "line", "strip", "continue", "json", "loads", "line", "total_chars", "len", "get", "content", "n_chunks", "baseline_tokens", "_est_tokens_from_len", "total_chars", "if", "not", "args", "quiet", "print", "baseline", "aprox", "de", "tokens", "do", "repo", "baseline_tokens", "chunks", "n_chunks", "if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks"]}
{"chunk_id": "2c930d141c1b8c1eca92b180", "file_path": "mcp_system/reindex.py", "start_line": 137, "end_line": 167, "content": "\n    if not args.quiet:\n        print(f\"‚úÖ Reindex conclu√≠da: files={files_indexed}, chunks={chunks}, tempo={elapsed}s, index_dir={index_dir}\")\n\n    # Loga linha no CSV de m√©tricas\n    row = {\n        \"ts\": dt.datetime.now(dt.UTC).isoformat(timespec=\"seconds\"),\n        \"op\": \"reindex\",\n        \"path\": str(args.path),\n        \"index_dir\": str(index_dir),\n        \"files_indexed\": files_indexed,\n        \"chunks\": chunks,\n        \"recursive\": bool(args.recursive),\n        \"include_globs\": \";\".join(include_globs) if include_globs else \"\",\n        \"exclude_globs\": \";\".join(exclude_globs) if exclude_globs else \"\",\n        \"elapsed_s\": elapsed,\n    }\n    if baseline_tokens is not None:\n        row[\"baseline_tokens_est\"] = baseline_tokens\n        row[\"topn_baseline\"] = int(args.topn_baseline or 0)\n\n    try:\n        _log_metrics(row)\n    except Exception:\n        # n√£o interrompe em caso de falha de log\n        pass\n\n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756161419.1409552, "terms": ["if", "not", "args", "quiet", "print", "reindex", "conclu", "da", "files", "files_indexed", "chunks", "chunks", "tempo", "elapsed", "index_dir", "index_dir", "loga", "linha", "no", "csv", "de", "tricas", "row", "ts", "dt", "datetime", "now", "dt", "utc", "isoformat", "timespec", "seconds", "op", "reindex", "path", "str", "args", "path", "index_dir", "str", "index_dir", "files_indexed", "files_indexed", "chunks", "chunks", "recursive", "bool", "args", "recursive", "include_globs", "join", "include_globs", "if", "include_globs", "else", "exclude_globs", "join", "exclude_globs", "if", "exclude_globs", "else", "elapsed_s", "elapsed", "if", "baseline_tokens", "is", "not", "none", "row", "baseline_tokens_est", "baseline_tokens", "row", "topn_baseline", "int", "args", "topn_baseline", "or", "try", "_log_metrics", "row", "except", "exception", "interrompe", "em", "caso", "de", "falha", "de", "log", "pass", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "3f993a8a8e009db391fa2800", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSetup autom√°tico do sistema MCP melhorado\nConfigura busca sem√¢ntica, auto-indexa√ß√£o e otimiza√ß√µes\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nfrom pathlib import Path\nfrom typing import Dict, List, Any\nimport pathlib\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ndef print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üöÄ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True", "mtime": 1756154931.1757026, "terms": ["usr", "bin", "env", "python3", "setup", "autom", "tico", "do", "sistema", "mcp", "melhorado", "configura", "busca", "sem", "ntica", "auto", "indexa", "otimiza", "es", "import", "os", "import", "sys", "import", "subprocess", "import", "time", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "any", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true"]}
{"chunk_id": "6444ec23468ea9e790a0f00a", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 18, "end_line": 97, "content": "def print_header(title: str):\n    \"\"\"Imprime header formatado\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"üöÄ {title}\")\n    print(f\"{'='*60}\")\n\ndef print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"", "mtime": 1756154931.1757026, "terms": ["def", "print_header", "title", "str", "imprime", "header", "formatado", "print", "print", "title", "print", "def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt"]}
{"chunk_id": "a9a46bf055a6209d3cafaa8f", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 24, "end_line": 103, "content": "def print_step(step: str, status: str = \"\"):\n    \"\"\"Imprime passo com status\"\"\"\n    if status == \"ok\":\n        print(f\"‚úÖ {step}\")\n    elif status == \"warn\":\n        print(f\"‚ö†Ô∏è  {step}\")\n    elif status == \"error\":\n        print(f\"‚ùå {step}\")\n    else:\n        print(f\"üîÑ {step}\")\n\ndef check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            ", "mtime": 1756154931.1757026, "terms": ["def", "print_step", "step", "str", "status", "str", "imprime", "passo", "com", "status", "if", "status", "ok", "print", "step", "elif", "status", "warn", "print", "step", "elif", "status", "error", "print", "step", "else", "print", "step", "def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true"]}
{"chunk_id": "ac77571b66ca63e9fe91742a", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 35, "end_line": 114, "content": "def check_dependencies() -> Dict[str, bool]:\n    \"\"\"Verifica depend√™ncias necess√°rias\"\"\"\n    print_header(\"Verificando Depend√™ncias\")\n    \n    deps = {}\n    \n    # Python b√°sico\n    try:\n        import sys\n        deps['python'] = sys.version_info >= (3, 8)\n        print_step(f\"Python {sys.version.split()[0]}\", \"ok\" if deps['python'] else \"error\")\n    except Exception:\n        deps['python'] = False\n        print_step(\"Python\", \"error\")\n    \n    # MCP SDK\n    try:\n        import mcp\n        deps['mcp'] = True\n        print_step(\"MCP SDK\", \"ok\")\n    except ImportError:\n        deps['mcp'] = False\n        print_step(\"MCP SDK (pip install mcp)\", \"error\")\n    \n    # Sentence Transformers\n    try:\n        import sentence_transformers\n        deps['sentence_transformers'] = True\n        print_step(\"Sentence Transformers\", \"ok\")\n    except ImportError:\n        deps['sentence_transformers'] = False\n        print_step(\"Sentence Transformers (busca sem√¢ntica)\", \"warn\")\n    \n    # Watchdog\n    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")", "mtime": 1756154931.1757026, "terms": ["def", "check_dependencies", "dict", "str", "bool", "verifica", "depend", "ncias", "necess", "rias", "print_header", "verificando", "depend", "ncias", "deps", "python", "sico", "try", "import", "sys", "deps", "python", "sys", "version_info", "print_step", "python", "sys", "version", "split", "ok", "if", "deps", "python", "else", "error", "except", "exception", "deps", "python", "false", "print_step", "python", "error", "mcp", "sdk", "try", "import", "mcp", "deps", "mcp", "true", "print_step", "mcp", "sdk", "ok", "except", "importerror", "deps", "mcp", "false", "print_step", "mcp", "sdk", "pip", "install", "mcp", "error", "sentence", "transformers", "try", "import", "sentence_transformers", "deps", "sentence_transformers", "true", "print_step", "sentence", "transformers", "ok", "except", "importerror", "deps", "sentence_transformers", "false", "print_step", "sentence", "transformers", "busca", "sem", "ntica", "warn", "watchdog", "try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers"]}
{"chunk_id": "f805a71e8cb390e18a189424", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 69, "end_line": 148, "content": "    try:\n        import watchdog\n        deps['watchdog'] = True\n        print_step(\"Watchdog\", \"ok\")\n    except ImportError:\n        deps['watchdog'] = False\n        print_step(\"Watchdog (auto-indexa√ß√£o)\", \"warn\")\n    \n    # NumPy\n    try:\n        import numpy\n        deps['numpy'] = True\n        print_step(\"NumPy\", \"ok\")\n    except ImportError:\n        deps['numpy'] = False\n        print_step(\"NumPy\", \"warn\")\n    \n    return deps\n\ndef install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")\n            if \"watchdog\" in missing_deps:\n                packages.append(\"watchdog>=3.0.0\")\n            if \"numpy\" in missing_deps:\n                packages.append(\"numpy>=1.21.0\")\n            if \"scikit_learn\" in missing_deps:\n                packages.append(\"scikit-learn>=1.0.0\")\n                \n            if packages:\n                print_step(\"Instalando pacotes individualmente\")\n                result = subprocess.run([\n                    sys.executable, \"-m\", \"pip\", \"install\"] + packages,\n                    capture_output=True, text=True\n                )\n                \n                if result.returncode == 0:\n                    print_step(\"Depend√™ncias instaladas\", \"ok\")\n                    return True\n                else:\n                    print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n                    return False\n            else:\n                print_step(\"Nenhum pacote para instalar\", \"ok\")\n                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar depend√™ncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diret√≥rio de √≠ndice\"\"\"\n    print_header(\"Configurando Diret√≥rio de √çndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"", "mtime": 1756154931.1757026, "terms": ["try", "import", "watchdog", "deps", "watchdog", "true", "print_step", "watchdog", "ok", "except", "importerror", "deps", "watchdog", "false", "print_step", "watchdog", "auto", "indexa", "warn", "numpy", "try", "import", "numpy", "deps", "numpy", "true", "print_step", "numpy", "ok", "except", "importerror", "deps", "numpy", "false", "print_step", "numpy", "warn", "return", "deps", "def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers", "if", "watchdog", "in", "missing_deps", "packages", "append", "watchdog", "if", "numpy", "in", "missing_deps", "packages", "append", "numpy", "if", "scikit_learn", "in", "missing_deps", "packages", "append", "scikit", "learn", "if", "packages", "print_step", "instalando", "pacotes", "individualmente", "result", "subprocess", "run", "sys", "executable", "pip", "install", "packages", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "print_step", "nenhum", "pacote", "para", "instalar", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index"]}
{"chunk_id": "d7986f82d676909d15dec12c", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 88, "end_line": 167, "content": "def install_dependencies(missing_deps: List[str]) -> bool:\n    \"\"\"Instala depend√™ncias em falta\"\"\"\n    if not missing_deps:\n        return True\n        \n    print_header(\"Instalando Depend√™ncias\")\n    \n    try:\n        # Instala requirements_enhanced.txt se existir\n        req_file = CURRENT_DIR / \"requirements_enhanced.txt\"\n        if req_file.exists():\n            print_step(\"Instalando via requirements_enhanced.txt\")\n            result = subprocess.run([\n                sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_file)\n            ], capture_output=True, text=True)\n            \n            if result.returncode == 0:\n                print_step(\"Depend√™ncias instaladas\", \"ok\")\n                return True\n            else:\n                print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n                return False\n        else:\n            # Instala pacotes individualmente\n            packages = []\n            if \"sentence_transformers\" in missing_deps:\n                packages.append(\"sentence-transformers>=2.0.0\")\n            if \"watchdog\" in missing_deps:\n                packages.append(\"watchdog>=3.0.0\")\n            if \"numpy\" in missing_deps:\n                packages.append(\"numpy>=1.21.0\")\n            if \"scikit_learn\" in missing_deps:\n                packages.append(\"scikit-learn>=1.0.0\")\n                \n            if packages:\n                print_step(\"Instalando pacotes individualmente\")\n                result = subprocess.run([\n                    sys.executable, \"-m\", \"pip\", \"install\"] + packages,\n                    capture_output=True, text=True\n                )\n                \n                if result.returncode == 0:\n                    print_step(\"Depend√™ncias instaladas\", \"ok\")\n                    return True\n                else:\n                    print_step(f\"Erro na instala√ß√£o: {result.stderr}\", \"error\")\n                    return False\n            else:\n                print_step(\"Nenhum pacote para instalar\", \"ok\")\n                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar depend√™ncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diret√≥rio de √≠ndice\"\"\"\n    print_header(\"Configurando Diret√≥rio de √çndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"Diret√≥rio de √≠ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diret√≥rio de √≠ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade b√°sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade B√°sica\")\n    \n    try:\n        # Testar importa√ß√£o dos m√≥dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"Importa√ß√£o do indexador b√°sico\", \"ok\")\n        \n        # Testar cria√ß√£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))", "mtime": 1756154931.1757026, "terms": ["def", "install_dependencies", "missing_deps", "list", "str", "bool", "instala", "depend", "ncias", "em", "falta", "if", "not", "missing_deps", "return", "true", "print_header", "instalando", "depend", "ncias", "try", "instala", "requirements_enhanced", "txt", "se", "existir", "req_file", "current_dir", "requirements_enhanced", "txt", "if", "req_file", "exists", "print_step", "instalando", "via", "requirements_enhanced", "txt", "result", "subprocess", "run", "sys", "executable", "pip", "install", "str", "req_file", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "instala", "pacotes", "individualmente", "packages", "if", "sentence_transformers", "in", "missing_deps", "packages", "append", "sentence", "transformers", "if", "watchdog", "in", "missing_deps", "packages", "append", "watchdog", "if", "numpy", "in", "missing_deps", "packages", "append", "numpy", "if", "scikit_learn", "in", "missing_deps", "packages", "append", "scikit", "learn", "if", "packages", "print_step", "instalando", "pacotes", "individualmente", "result", "subprocess", "run", "sys", "executable", "pip", "install", "packages", "capture_output", "true", "text", "true", "if", "result", "returncode", "print_step", "depend", "ncias", "instaladas", "ok", "return", "true", "else", "print_step", "erro", "na", "instala", "result", "stderr", "error", "return", "false", "else", "print_step", "nenhum", "pacote", "para", "instalar", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir"]}
{"chunk_id": "2f1304046725b97db9ac6153", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 137, "end_line": 216, "content": "                return True\n                \n    except Exception as e:\n        print_step(f\"Erro ao instalar depend√™ncias: {e}\", \"error\")\n        return False\n\ndef setup_index_directory():\n    \"\"\"Configura o diret√≥rio de √≠ndice\"\"\"\n    print_header(\"Configurando Diret√≥rio de √çndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"Diret√≥rio de √≠ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diret√≥rio de √≠ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade b√°sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade B√°sica\")\n    \n    try:\n        # Testar importa√ß√£o dos m√≥dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"Importa√ß√£o do indexador b√°sico\", \"ok\")\n        \n        # Testar cria√ß√£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"Cria√ß√£o do indexador\", \"ok\")\n        \n        # Testar m√©todos b√°sicos\n        stats = indexer.get_stats()\n        print_step(\"Obten√ß√£o de estat√≠sticas\", \"ok\")\n        \n        print_step(\"Testes b√°sicos conclu√≠dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes b√°sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"Fun√ß√£o principal de setup\"\"\"\n    print_header(\"Setup Autom√°tico do Sistema MCP\")\n    \n    # Verificar depend√™ncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDepend√™ncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as depend√™ncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instala√ß√£o de depend√™ncias. Saindo.\")\n                return 1\n        else:\n            print(\"Instala√ß√£o de depend√™ncias cancelada.\")\n    else:\n        print(\"\\n‚úÖ Todas as depend√™ncias est√£o instaladas!\")\n    \n    # Configurar diret√≥rio de √≠ndice\n    if not setup_index_directory():\n        print(\"Falha na configura√ß√£o do diret√≥rio de √≠ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade b√°sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes b√°sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup Conclu√≠do com Sucesso!\")\n    print(\"O sistema MCP est√° pronto para uso.\")\n    print(f\"O diret√≥rio de √≠ndice est√° localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n", "mtime": 1756154931.1757026, "terms": ["return", "true", "except", "exception", "as", "print_step", "erro", "ao", "instalar", "depend", "ncias", "error", "return", "false", "def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return"]}
{"chunk_id": "ec4afc2833c17ea315ca824d", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 143, "end_line": 218, "content": "def setup_index_directory():\n    \"\"\"Configura o diret√≥rio de √≠ndice\"\"\"\n    print_header(\"Configurando Diret√≥rio de √çndice\")\n    \n    try:\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        index_dir.mkdir(exist_ok=True)\n        print_step(f\"Diret√≥rio de √≠ndice criado: {index_dir}\", \"ok\")\n        return True\n    except Exception as e:\n        print_step(f\"Erro ao criar diret√≥rio de √≠ndice: {e}\", \"error\")\n        return False\n\ndef test_basic_functionality():\n    \"\"\"Testa funcionalidade b√°sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade B√°sica\")\n    \n    try:\n        # Testar importa√ß√£o dos m√≥dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"Importa√ß√£o do indexador b√°sico\", \"ok\")\n        \n        # Testar cria√ß√£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"Cria√ß√£o do indexador\", \"ok\")\n        \n        # Testar m√©todos b√°sicos\n        stats = indexer.get_stats()\n        print_step(\"Obten√ß√£o de estat√≠sticas\", \"ok\")\n        \n        print_step(\"Testes b√°sicos conclu√≠dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes b√°sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"Fun√ß√£o principal de setup\"\"\"\n    print_header(\"Setup Autom√°tico do Sistema MCP\")\n    \n    # Verificar depend√™ncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDepend√™ncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as depend√™ncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instala√ß√£o de depend√™ncias. Saindo.\")\n                return 1\n        else:\n            print(\"Instala√ß√£o de depend√™ncias cancelada.\")\n    else:\n        print(\"\\n‚úÖ Todas as depend√™ncias est√£o instaladas!\")\n    \n    # Configurar diret√≥rio de √≠ndice\n    if not setup_index_directory():\n        print(\"Falha na configura√ß√£o do diret√≥rio de √≠ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade b√°sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes b√°sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup Conclu√≠do com Sucesso!\")\n    print(\"O sistema MCP est√° pronto para uso.\")\n    print(f\"O diret√≥rio de √≠ndice est√° localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "setup_index_directory", "configura", "diret", "rio", "de", "ndice", "print_header", "configurando", "diret", "rio", "de", "ndice", "try", "index_dir", "current_dir", "mcp_index", "index_dir", "mkdir", "exist_ok", "true", "print_step", "diret", "rio", "de", "ndice", "criado", "index_dir", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "ao", "criar", "diret", "rio", "de", "ndice", "error", "return", "false", "def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "399a8d5f2197b1123cd7f778", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 156, "end_line": 218, "content": "def test_basic_functionality():\n    \"\"\"Testa funcionalidade b√°sica do MCP\"\"\"\n    print_header(\"Testando Funcionalidade B√°sica\")\n    \n    try:\n        # Testar importa√ß√£o dos m√≥dulos principais\n        from code_indexer_enhanced import BaseCodeIndexer\n        print_step(\"Importa√ß√£o do indexador b√°sico\", \"ok\")\n        \n        # Testar cria√ß√£o do indexador\n        index_dir = CURRENT_DIR / \".mcp_index\"\n        indexer = BaseCodeIndexer(index_dir=str(index_dir))\n        print_step(\"Cria√ß√£o do indexador\", \"ok\")\n        \n        # Testar m√©todos b√°sicos\n        stats = indexer.get_stats()\n        print_step(\"Obten√ß√£o de estat√≠sticas\", \"ok\")\n        \n        print_step(\"Testes b√°sicos conclu√≠dos com sucesso\", \"ok\")\n        return True\n        \n    except Exception as e:\n        print_step(f\"Erro nos testes b√°sicos: {e}\", \"error\")\n        return False\n\ndef main():\n    \"\"\"Fun√ß√£o principal de setup\"\"\"\n    print_header(\"Setup Autom√°tico do Sistema MCP\")\n    \n    # Verificar depend√™ncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDepend√™ncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as depend√™ncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instala√ß√£o de depend√™ncias. Saindo.\")\n                return 1\n        else:\n            print(\"Instala√ß√£o de depend√™ncias cancelada.\")\n    else:\n        print(\"\\n‚úÖ Todas as depend√™ncias est√£o instaladas!\")\n    \n    # Configurar diret√≥rio de √≠ndice\n    if not setup_index_directory():\n        print(\"Falha na configura√ß√£o do diret√≥rio de √≠ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade b√°sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes b√°sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup Conclu√≠do com Sucesso!\")\n    print(\"O sistema MCP est√° pronto para uso.\")\n    print(f\"O diret√≥rio de √≠ndice est√° localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "test_basic_functionality", "testa", "funcionalidade", "sica", "do", "mcp", "print_header", "testando", "funcionalidade", "sica", "try", "testar", "importa", "dos", "dulos", "principais", "from", "code_indexer_enhanced", "import", "basecodeindexer", "print_step", "importa", "do", "indexador", "sico", "ok", "testar", "cria", "do", "indexador", "index_dir", "current_dir", "mcp_index", "indexer", "basecodeindexer", "index_dir", "str", "index_dir", "print_step", "cria", "do", "indexador", "ok", "testar", "todos", "sicos", "stats", "indexer", "get_stats", "print_step", "obten", "de", "estat", "sticas", "ok", "print_step", "testes", "sicos", "conclu", "dos", "com", "sucesso", "ok", "return", "true", "except", "exception", "as", "print_step", "erro", "nos", "testes", "sicos", "error", "return", "false", "def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "9a283c92b0a2d85c790ce7c7", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 181, "end_line": 218, "content": "def main():\n    \"\"\"Fun√ß√£o principal de setup\"\"\"\n    print_header(\"Setup Autom√°tico do Sistema MCP\")\n    \n    # Verificar depend√™ncias\n    deps = check_dependencies()\n    missing_deps = [dep for dep, available in deps.items() if not available]\n    \n    if missing_deps:\n        print(f\"\\nDepend√™ncias em falta: {', '.join(missing_deps)}\")\n        install = input(\"\\nDeseja instalar as depend√™ncias em falta? (s/n): \").lower().strip()\n        if install == 's':\n            if not install_dependencies(missing_deps):\n                print(\"Falha na instala√ß√£o de depend√™ncias. Saindo.\")\n                return 1\n        else:\n            print(\"Instala√ß√£o de depend√™ncias cancelada.\")\n    else:\n        print(\"\\n‚úÖ Todas as depend√™ncias est√£o instaladas!\")\n    \n    # Configurar diret√≥rio de √≠ndice\n    if not setup_index_directory():\n        print(\"Falha na configura√ß√£o do diret√≥rio de √≠ndice. Saindo.\")\n        return 1\n    \n    # Testar funcionalidade b√°sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes b√°sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup Conclu√≠do com Sucesso!\")\n    print(\"O sistema MCP est√° pronto para uso.\")\n    print(f\"O diret√≥rio de √≠ndice est√° localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["def", "main", "fun", "principal", "de", "setup", "print_header", "setup", "autom", "tico", "do", "sistema", "mcp", "verificar", "depend", "ncias", "deps", "check_dependencies", "missing_deps", "dep", "for", "dep", "available", "in", "deps", "items", "if", "not", "available", "if", "missing_deps", "print", "ndepend", "ncias", "em", "falta", "join", "missing_deps", "install", "input", "ndeseja", "instalar", "as", "depend", "ncias", "em", "falta", "lower", "strip", "if", "install", "if", "not", "install_dependencies", "missing_deps", "print", "falha", "na", "instala", "de", "depend", "ncias", "saindo", "return", "else", "print", "instala", "de", "depend", "ncias", "cancelada", "else", "print", "todas", "as", "depend", "ncias", "est", "instaladas", "configurar", "diret", "rio", "de", "ndice", "if", "not", "setup_index_directory", "print", "falha", "na", "configura", "do", "diret", "rio", "de", "ndice", "saindo", "return", "testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "1c5e862a4630a5adb92a3f7d", "file_path": "mcp_system/setup_enhanced_mcp.py", "start_line": 205, "end_line": 218, "content": "    \n    # Testar funcionalidade b√°sica\n    if not test_basic_functionality():\n        print(\"Falha nos testes b√°sicos. Saindo.\")\n        return 1\n    \n    print_header(\"Setup Conclu√≠do com Sucesso!\")\n    print(\"O sistema MCP est√° pronto para uso.\")\n    print(f\"O diret√≥rio de √≠ndice est√° localizado em: {CURRENT_DIR / '.mcp_index'}\")\n    \n    return 0\n\nif __name__ == \"__main__\":\n    sys.exit(main())", "mtime": 1756154931.1757026, "terms": ["testar", "funcionalidade", "sica", "if", "not", "test_basic_functionality", "print", "falha", "nos", "testes", "sicos", "saindo", "return", "print_header", "setup", "conclu", "do", "com", "sucesso", "print", "sistema", "mcp", "est", "pronto", "para", "uso", "print", "diret", "rio", "de", "ndice", "est", "localizado", "em", "current_dir", "mcp_index", "return", "if", "__name__", "__main__", "sys", "exit", "main"]}
{"chunk_id": "018e6cb2e7752aae00f8f7e0", "file_path": "tests/test_currency_utils.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSuite de testes para o m√≥dulo de utilit√°rios de moeda.\n\"\"\"\n\nimport sys\nimport os\nimport pytest\nfrom decimal import Decimal\nimport pandas as pd\n\n# Adiciona o diret√≥rio raiz ao path para imports\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\ndef test_currency_detection_from_text():\n    \"\"\"Testa a detec√ß√£o de moeda a partir de texto.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa detec√ß√£o de Euro\n    text_eur = \"Movimenta√ß√µes em Euros (‚Ç¨) para o per√≠odo\"\n    assert CurrencyUtils.detect_currency_from_text(text_eur) == \"EUR\"\n    \n    # Testa detec√ß√£o de Real - corrigido para usar padr√£o que funciona com a implementa√ß√£o\n    text_brl = \"R$ 1.234,56 - Extrato em Reais\"\n    assert CurrencyUtils.detect_currency_from_text(text_brl) == \"BRL\"\n    \n    # Testa detec√ß√£o de D√≥lar\n    text_usd = \"Account Statement in US Dollars $1,234.56\"\n    assert CurrencyUtils.detect_currency_from_text(text_usd) == \"USD\"\n    \n    # Testa fallback para EUR\n    text_unknown = \"Random text without currency symbols\"\n    assert CurrencyUtils.detect_currency_from_text(text_unknown) == \"EUR\"\n\ndef test_currency_symbol_retrieval():\n    \"\"\"Testa a obten√ß√£o de s√≠mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa s√≠mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"‚Ç¨\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"¬£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formata√ß√£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formata√ß√£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"‚Ç¨ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"‚Ç¨ -1.234,56\"\n    \n    # Testa formata√ß√£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formata√ß√£o em D√≥lar (estilo americano) - corrigido para o formato real da implementa√ß√£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formata√ß√£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"‚Ç¨ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores", "mtime": 1756145852.325752, "terms": ["usr", "bin", "env", "python3", "suite", "de", "testes", "para", "dulo", "de", "utilit", "rios", "de", "moeda", "import", "sys", "import", "os", "import", "pytest", "from", "decimal", "import", "decimal", "import", "pandas", "as", "pd", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "imports", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "def", "test_currency_detection_from_text", "testa", "detec", "de", "moeda", "partir", "de", "texto", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "detec", "de", "euro", "text_eur", "movimenta", "es", "em", "euros", "para", "per", "odo", "assert", "currencyutils", "detect_currency_from_text", "text_eur", "eur", "testa", "detec", "de", "real", "corrigido", "para", "usar", "padr", "que", "funciona", "com", "implementa", "text_brl", "extrato", "em", "reais", "assert", "currencyutils", "detect_currency_from_text", "text_brl", "brl", "testa", "detec", "de", "lar", "text_usd", "account", "statement", "in", "us", "dollars", "assert", "currencyutils", "detect_currency_from_text", "text_usd", "usd", "testa", "fallback", "para", "eur", "text_unknown", "random", "text", "without", "currency", "symbols", "assert", "currencyutils", "detect_currency_from_text", "text_unknown", "eur", "def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores"]}
{"chunk_id": "5674ac45320904ca7313c3ef", "file_path": "tests/test_currency_utils.py", "start_line": 16, "end_line": 95, "content": "def test_currency_detection_from_text():\n    \"\"\"Testa a detec√ß√£o de moeda a partir de texto.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa detec√ß√£o de Euro\n    text_eur = \"Movimenta√ß√µes em Euros (‚Ç¨) para o per√≠odo\"\n    assert CurrencyUtils.detect_currency_from_text(text_eur) == \"EUR\"\n    \n    # Testa detec√ß√£o de Real - corrigido para usar padr√£o que funciona com a implementa√ß√£o\n    text_brl = \"R$ 1.234,56 - Extrato em Reais\"\n    assert CurrencyUtils.detect_currency_from_text(text_brl) == \"BRL\"\n    \n    # Testa detec√ß√£o de D√≥lar\n    text_usd = \"Account Statement in US Dollars $1,234.56\"\n    assert CurrencyUtils.detect_currency_from_text(text_usd) == \"USD\"\n    \n    # Testa fallback para EUR\n    text_unknown = \"Random text without currency symbols\"\n    assert CurrencyUtils.detect_currency_from_text(text_unknown) == \"EUR\"\n\ndef test_currency_symbol_retrieval():\n    \"\"\"Testa a obten√ß√£o de s√≠mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa s√≠mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"‚Ç¨\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"¬£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formata√ß√£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formata√ß√£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"‚Ç¨ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"‚Ç¨ -1.234,56\"\n    \n    # Testa formata√ß√£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formata√ß√£o em D√≥lar (estilo americano) - corrigido para o formato real da implementa√ß√£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formata√ß√£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"‚Ç¨ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo s√≠mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com s√≠mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes", "mtime": 1756145852.325752, "terms": ["def", "test_currency_detection_from_text", "testa", "detec", "de", "moeda", "partir", "de", "texto", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "detec", "de", "euro", "text_eur", "movimenta", "es", "em", "euros", "para", "per", "odo", "assert", "currencyutils", "detect_currency_from_text", "text_eur", "eur", "testa", "detec", "de", "real", "corrigido", "para", "usar", "padr", "que", "funciona", "com", "implementa", "text_brl", "extrato", "em", "reais", "assert", "currencyutils", "detect_currency_from_text", "text_brl", "brl", "testa", "detec", "de", "lar", "text_usd", "account", "statement", "in", "us", "dollars", "assert", "currencyutils", "detect_currency_from_text", "text_usd", "usd", "testa", "fallback", "para", "eur", "text_unknown", "random", "text", "without", "currency", "symbols", "assert", "currencyutils", "detect_currency_from_text", "text_unknown", "eur", "def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes"]}
{"chunk_id": "b64cffad2da353661d1148a1", "file_path": "tests/test_currency_utils.py", "start_line": 36, "end_line": 96, "content": "def test_currency_symbol_retrieval():\n    \"\"\"Testa a obten√ß√£o de s√≠mbolos de moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa s√≠mbolos conhecidos\n    assert CurrencyUtils.get_currency_symbol(\"EUR\") == \"‚Ç¨\"\n    assert CurrencyUtils.get_currency_symbol(\"BRL\") == \"R$\"\n    assert CurrencyUtils.get_currency_symbol(\"USD\") == \"$\"\n    assert CurrencyUtils.get_currency_symbol(\"GBP\") == \"¬£\"\n    \n    # Testa moeda desconhecida (fallback)\n    assert CurrencyUtils.get_currency_symbol(\"XYZ\") == \"XYZ\"\n\ndef test_currency_formatting():\n    \"\"\"Testa a formata√ß√£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formata√ß√£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"‚Ç¨ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"‚Ç¨ -1.234,56\"\n    \n    # Testa formata√ß√£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formata√ß√£o em D√≥lar (estilo americano) - corrigido para o formato real da implementa√ß√£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formata√ß√£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"‚Ç¨ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo s√≠mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com s√≠mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_symbol_retrieval", "testa", "obten", "de", "mbolos", "de", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "mbolos", "conhecidos", "assert", "currencyutils", "get_currency_symbol", "eur", "assert", "currencyutils", "get_currency_symbol", "brl", "assert", "currencyutils", "get_currency_symbol", "usd", "assert", "currencyutils", "get_currency_symbol", "gbp", "testa", "moeda", "desconhecida", "fallback", "assert", "currencyutils", "get_currency_symbol", "xyz", "xyz", "def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "77c702a4656a69c66e34f137", "file_path": "tests/test_currency_utils.py", "start_line": 49, "end_line": 96, "content": "def test_currency_formatting():\n    \"\"\"Testa a formata√ß√£o de valores com moeda.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Testa formata√ß√£o em Euro (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"EUR\") == \"‚Ç¨ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"EUR\") == \"‚Ç¨ -1.234,56\"\n    \n    # Testa formata√ß√£o em Real (estilo europeu)\n    assert CurrencyUtils.format_currency(1234.56, \"BRL\") == \"R$ 1.234,56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"BRL\") == \"R$ -1.234,56\"\n    \n    # Testa formata√ß√£o em D√≥lar (estilo americano) - corrigido para o formato real da implementa√ß√£o\n    assert CurrencyUtils.format_currency(1234.56, \"USD\") == \"$ 1,234.56\"\n    assert CurrencyUtils.format_currency(-1234.56, \"USD\") == \"$ -1,234.56\"\n    \n    # Testa formata√ß√£o com zero\n    assert CurrencyUtils.format_currency(0, \"EUR\") == \"‚Ç¨ 0,00\"\n\ndef test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo s√≠mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com s√≠mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_formatting", "testa", "formata", "de", "valores", "com", "moeda", "from", "src", "utils", "currency_utils", "import", "currencyutils", "testa", "formata", "em", "euro", "estilo", "europeu", "assert", "currencyutils", "format_currency", "eur", "assert", "currencyutils", "format_currency", "eur", "testa", "formata", "em", "real", "estilo", "europeu", "assert", "currencyutils", "format_currency", "brl", "assert", "currencyutils", "format_currency", "brl", "testa", "formata", "em", "lar", "estilo", "americano", "corrigido", "para", "formato", "real", "da", "implementa", "assert", "currencyutils", "format_currency", "usd", "assert", "currencyutils", "format_currency", "usd", "testa", "formata", "com", "zero", "assert", "currencyutils", "format_currency", "eur", "def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "4cba7a0e9e26ed1052783374", "file_path": "tests/test_currency_utils.py", "start_line": 68, "end_line": 96, "content": "def test_currency_extraction_from_dataframe():\n    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo s√≠mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com s√≠mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["def", "test_currency_extraction_from_dataframe", "testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "90dcca73a44b24ccd7436c25", "file_path": "tests/test_currency_utils.py", "start_line": 69, "end_line": 96, "content": "    \"\"\"Testa a extra√ß√£o de moeda a partir de um DataFrame.\"\"\"\n    from src.utils.currency_utils import CurrencyUtils\n    \n    # Cria um DataFrame simulando dados de extrato com valores monet√°rios\n    data = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['‚Ç¨ -85,30', '‚Ç¨ 2.500,00']  # Valores com s√≠mbolo de euro\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve detectar Euro a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df) == \"EUR\"\n    \n    # Testa com valores contendo s√≠mbolo de Real\n    data_brl = {\n        'Data': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Pagamento conta luz', 'Dep√≥sito sal√°rio'],\n        'Valor': ['R$ -85,30', 'R$ 2.500,00']  # Valores com s√≠mbolo de real\n    }\n    df_brl = pd.DataFrame(data_brl)\n    \n    # Deve detectar Real a partir dos valores\n    assert CurrencyUtils.extract_currency_from_dataframe(df_brl) == \"BRL\"\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145852.325752, "terms": ["testa", "extra", "de", "moeda", "partir", "de", "um", "dataframe", "from", "src", "utils", "currency_utils", "import", "currencyutils", "cria", "um", "dataframe", "simulando", "dados", "de", "extrato", "com", "valores", "monet", "rios", "data", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "euro", "df", "pd", "dataframe", "data", "deve", "detectar", "euro", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df", "eur", "testa", "com", "valores", "contendo", "mbolo", "de", "real", "data_brl", "data", "descri", "pagamento", "conta", "luz", "dep", "sito", "sal", "rio", "valor", "valores", "com", "mbolo", "de", "real", "df_brl", "pd", "dataframe", "data_brl", "deve", "detectar", "real", "partir", "dos", "valores", "assert", "currencyutils", "extract_currency_from_dataframe", "df_brl", "brl", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "95dcde83e904f9c9e58813a1", "file_path": "tests/test_csv_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nTestes para o leitor de extratos CSV.\n\"\"\"\nimport sys\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nimport pytest\nimport pandas as pd\n\n# Adiciona o diret√≥rio raiz ao path para importa√ß√µes\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\nfrom src.infrastructure.readers.csv_reader import CSVStatementReader\nfrom src.domain.models import Transaction, TransactionType\n\n\ndef test_csv_reader_can_read():\n    \"\"\"Testa se o CSV reader pode identificar arquivos CSV.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Deve retornar True para arquivos CSV\n    assert reader.can_read(Path(\"test.csv\")) == True\n    assert reader.can_read(Path(\"test.CSV\")) == True\n    \n    # Deve retornar False para outros formatos\n    assert reader.can_read(Path(\"test.pdf\")) == False\n    assert reader.can_read(Path(\"test.xlsx\")) == False\n    \n    # Testa com string tamb√©m\n    assert reader.can_read(\"test.csv\") == True\n\n\ndef test_csv_reader_extract_transactions():\n    \"\"\"Testa a extra√ß√£o de transa√ß√µes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transa√ß√£o (receita)\n    assert transactions[0].description == 'Sal√°rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transa√ß√£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transa√ß√£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n", "mtime": 1756147893.729488, "terms": ["testes", "para", "leitor", "de", "extratos", "csv", "import", "sys", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "import", "pytest", "import", "pandas", "as", "pd", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "importa", "es", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "def", "test_csv_reader_can_read", "testa", "se", "csv", "reader", "pode", "identificar", "arquivos", "csv", "reader", "csvstatementreader", "deve", "retornar", "true", "para", "arquivos", "csv", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "csv", "true", "deve", "retornar", "false", "para", "outros", "formatos", "assert", "reader", "can_read", "path", "test", "pdf", "false", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "testa", "com", "string", "tamb", "assert", "reader", "can_read", "test", "csv", "true", "def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime"]}
{"chunk_id": "58652c5eb183f0a1de1a843d", "file_path": "tests/test_csv_reader.py", "start_line": 20, "end_line": 99, "content": "def test_csv_reader_can_read():\n    \"\"\"Testa se o CSV reader pode identificar arquivos CSV.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Deve retornar True para arquivos CSV\n    assert reader.can_read(Path(\"test.csv\")) == True\n    assert reader.can_read(Path(\"test.CSV\")) == True\n    \n    # Deve retornar False para outros formatos\n    assert reader.can_read(Path(\"test.pdf\")) == False\n    assert reader.can_read(Path(\"test.xlsx\")) == False\n    \n    # Testa com string tamb√©m\n    assert reader.can_read(\"test.csv\") == True\n\n\ndef test_csv_reader_extract_transactions():\n    \"\"\"Testa a extra√ß√£o de transa√ß√µes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transa√ß√£o (receita)\n    assert transactions[0].description == 'Sal√°rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transa√ß√£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transa√ß√£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monet√°rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com par√™nteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_can_read", "testa", "se", "csv", "reader", "pode", "identificar", "arquivos", "csv", "reader", "csvstatementreader", "deve", "retornar", "true", "para", "arquivos", "csv", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "csv", "true", "deve", "retornar", "false", "para", "outros", "formatos", "assert", "reader", "can_read", "path", "test", "pdf", "false", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "testa", "com", "string", "tamb", "assert", "reader", "can_read", "test", "csv", "true", "def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit"]}
{"chunk_id": "de10b938e85a3c7633646728", "file_path": "tests/test_csv_reader.py", "start_line": 36, "end_line": 115, "content": "def test_csv_reader_extract_transactions():\n    \"\"\"Testa a extra√ß√£o de transa√ß√µes de um DataFrame.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'data': ['01/01/2023', '02/01/2023', '03/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado', 'Conta de Luz'],\n        'valor': ['2500.00', '-150.50', '-80.00']\n    }\n    df = pd.DataFrame(data)\n    \n    transactions = reader._extract_transactions(df)\n    \n    assert len(transactions) == 3\n    \n    # Verifica a primeira transa√ß√£o (receita)\n    assert transactions[0].description == 'Sal√°rio'\n    assert transactions[0].amount == Decimal('2500.00')\n    assert transactions[0].type == TransactionType.CREDIT\n    \n    # Verifica a segunda transa√ß√£o (despesa)\n    assert transactions[1].description == 'Supermercado'\n    assert transactions[1].amount == Decimal('150.50')\n    assert transactions[1].type == TransactionType.DEBIT\n    \n    # Verifica a terceira transa√ß√£o (despesa)\n    assert transactions[2].description == 'Conta de Luz'\n    assert transactions[2].amount == Decimal('80.00')\n    assert transactions[2].type == TransactionType.DEBIT\n\n\ndef test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monet√°rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com par√™nteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com v√≠rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores v√°lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com v√≠rgula", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_extract_transactions", "testa", "extra", "de", "transa", "es", "de", "um", "dataframe", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descricao", "sal", "rio", "supermercado", "conta", "de", "luz", "valor", "df", "pd", "dataframe", "data", "transactions", "reader", "_extract_transactions", "df", "assert", "len", "transactions", "verifica", "primeira", "transa", "receita", "assert", "transactions", "description", "sal", "rio", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "credit", "verifica", "segunda", "transa", "despesa", "assert", "transactions", "description", "supermercado", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "verifica", "terceira", "transa", "despesa", "assert", "transactions", "description", "conta", "de", "luz", "assert", "transactions", "amount", "decimal", "assert", "transactions", "type", "transactiontype", "debit", "def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula"]}
{"chunk_id": "ece790707a034ab8bb607554", "file_path": "tests/test_csv_reader.py", "start_line": 68, "end_line": 147, "content": "def test_csv_reader_parse_date():\n    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monet√°rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com par√™nteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com v√≠rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores v√°lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com v√≠rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores inv√°lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    ", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_date", "testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri"]}
{"chunk_id": "a769ed1a55d2886508beb7d8", "file_path": "tests/test_csv_reader.py", "start_line": 69, "end_line": 148, "content": "    \"\"\"Testa o parsing de datas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa diferentes formatos de data\n    assert reader._parse_date('01/01/2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('01-01-2023') == datetime(2023, 1, 1)\n    assert reader._parse_date('2023-01-01') == datetime(2023, 1, 1)\n    \n    # Testa com pandas datetime parsing\n    result = reader._parse_date('01/01/2023')\n    assert isinstance(result, datetime)\n\n\ndef test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monet√°rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com par√™nteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com v√≠rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores v√°lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com v√≠rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores inv√°lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    \n    # Deve retornar None para colunas inexistentes", "mtime": 1756147893.729488, "terms": ["testa", "parsing", "de", "datas", "reader", "csvstatementreader", "testa", "diferentes", "formatos", "de", "data", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "assert", "reader", "_parse_date", "datetime", "testa", "com", "pandas", "datetime", "parsing", "result", "reader", "_parse_date", "assert", "isinstance", "result", "datetime", "def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes"]}
{"chunk_id": "5881ef2dced199e32cbbf02f", "file_path": "tests/test_csv_reader.py", "start_line": 82, "end_line": 161, "content": "def test_csv_reader_parse_amount():\n    \"\"\"Testa o parsing de valores monet√°rios.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores positivos\n    amount, transaction_type = reader._parse_amount('2500.00')\n    assert amount == Decimal('2500.00')\n    assert transaction_type == TransactionType.CREDIT\n    \n    # Testa valores negativos\n    amount, transaction_type = reader._parse_amount('-150.50')\n    assert amount == Decimal('150.50')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com par√™nteses (formato comum para negativos)\n    amount, transaction_type = reader._parse_amount('(80.00)')\n    assert amount == Decimal('80.00')\n    assert transaction_type == TransactionType.DEBIT\n    \n    # Testa valores com v√≠rgula como separador decimal\n    amount, transaction_type = reader._parse_amount('1.500,75')\n    assert amount == Decimal('1500.75')\n    assert transaction_type == TransactionType.CREDIT\n\n\ndef test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores v√°lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com v√≠rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores inv√°lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame m√≠nimo sem informa√ß√£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV tempor√°rio", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_amount", "testa", "parsing", "de", "valores", "monet", "rios", "reader", "csvstatementreader", "testa", "valores", "positivos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "testa", "valores", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "par", "nteses", "formato", "comum", "para", "negativos", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "debit", "testa", "valores", "com", "rgula", "como", "separador", "decimal", "amount", "transaction_type", "reader", "_parse_amount", "assert", "amount", "decimal", "assert", "transaction_type", "transactiontype", "credit", "def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio"]}
{"chunk_id": "605e503c575b681184f97416", "file_path": "tests/test_csv_reader.py", "start_line": 107, "end_line": 176, "content": "def test_csv_reader_parse_balance():\n    \"\"\"Testa o parsing de saldos.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Testa valores v√°lidos\n    balance = reader._parse_balance('2500.00')\n    assert balance == Decimal('2500.00')\n    \n    # Testa valores com v√≠rgula\n    balance = reader._parse_balance('1.500,75')\n    assert balance == Decimal('1500.75')\n    \n    # Testa valores inv√°lidos\n    balance = reader._parse_balance('invalid')\n    assert balance is None\n    \n    # Testa valores vazios\n    balance = reader._parse_balance('')\n    assert balance is None\n\n\ndef test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame m√≠nimo sem informa√ß√£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV tempor√°rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padr√£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo tempor√°rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_parse_balance", "testa", "parsing", "de", "saldos", "reader", "csvstatementreader", "testa", "valores", "lidos", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "com", "rgula", "balance", "reader", "_parse_balance", "assert", "balance", "decimal", "testa", "valores", "inv", "lidos", "balance", "reader", "_parse_balance", "invalid", "assert", "balance", "is", "none", "testa", "valores", "vazios", "balance", "reader", "_parse_balance", "assert", "balance", "is", "none", "def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "212518b5b09514dd1ad7f2bc", "file_path": "tests/test_csv_reader.py", "start_line": 128, "end_line": 176, "content": "def test_csv_reader_find_column():\n    \"\"\"Testa a busca de colunas.\"\"\"\n    reader = CSVStatementReader()\n    \n    # Cria um DataFrame de exemplo\n    data = {\n        'Data': ['01/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio'],\n        'Valor': ['2500.00']\n    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame m√≠nimo sem informa√ß√£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV tempor√°rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padr√£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo tempor√°rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_find_column", "testa", "busca", "de", "colunas", "reader", "csvstatementreader", "cria", "um", "dataframe", "de", "exemplo", "data", "data", "descri", "sal", "rio", "valor", "df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "d0ae111bd7ce5dbe999403ab", "file_path": "tests/test_csv_reader.py", "start_line": 137, "end_line": 176, "content": "    }\n    df = pd.DataFrame(data)\n    \n    # Deve encontrar colunas com nomes diferentes (case insensitive)\n    assert reader._find_column(df, ['data', 'date']) == 'Data'\n    assert reader._find_column(df, ['descricao', 'description']) == 'Descri√ß√£o'\n    assert reader._find_column(df, ['valor', 'amount']) == 'Valor'\n    \n    # Testa com acentos\n    assert reader._find_column(df, ['descri√ß√£o', 'description']) == 'Descri√ß√£o'\n    \n    # Deve retornar None para colunas inexistentes\n    assert reader._find_column(df, ['saldo', 'balance']) is None\n\n\ndef test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame m√≠nimo sem informa√ß√£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV tempor√°rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padr√£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo tempor√°rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["df", "pd", "dataframe", "data", "deve", "encontrar", "colunas", "com", "nomes", "diferentes", "case", "insensitive", "assert", "reader", "_find_column", "df", "data", "date", "data", "assert", "reader", "_find_column", "df", "descricao", "description", "descri", "assert", "reader", "_find_column", "df", "valor", "amount", "valor", "testa", "com", "acentos", "assert", "reader", "_find_column", "df", "descri", "description", "descri", "deve", "retornar", "none", "para", "colunas", "inexistentes", "assert", "reader", "_find_column", "df", "saldo", "balance", "is", "none", "def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "2c908d230eae25a22aea2b18", "file_path": "tests/test_csv_reader.py", "start_line": 152, "end_line": 176, "content": "def test_csv_reader_defaults_to_euro_if_no_currency():\n    # Cria um DataFrame m√≠nimo sem informa√ß√£o de moeda e sem valores que possam ser interpretados como CAD\n    data = {\n        'data': ['01/01/2023', '02/01/2023'],\n        'descricao': ['Sal√°rio', 'Supermercado'],\n        'valor': ['1000', '-50']\n    }\n    df = pd.DataFrame(data)\n\n    # Salva em um arquivo CSV tempor√°rio\n    temp_csv = Path('temp_test_no_currency.csv')\n    df.to_csv(temp_csv, index=False)\n\n    reader = CSVStatementReader()\n    statement = reader.read(temp_csv)\n\n    # A moeda deve ser EUR por padr√£o\n    assert getattr(reader, 'currency', None) == 'EUR'\n\n    # Limpa o arquivo tempor√°rio\n    temp_csv.unlink()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147893.729488, "terms": ["def", "test_csv_reader_defaults_to_euro_if_no_currency", "cria", "um", "dataframe", "nimo", "sem", "informa", "de", "moeda", "sem", "valores", "que", "possam", "ser", "interpretados", "como", "cad", "data", "data", "descricao", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "salva", "em", "um", "arquivo", "csv", "tempor", "rio", "temp_csv", "path", "temp_test_no_currency", "csv", "df", "to_csv", "temp_csv", "index", "false", "reader", "csvstatementreader", "statement", "reader", "read", "temp_csv", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "getattr", "reader", "currency", "none", "eur", "limpa", "arquivo", "tempor", "rio", "temp_csv", "unlink", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "44d5ab6f1d23e1dd00bd559c", "file_path": "tests/test_suite.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nSuite de testes b√°sica para o projeto de an√°lise de extratos banc√°rios\n\"\"\"\n\nimport sys\nimport os\nimport pytest\nfrom decimal import Decimal\nfrom datetime import datetime\nimport uuid\n\n# Adiciona o diret√≥rio raiz ao path para imports\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\ndef test_imports():\n    \"\"\"Testa se os principais m√≥dulos podem ser importados\"\"\"\n    try:\n        import main\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar main: {e}\")\n    \n    try:\n        from src.domain import models\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar src.domain.models: {e}\")\n    \n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar PDFStatementReader: {e}\")\n\ndef test_model_creation():\n    \"\"\"Testa a cria√ß√£o de modelos b√°sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa cria√ß√£o de uma transa√ß√£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa cria√ß√£o de um extrato banc√°rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa cria√ß√£o de um resultado de an√°lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],", "mtime": 1756145955.0716913, "terms": ["usr", "bin", "env", "python3", "suite", "de", "testes", "sica", "para", "projeto", "de", "an", "lise", "de", "extratos", "banc", "rios", "import", "sys", "import", "os", "import", "pytest", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "import", "uuid", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "imports", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "def", "test_imports", "testa", "se", "os", "principais", "dulos", "podem", "ser", "importados", "try", "import", "main", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "main", "try", "from", "src", "domain", "import", "models", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "src", "domain", "models", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "pdfstatementreader", "def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts"]}
{"chunk_id": "31cf527edbbec1bb1d0d8861", "file_path": "tests/test_suite.py", "start_line": 17, "end_line": 91, "content": "def test_imports():\n    \"\"\"Testa se os principais m√≥dulos podem ser importados\"\"\"\n    try:\n        import main\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar main: {e}\")\n    \n    try:\n        from src.domain import models\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar src.domain.models: {e}\")\n    \n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        assert True\n    except Exception as e:\n        pytest.fail(f\"Falha ao importar PDFStatementReader: {e}\")\n\ndef test_model_creation():\n    \"\"\"Testa a cria√ß√£o de modelos b√°sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa cria√ß√£o de uma transa√ß√£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa cria√ß√£o de um extrato banc√°rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa cria√ß√£o de um resultado de an√°lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["def", "test_imports", "testa", "se", "os", "principais", "dulos", "podem", "ser", "importados", "try", "import", "main", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "main", "try", "from", "src", "domain", "import", "models", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "src", "domain", "models", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "assert", "true", "except", "exception", "as", "pytest", "fail", "falha", "ao", "importar", "pdfstatementreader", "def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "46460358db719b0aa06fc5f4", "file_path": "tests/test_suite.py", "start_line": 37, "end_line": 91, "content": "def test_model_creation():\n    \"\"\"Testa a cria√ß√£o de modelos b√°sicos\"\"\"\n    try:\n        from src.domain.models import Transaction, TransactionType, BankStatement, AnalysisResult, TransactionCategory\n        from decimal import Decimal\n        from datetime import datetime\n        \n        # Testa cria√ß√£o de uma transa√ß√£o simples\n        transaction = Transaction(\n            date=datetime.now(),\n            description=\"Test transaction\",\n            amount=Decimal(\"100.50\"),\n            type=TransactionType.DEBIT\n        )\n        \n        assert transaction.amount == Decimal(\"100.50\")\n        assert transaction.type == TransactionType.DEBIT\n        \n        # Testa cria√ß√£o de um extrato banc√°rio com moeda\n        statement = BankStatement(\n            bank_name=\"Test Bank\",\n            account_number=\"12345-6\",\n            period_start=datetime.now(),\n            period_end=datetime.now(),\n            initial_balance=Decimal(\"1000.00\"),\n            final_balance=Decimal(\"1100.50\"),\n            transactions=[transaction],\n            currency=\"EUR\"  # Testa novo campo de moeda\n        )\n        \n        assert statement.bank_name == \"Test Bank\"\n        assert statement.currency == \"EUR\"\n        \n        # Testa cria√ß√£o de um resultado de an√°lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["def", "test_model_creation", "testa", "cria", "de", "modelos", "sicos", "try", "from", "src", "domain", "models", "import", "transaction", "transactiontype", "bankstatement", "analysisresult", "transactioncategory", "from", "decimal", "import", "decimal", "from", "datetime", "import", "datetime", "testa", "cria", "de", "uma", "transa", "simples", "transaction", "transaction", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "debit", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "debit", "testa", "cria", "de", "um", "extrato", "banc", "rio", "com", "moeda", "statement", "bankstatement", "bank_name", "test", "bank", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "transactions", "transaction", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "assert", "statement", "bank_name", "test", "bank", "assert", "statement", "currency", "eur", "testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "1dcfdbe11d3d9beea7c0e57d", "file_path": "tests/test_suite.py", "start_line": 69, "end_line": 91, "content": "        \n        # Testa cria√ß√£o de um resultado de an√°lise com moeda\n        analysis_result = AnalysisResult(\n            statement_id=str(uuid.uuid4()),\n            total_income=Decimal(\"1500.00\"),\n            total_expenses=Decimal(\"1200.00\"),\n            net_flow=Decimal(\"300.00\"),\n            currency=\"EUR\",  # Testa novo campo de moeda\n            categories_summary={TransactionCategory.NAO_CATEGORIZADO: Decimal(\"300.00\")},\n            monthly_summary={\"2023-01\": {\"income\": Decimal(\"1500.00\"), \"expenses\": Decimal(\"1200.00\")}},\n            metadata={},\n            alerts=[],\n            insights=[]\n        )\n        \n        assert analysis_result.total_income == Decimal(\"1500.00\")\n        assert analysis_result.currency == \"EUR\"\n    except Exception as e:\n        pytest.fail(f\"Falha ao criar modelo: {e}\")\n\nif __name__ == \"__main__\":\n    # Executa os testes\n    pytest.main([__file__, \"-v\"])", "mtime": 1756145955.0716913, "terms": ["testa", "cria", "de", "um", "resultado", "de", "an", "lise", "com", "moeda", "analysis_result", "analysisresult", "statement_id", "str", "uuid", "uuid4", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "eur", "testa", "novo", "campo", "de", "moeda", "categories_summary", "transactioncategory", "nao_categorizado", "decimal", "monthly_summary", "income", "decimal", "expenses", "decimal", "metadata", "alerts", "insights", "assert", "analysis_result", "total_income", "decimal", "assert", "analysis_result", "currency", "eur", "except", "exception", "as", "pytest", "fail", "falha", "ao", "criar", "modelo", "if", "__name__", "__main__", "executa", "os", "testes", "pytest", "main", "__file__"]}
{"chunk_id": "09b3445e571a4749a735fda7", "file_path": "tests/test_comprehensive_suite.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nTestes abrangentes para o sistema de an√°lise de extratos.\n\"\"\"\nimport sys\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nimport pytest\nimport uuid\n\n# Adiciona o diret√≥rio raiz ao path para importa√ß√µes\nproject_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nsys.path.insert(0, project_root)\n\n# Testes de modelos de dom√≠nio\ndef test_domain_models():\n    \"\"\"Testa a cria√ß√£o de modelos de dom√≠nio.\"\"\"\n    from src.domain.models import Transaction, BankStatement, AnalysisResult, TransactionType, TransactionCategory\n    \n    # Testa cria√ß√£o de transa√ß√£o\n    transaction = Transaction(\n        id=str(uuid.uuid4()),\n        date=datetime.now(),\n        description=\"Test transaction\",\n        amount=Decimal(\"100.50\"),\n        type=TransactionType.CREDIT,\n        category=TransactionCategory.SALARIO\n    )\n    \n    assert transaction.description == \"Test transaction\"\n    assert transaction.amount == Decimal(\"100.50\")\n    assert transaction.type == TransactionType.CREDIT\n    assert transaction.category == TransactionCategory.SALARIO\n    assert transaction.is_income == True\n    assert transaction.is_expense == False\n    \n    # Testa cria√ß√£o de extrato banc√°rio\n    statement = BankStatement(\n        id=str(uuid.uuid4()),\n        bank_name=\"Banco Teste\",\n        account_number=\"12345-6\",\n        period_start=datetime.now(),\n        period_end=datetime.now(),\n        initial_balance=Decimal(\"1000.00\"),\n        final_balance=Decimal(\"1500.00\"),\n        currency=\"EUR\",\n        transactions=[transaction]\n    )\n    \n    assert statement.bank_name == \"Banco Teste\"\n    assert statement.account_number == \"12345-6\"\n    assert statement.currency == \"EUR\"\n    assert len(statement.transactions) == 1\n    \n    # Testa c√°lculo de totais\n    assert statement.total_income == Decimal(\"100.50\")\n    assert statement.total_expenses == Decimal(\"0.00\")\n\ndef test_pdf_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detec√ß√£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest", "mtime": 1756147904.95932, "terms": ["testes", "abrangentes", "para", "sistema", "de", "an", "lise", "de", "extratos", "import", "sys", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "import", "pytest", "import", "uuid", "adiciona", "diret", "rio", "raiz", "ao", "path", "para", "importa", "es", "project_root", "os", "path", "dirname", "os", "path", "dirname", "os", "path", "abspath", "__file__", "sys", "path", "insert", "project_root", "testes", "de", "modelos", "de", "dom", "nio", "def", "test_domain_models", "testa", "cria", "de", "modelos", "de", "dom", "nio", "from", "src", "domain", "models", "import", "transaction", "bankstatement", "analysisresult", "transactiontype", "transactioncategory", "testa", "cria", "de", "transa", "transaction", "transaction", "id", "str", "uuid", "uuid4", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "category", "transactioncategory", "salario", "assert", "transaction", "description", "test", "transaction", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "credit", "assert", "transaction", "category", "transactioncategory", "salario", "assert", "transaction", "is_income", "true", "assert", "transaction", "is_expense", "false", "testa", "cria", "de", "extrato", "banc", "rio", "statement", "bankstatement", "id", "str", "uuid", "uuid4", "bank_name", "banco", "teste", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "currency", "eur", "transactions", "transaction", "assert", "statement", "bank_name", "banco", "teste", "assert", "statement", "account_number", "assert", "statement", "currency", "eur", "assert", "len", "statement", "transactions", "testa", "lculo", "de", "totais", "assert", "statement", "total_income", "decimal", "assert", "statement", "total_expenses", "decimal", "def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest"]}
{"chunk_id": "9a20eaaccb7ee734d7c3bbf5", "file_path": "tests/test_comprehensive_suite.py", "start_line": 16, "end_line": 95, "content": "def test_domain_models():\n    \"\"\"Testa a cria√ß√£o de modelos de dom√≠nio.\"\"\"\n    from src.domain.models import Transaction, BankStatement, AnalysisResult, TransactionType, TransactionCategory\n    \n    # Testa cria√ß√£o de transa√ß√£o\n    transaction = Transaction(\n        id=str(uuid.uuid4()),\n        date=datetime.now(),\n        description=\"Test transaction\",\n        amount=Decimal(\"100.50\"),\n        type=TransactionType.CREDIT,\n        category=TransactionCategory.SALARIO\n    )\n    \n    assert transaction.description == \"Test transaction\"\n    assert transaction.amount == Decimal(\"100.50\")\n    assert transaction.type == TransactionType.CREDIT\n    assert transaction.category == TransactionCategory.SALARIO\n    assert transaction.is_income == True\n    assert transaction.is_expense == False\n    \n    # Testa cria√ß√£o de extrato banc√°rio\n    statement = BankStatement(\n        id=str(uuid.uuid4()),\n        bank_name=\"Banco Teste\",\n        account_number=\"12345-6\",\n        period_start=datetime.now(),\n        period_end=datetime.now(),\n        initial_balance=Decimal(\"1000.00\"),\n        final_balance=Decimal(\"1500.00\"),\n        currency=\"EUR\",\n        transactions=[transaction]\n    )\n    \n    assert statement.bank_name == \"Banco Teste\"\n    assert statement.account_number == \"12345-6\"\n    assert statement.currency == \"EUR\"\n    assert len(statement.transactions) == 1\n    \n    # Testa c√°lculo de totais\n    assert statement.total_income == Decimal(\"100.50\")\n    assert statement.total_expenses == Decimal(\"0.00\")\n\ndef test_pdf_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detec√ß√£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabe√ßalhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:", "mtime": 1756147904.95932, "terms": ["def", "test_domain_models", "testa", "cria", "de", "modelos", "de", "dom", "nio", "from", "src", "domain", "models", "import", "transaction", "bankstatement", "analysisresult", "transactiontype", "transactioncategory", "testa", "cria", "de", "transa", "transaction", "transaction", "id", "str", "uuid", "uuid4", "date", "datetime", "now", "description", "test", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "category", "transactioncategory", "salario", "assert", "transaction", "description", "test", "transaction", "assert", "transaction", "amount", "decimal", "assert", "transaction", "type", "transactiontype", "credit", "assert", "transaction", "category", "transactioncategory", "salario", "assert", "transaction", "is_income", "true", "assert", "transaction", "is_expense", "false", "testa", "cria", "de", "extrato", "banc", "rio", "statement", "bankstatement", "id", "str", "uuid", "uuid4", "bank_name", "banco", "teste", "account_number", "period_start", "datetime", "now", "period_end", "datetime", "now", "initial_balance", "decimal", "final_balance", "decimal", "currency", "eur", "transactions", "transaction", "assert", "statement", "bank_name", "banco", "teste", "assert", "statement", "account_number", "assert", "statement", "currency", "eur", "assert", "len", "statement", "transactions", "testa", "lculo", "de", "totais", "assert", "statement", "total_income", "decimal", "assert", "statement", "total_expenses", "decimal", "def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror"]}
{"chunk_id": "f9c27f3c9de02eb7f345e3c5", "file_path": "tests/test_comprehensive_suite.py", "start_line": 59, "end_line": 138, "content": "def test_pdf_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de PDF.\"\"\"\n    try:\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        reader = PDFStatementReader()\n        assert reader is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar PDFStatementReader\")\n\ndef test_excel_reader_currency_detection():\n    \"\"\"Testa a detec√ß√£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabe√ßalhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padr√£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(", "mtime": 1756147904.95932, "terms": ["def", "test_pdf_reader_import", "testa", "importa", "do", "leitor", "de", "pdf", "try", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "reader", "pdfstatementreader", "assert", "reader", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "pdfstatementreader", "def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction"]}
{"chunk_id": "0ef1c0606a6c3e333fac91d1", "file_path": "tests/test_comprehensive_suite.py", "start_line": 68, "end_line": 147, "content": "def test_excel_reader_currency_detection():\n    \"\"\"Testa a detec√ß√£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabe√ßalhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padr√£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        ", "mtime": 1756147904.95932, "terms": ["def", "test_excel_reader_currency_detection", "testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement"]}
{"chunk_id": "ddaa0d350016ec99bf8d9f5b", "file_path": "tests/test_comprehensive_suite.py", "start_line": 69, "end_line": 148, "content": "    \"\"\"Testa a detec√ß√£o de moeda no leitor de Excel.\"\"\"\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        reader = ExcelStatementReader()\n        assert hasattr(reader, 'currency')\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n\nimport pandas as pd\nfrom pathlib import Path\nimport pytest\n\ndef test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabe√ßalhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padr√£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada", "mtime": 1756147904.95932, "terms": ["testa", "detec", "de", "moeda", "no", "leitor", "de", "excel", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "reader", "excelstatementreader", "assert", "hasattr", "reader", "currency", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "import", "pytest", "def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada"]}
{"chunk_id": "3aa60c4837681b9f1d74a237", "file_path": "tests/test_comprehensive_suite.py", "start_line": 82, "end_line": 161, "content": "def test_excel_reader_defaults_to_euro_if_no_currency(tmp_path):\n    # Cria um DataFrame com os cabe√ßalhos esperados pelo leitor Excel\n    data = {\n        'Data Mov.': ['01/01/2023', '02/01/2023'],\n        'Descri√ß√£o': ['Sal√°rio', 'Supermercado'],\n        'Valor': [1000, -50]\n    }\n    df = pd.DataFrame(data)\n    excel_file = tmp_path / \"test_no_currency.xlsx\"\n    df.to_excel(excel_file, index=False)\n\n    try:\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar ExcelStatementReader\")\n\n    reader = ExcelStatementReader()\n    statement = reader.read(excel_file)\n\n    # A moeda deve ser EUR por padr√£o\n    assert reader.currency == 'EUR'\n\ndef test_csv_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")", "mtime": 1756147904.95932, "terms": ["def", "test_excel_reader_defaults_to_euro_if_no_currency", "tmp_path", "cria", "um", "dataframe", "com", "os", "cabe", "alhos", "esperados", "pelo", "leitor", "excel", "data", "data", "mov", "descri", "sal", "rio", "supermercado", "valor", "df", "pd", "dataframe", "data", "excel_file", "tmp_path", "test_no_currency", "xlsx", "df", "to_excel", "excel_file", "index", "false", "try", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "excelstatementreader", "reader", "excelstatementreader", "statement", "reader", "read", "excel_file", "moeda", "deve", "ser", "eur", "por", "padr", "assert", "reader", "currency", "eur", "def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator"]}
{"chunk_id": "0c8adb05cf0b75604f1398ae", "file_path": "tests/test_comprehensive_suite.py", "start_line": 104, "end_line": 180, "content": "def test_csv_reader_import():\n    \"\"\"Testa a importa√ß√£o do leitor de CSV.\"\"\"\n    try:\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        reader = CSVStatementReader()\n        assert reader is not None\n        # Testa se o leitor pode ler arquivos CSV\n        from pathlib import Path\n        assert reader.can_read(Path(\"test.csv\")) == True\n        assert reader.can_read(Path(\"test.xlsx\")) == False\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar CSVStatementReader\")\n\ndef test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_csv_reader_import", "testa", "importa", "do", "leitor", "de", "csv", "try", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "reader", "csvstatementreader", "assert", "reader", "is", "not", "none", "testa", "se", "leitor", "pode", "ler", "arquivos", "csv", "from", "pathlib", "import", "path", "assert", "reader", "can_read", "path", "test", "csv", "true", "assert", "reader", "can_read", "path", "test", "xlsx", "false", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "csvstatementreader", "def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "130700e6b10ecdca30745e43", "file_path": "tests/test_comprehensive_suite.py", "start_line": 117, "end_line": 180, "content": "def test_categorizer_import():\n    \"\"\"Testa a importa√ß√£o do categorizador.\"\"\"\n    try:\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        categorizer = KeywordCategorizer()\n        assert categorizer is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar KeywordCategorizer\")\n\ndef test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_categorizer_import", "testa", "importa", "do", "categorizador", "try", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "categorizer", "keywordcategorizer", "assert", "categorizer", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "keywordcategorizer", "def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "cfc28fb494c7a57a87902b80", "file_path": "tests/test_comprehensive_suite.py", "start_line": 126, "end_line": 180, "content": "def test_analyzer_currency_handling():\n    \"\"\"Testa o tratamento de moeda no analisador.\"\"\"\n    try:\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.domain.models import BankStatement, Transaction, TransactionType, AnalysisResult\n        \n        analyzer = BasicStatementAnalyzer()\n        \n        # Cria um extrato de exemplo com moeda\n        statement = BankStatement(\n            currency=\"USD\",\n            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_analyzer_currency_handling", "testa", "tratamento", "de", "moeda", "no", "analisador", "try", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "analysisresult", "analyzer", "basicstatementanalyzer", "cria", "um", "extrato", "de", "exemplo", "com", "moeda", "statement", "bankstatement", "currency", "usd", "transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "b0aae422c1ab3aad501a6533", "file_path": "tests/test_comprehensive_suite.py", "start_line": 137, "end_line": 180, "content": "            transactions=[\n                Transaction(\n                    amount=Decimal(\"100.00\"),\n                    type=TransactionType.CREDIT\n                )\n            ]\n        )\n        \n        # Analisa o extrato\n        result = analyzer.analyze(statement)\n        \n        # Verifica se a moeda foi preservada\n        assert result.currency == \"USD\"\n        \n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar BasicStatementAnalyzer\")\n\ndef test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["transactions", "transaction", "amount", "decimal", "type", "transactiontype", "credit", "analisa", "extrato", "result", "analyzer", "analyze", "statement", "verifica", "se", "moeda", "foi", "preservada", "assert", "result", "currency", "usd", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "basicstatementanalyzer", "def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "82dec4995d132c1275175fcc", "file_path": "tests/test_comprehensive_suite.py", "start_line": 154, "end_line": 180, "content": "def test_report_generator_import():\n    \"\"\"Testa a importa√ß√£o do gerador de relat√≥rios.\"\"\"\n    try:\n        from src.infrastructure.reports.text_report import TextReportGenerator\n        generator = TextReportGenerator()\n        assert generator is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar TextReportGenerator\")\n\ndef test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_report_generator_import", "testa", "importa", "do", "gerador", "de", "relat", "rios", "try", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "generator", "textreportgenerator", "assert", "generator", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "textreportgenerator", "def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "4123e1acebdb3dc2cd1bd4f8", "file_path": "tests/test_comprehensive_suite.py", "start_line": 163, "end_line": 180, "content": "def test_use_case_import():\n    \"\"\"Testa a importa√ß√£o do caso de uso.\"\"\"\n    try:\n        from src.application.use_cases import AnalyzeStatementUseCase\n        assert AnalyzeStatementUseCase is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar AnalyzeStatementUseCase\")\n\ndef test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_use_case_import", "testa", "importa", "do", "caso", "de", "uso", "try", "from", "src", "application", "use_cases", "import", "analyzestatementusecase", "assert", "analyzestatementusecase", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "analyzestatementusecase", "def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "52f48a24027019e0b6494a5c", "file_path": "tests/test_comprehensive_suite.py", "start_line": 171, "end_line": 180, "content": "def test_main_import():\n    \"\"\"Testa a importa√ß√£o do m√≥dulo principal.\"\"\"\n    try:\n        import main\n        assert main is not None\n    except ImportError:\n        pytest.fail(\"N√£o foi poss√≠vel importar main\")\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])", "mtime": 1756147904.95932, "terms": ["def", "test_main_import", "testa", "importa", "do", "dulo", "principal", "try", "import", "main", "assert", "main", "is", "not", "none", "except", "importerror", "pytest", "fail", "foi", "poss", "vel", "importar", "main", "if", "__name__", "__main__", "pytest", "main", "__file__"]}
{"chunk_id": "95bb5318c9cc3fc9387dc673", "file_path": "scripts/examine_excel.py", "start_line": 1, "end_line": 42, "content": "#!/usr/bin/env python3\n\"\"\"\nScript to examine the structure of the sample Excel file.\n\"\"\"\nimport pandas as pd\nfrom pathlib import Path\n\ndef examine_excel_file(file_path):\n    \"\"\"Examine the structure of an Excel file.\"\"\"\n    print(f\"Examining file: {file_path}\")\n    \n    # Read all sheets\n    excel_file = pd.ExcelFile(file_path)\n    print(f\"Sheet names: {excel_file.sheet_names}\")\n    \n    # Read each sheet\n    for sheet_name in excel_file.sheet_names:\n        print(f\"\\n--- Sheet: {sheet_name} ---\")\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        print(f\"Shape: {df.shape}\")\n        print(\"Columns:\")\n        for i, col in enumerate(df.columns):\n            print(f\"  {i+1}. {col}\")\n        print(\"\\nFirst 20 rows:\")\n        print(df.head(20))\n        print(\"\\nData types:\")\n        print(df.dtypes)\n        \n        # Check for non-empty rows\n        print(\"\\nNon-empty rows:\")\n        non_empty = df.dropna(how='all')\n        print(f\"Total non-empty rows: {len(non_empty)}\")\n        print(\"First 10 non-empty rows:\")\n        print(non_empty.head(10))\n\nif __name__ == \"__main__\":\n    # Path to the sample Excel file\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    if excel_path.exists():\n        examine_excel_file(excel_path)\n    else:\n        print(f\"File not found: {excel_path}\")", "mtime": 1755769048.2636738, "terms": ["usr", "bin", "env", "python3", "script", "to", "examine", "the", "structure", "of", "the", "sample", "excel", "file", "import", "pandas", "as", "pd", "from", "pathlib", "import", "path", "def", "examine_excel_file", "file_path", "examine", "the", "structure", "of", "an", "excel", "file", "print", "examining", "file", "file_path", "read", "all", "sheets", "excel_file", "pd", "excelfile", "file_path", "print", "sheet", "names", "excel_file", "sheet_names", "read", "each", "sheet", "for", "sheet_name", "in", "excel_file", "sheet_names", "print", "sheet", "sheet_name", "df", "pd", "read_excel", "file_path", "sheet_name", "sheet_name", "print", "shape", "df", "shape", "print", "columns", "for", "col", "in", "enumerate", "df", "columns", "print", "col", "print", "nfirst", "rows", "print", "df", "head", "print", "ndata", "types", "print", "df", "dtypes", "check", "for", "non", "empty", "rows", "print", "nnon", "empty", "rows", "non_empty", "df", "dropna", "how", "all", "print", "total", "non", "empty", "rows", "len", "non_empty", "print", "first", "non", "empty", "rows", "print", "non_empty", "head", "if", "__name__", "__main__", "path", "to", "the", "sample", "excel", "file", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "excel_path", "exists", "examine_excel_file", "excel_path", "else", "print", "file", "not", "found", "excel_path"]}
{"chunk_id": "f44126c63e8c62c51ffe866e", "file_path": "scripts/examine_excel.py", "start_line": 8, "end_line": 42, "content": "def examine_excel_file(file_path):\n    \"\"\"Examine the structure of an Excel file.\"\"\"\n    print(f\"Examining file: {file_path}\")\n    \n    # Read all sheets\n    excel_file = pd.ExcelFile(file_path)\n    print(f\"Sheet names: {excel_file.sheet_names}\")\n    \n    # Read each sheet\n    for sheet_name in excel_file.sheet_names:\n        print(f\"\\n--- Sheet: {sheet_name} ---\")\n        df = pd.read_excel(file_path, sheet_name=sheet_name)\n        print(f\"Shape: {df.shape}\")\n        print(\"Columns:\")\n        for i, col in enumerate(df.columns):\n            print(f\"  {i+1}. {col}\")\n        print(\"\\nFirst 20 rows:\")\n        print(df.head(20))\n        print(\"\\nData types:\")\n        print(df.dtypes)\n        \n        # Check for non-empty rows\n        print(\"\\nNon-empty rows:\")\n        non_empty = df.dropna(how='all')\n        print(f\"Total non-empty rows: {len(non_empty)}\")\n        print(\"First 10 non-empty rows:\")\n        print(non_empty.head(10))\n\nif __name__ == \"__main__\":\n    # Path to the sample Excel file\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    if excel_path.exists():\n        examine_excel_file(excel_path)\n    else:\n        print(f\"File not found: {excel_path}\")", "mtime": 1755769048.2636738, "terms": ["def", "examine_excel_file", "file_path", "examine", "the", "structure", "of", "an", "excel", "file", "print", "examining", "file", "file_path", "read", "all", "sheets", "excel_file", "pd", "excelfile", "file_path", "print", "sheet", "names", "excel_file", "sheet_names", "read", "each", "sheet", "for", "sheet_name", "in", "excel_file", "sheet_names", "print", "sheet", "sheet_name", "df", "pd", "read_excel", "file_path", "sheet_name", "sheet_name", "print", "shape", "df", "shape", "print", "columns", "for", "col", "in", "enumerate", "df", "columns", "print", "col", "print", "nfirst", "rows", "print", "df", "head", "print", "ndata", "types", "print", "df", "dtypes", "check", "for", "non", "empty", "rows", "print", "nnon", "empty", "rows", "non_empty", "df", "dropna", "how", "all", "print", "total", "non", "empty", "rows", "len", "non_empty", "print", "first", "non", "empty", "rows", "print", "non_empty", "head", "if", "__name__", "__main__", "path", "to", "the", "sample", "excel", "file", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "excel_path", "exists", "examine_excel_file", "excel_path", "else", "print", "file", "not", "found", "excel_path"]}
{"chunk_id": "1eb22b284c906636e2bc435d", "file_path": "scripts/test_excel_reader.py", "start_line": 1, "end_line": 62, "content": "#!/usr/bin/env python3\n\"\"\"\nScript de teste para o leitor de Excel.\n\"\"\"\nimport sys\nfrom pathlib import Path\n\n# Adiciona o diret√≥rio raiz ao path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom src.infrastructure.readers.excel_reader import ExcelStatementReader\nfrom src.domain.models import TransactionType\n\ndef test_excel_reader():\n    \"\"\"Testa o leitor de Excel com o arquivo de exemplo.\"\"\"\n    reader = ExcelStatementReader()\n    \n    # Caminho para o arquivo de exemplo\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    \n    if not excel_path.exists():\n        print(f\"Arquivo n√£o encontrado: {excel_path}\")\n        return\n    \n    print(f\"Testando leitor de Excel com: {excel_path}\")\n    \n    # Verifica se pode ler o arquivo\n    if not reader.can_read(excel_path):\n        print(\"O leitor n√£o reconhece este arquivo como Excel\")\n        return\n    \n    try:\n        # L√™ o extrato\n        statement = reader.read(excel_path)\n        \n        print(f\"\\nExtrato lido com sucesso!\")\n        print(f\"Banco: {statement.bank_name}\")\n        print(f\"Conta: {statement.account_number}\")\n        print(f\"Per√≠odo: {statement.period_start} a {statement.period_end}\")\n        print(f\"Saldo inicial: ‚Ç¨ {statement.initial_balance}\")\n        print(f\"Saldo final: ‚Ç¨ {statement.final_balance}\")\n        print(f\"Total de transa√ß√µes: {len(statement.transactions)}\")\n        \n        print(f\"\\nPrimeiras 5 transa√ß√µes:\")\n        for i, transaction in enumerate(statement.transactions[:5]):\n            print(f\"  {i+1}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}‚Ç¨ {transaction.amount}\")\n        \n        print(f\"\\n√öltimas 5 transa√ß√µes:\")\n        for i, transaction in enumerate(statement.transactions[-5:], len(statement.transactions)-4):\n            print(f\"  {i}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}‚Ç¨ {transaction.amount}\")\n                  \n    except Exception as e:\n        print(f\"Erro ao ler o extrato: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    test_excel_reader()", "mtime": 1755769411.9854424, "terms": ["usr", "bin", "env", "python3", "script", "de", "teste", "para", "leitor", "de", "excel", "import", "sys", "from", "pathlib", "import", "path", "adiciona", "diret", "rio", "raiz", "ao", "path", "sys", "path", "insert", "str", "path", "__file__", "parent", "parent", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "domain", "models", "import", "transactiontype", "def", "test_excel_reader", "testa", "leitor", "de", "excel", "com", "arquivo", "de", "exemplo", "reader", "excelstatementreader", "caminho", "para", "arquivo", "de", "exemplo", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "not", "excel_path", "exists", "print", "arquivo", "encontrado", "excel_path", "return", "print", "testando", "leitor", "de", "excel", "com", "excel_path", "verifica", "se", "pode", "ler", "arquivo", "if", "not", "reader", "can_read", "excel_path", "print", "leitor", "reconhece", "este", "arquivo", "como", "excel", "return", "try", "extrato", "statement", "reader", "read", "excel_path", "print", "nextrato", "lido", "com", "sucesso", "print", "banco", "statement", "bank_name", "print", "conta", "statement", "account_number", "print", "per", "odo", "statement", "period_start", "statement", "period_end", "print", "saldo", "inicial", "statement", "initial_balance", "print", "saldo", "final", "statement", "final_balance", "print", "total", "de", "transa", "es", "len", "statement", "transactions", "print", "nprimeiras", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "print", "ltimas", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "len", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "except", "exception", "as", "print", "erro", "ao", "ler", "extrato", "import", "traceback", "traceback", "print_exc", "if", "__name__", "__main__", "test_excel_reader"]}
{"chunk_id": "bba78b772bbd8feb95847921", "file_path": "scripts/test_excel_reader.py", "start_line": 14, "end_line": 62, "content": "def test_excel_reader():\n    \"\"\"Testa o leitor de Excel com o arquivo de exemplo.\"\"\"\n    reader = ExcelStatementReader()\n    \n    # Caminho para o arquivo de exemplo\n    excel_path = Path(\"data/samples/extmovs_bpi2108102947.xlsx\")\n    \n    if not excel_path.exists():\n        print(f\"Arquivo n√£o encontrado: {excel_path}\")\n        return\n    \n    print(f\"Testando leitor de Excel com: {excel_path}\")\n    \n    # Verifica se pode ler o arquivo\n    if not reader.can_read(excel_path):\n        print(\"O leitor n√£o reconhece este arquivo como Excel\")\n        return\n    \n    try:\n        # L√™ o extrato\n        statement = reader.read(excel_path)\n        \n        print(f\"\\nExtrato lido com sucesso!\")\n        print(f\"Banco: {statement.bank_name}\")\n        print(f\"Conta: {statement.account_number}\")\n        print(f\"Per√≠odo: {statement.period_start} a {statement.period_end}\")\n        print(f\"Saldo inicial: ‚Ç¨ {statement.initial_balance}\")\n        print(f\"Saldo final: ‚Ç¨ {statement.final_balance}\")\n        print(f\"Total de transa√ß√µes: {len(statement.transactions)}\")\n        \n        print(f\"\\nPrimeiras 5 transa√ß√µes:\")\n        for i, transaction in enumerate(statement.transactions[:5]):\n            print(f\"  {i+1}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}‚Ç¨ {transaction.amount}\")\n        \n        print(f\"\\n√öltimas 5 transa√ß√µes:\")\n        for i, transaction in enumerate(statement.transactions[-5:], len(statement.transactions)-4):\n            print(f\"  {i}. {transaction.date.strftime('%d/%m/%Y')} - \"\n                  f\"{transaction.description} - \"\n                  f\"{'+' if transaction.type == TransactionType.CREDIT else '-'}‚Ç¨ {transaction.amount}\")\n                  \n    except Exception as e:\n        print(f\"Erro ao ler o extrato: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    test_excel_reader()", "mtime": 1755769411.9854424, "terms": ["def", "test_excel_reader", "testa", "leitor", "de", "excel", "com", "arquivo", "de", "exemplo", "reader", "excelstatementreader", "caminho", "para", "arquivo", "de", "exemplo", "excel_path", "path", "data", "samples", "extmovs_bpi2108102947", "xlsx", "if", "not", "excel_path", "exists", "print", "arquivo", "encontrado", "excel_path", "return", "print", "testando", "leitor", "de", "excel", "com", "excel_path", "verifica", "se", "pode", "ler", "arquivo", "if", "not", "reader", "can_read", "excel_path", "print", "leitor", "reconhece", "este", "arquivo", "como", "excel", "return", "try", "extrato", "statement", "reader", "read", "excel_path", "print", "nextrato", "lido", "com", "sucesso", "print", "banco", "statement", "bank_name", "print", "conta", "statement", "account_number", "print", "per", "odo", "statement", "period_start", "statement", "period_end", "print", "saldo", "inicial", "statement", "initial_balance", "print", "saldo", "final", "statement", "final_balance", "print", "total", "de", "transa", "es", "len", "statement", "transactions", "print", "nprimeiras", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "print", "ltimas", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "len", "statement", "transactions", "print", "transaction", "date", "strftime", "transaction", "description", "if", "transaction", "type", "transactiontype", "credit", "else", "transaction", "amount", "except", "exception", "as", "print", "erro", "ao", "ler", "extrato", "import", "traceback", "traceback", "print_exc", "if", "__name__", "__main__", "test_excel_reader"]}
{"chunk_id": "d177fd23d02a98ea16659b99", "file_path": "scripts/create_sample_pdf.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para criar um PDF de extrato banc√°rio de exemplo para testes.\n\"\"\"\n\nfrom reportlab.lib.pagesizes import A4\nfrom reportlab.lib import colors\nfrom reportlab.lib.units import mm\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.enums import TA_CENTER, TA_RIGHT\nimport os\nfrom datetime import datetime, timedelta\n\ndef create_sample_statement():\n    \"\"\"Cria um extrato banc√°rio de exemplo em PDF.\"\"\"\n    \n    # Criar diret√≥rio se n√£o existir\n    os.makedirs(\"data/samples\", exist_ok=True)\n    \n    # Nome do arquivo\n    filename = \"data/samples/extrato_exemplo.pdf\"\n    \n    # Criar documento\n    doc = SimpleDocTemplate(filename, pagesize=A4)\n    story = []\n    \n    # Estilos\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(\n        'CustomTitle',\n        parent=styles['Heading1'],\n        fontSize=16,\n        textColor=colors.HexColor('#003366'),\n        alignment=TA_CENTER\n    )\n    \n    # Cabe√ßalho\n    story.append(Paragraph(\"BANCO EXEMPLO S.A.\", title_style))\n    story.append(Spacer(1, 12))\n    story.append(Paragraph(\"Extrato de Conta Corrente\", styles['Heading2']))\n    story.append(Spacer(1, 12))\n    \n    # Informa√ß√µes da conta\n    info_data = [\n        [\"Ag√™ncia:\", \"1234\", \"Conta:\", \"56789-0\"],\n        [\"Cliente:\", \"Jo√£o da Silva\", \"CPF:\", \"123.456.789-00\"],\n        [\"Per√≠odo:\", \"01/08/2025 a 31/08/2025\", \"\", \"\"]\n    ]\n    \n    info_table = Table(info_data, colWidths=[60, 120, 60, 120])\n    info_table.setStyle(TableStyle([\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),\n    ]))\n    \n    story.append(info_table)\n    story.append(Spacer(1, 20))\n    \n    # Saldo anterior\n    story.append(Paragraph(\"Saldo Anterior (31/07/2025): ‚Ç¨ 2.500,00\", styles['Normal']))\n    story.append(Spacer(1, 12))\n\n    # Transa√ß√µes\n    transactions = [\n        [\"Data\", \"Descri√ß√£o\", \"Valor\", \"Saldo\"],\n        [\"01/08\", \"SALARIO EMPRESA XYZ\", \"‚Ç¨ 5.000,00 C\", \"‚Ç¨ 7.500,00\"],\n        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"‚Ç¨ 150,00 D\", \"‚Ç¨ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"‚Ç¨ 287,45 D\", \"‚Ç¨ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"‚Ç¨ 89,90 D\", \"‚Ç¨ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"‚Ç¨ 23,50 D\", \"‚Ç¨ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"‚Ç¨ 156,00 D\", \"‚Ç¨ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"‚Ç¨ 234,78 D\", \"‚Ç¨ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"‚Ç¨ 39,90 D\", \"‚Ç¨ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"‚Ç¨ 500,00 C\", \"‚Ç¨ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"‚Ç¨ 200,00 D\", \"‚Ç¨ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"‚Ç¨ 120,00 D\", \"‚Ç¨ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"‚Ç¨ 80,00 C\", \"‚Ç¨ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"‚Ç¨ 450,00 D\", \"‚Ç¨ 6.328,47\"],", "mtime": 1755109561.8157306, "terms": ["usr", "bin", "env", "python3", "script", "para", "criar", "um", "pdf", "de", "extrato", "banc", "rio", "de", "exemplo", "para", "testes", "from", "reportlab", "lib", "pagesizes", "import", "a4", "from", "reportlab", "lib", "import", "colors", "from", "reportlab", "lib", "units", "import", "mm", "from", "reportlab", "platypus", "import", "simpledoctemplate", "table", "tablestyle", "paragraph", "spacer", "from", "reportlab", "lib", "styles", "import", "getsamplestylesheet", "paragraphstyle", "from", "reportlab", "lib", "enums", "import", "ta_center", "ta_right", "import", "os", "from", "datetime", "import", "datetime", "timedelta", "def", "create_sample_statement", "cria", "um", "extrato", "banc", "rio", "de", "exemplo", "em", "pdf", "criar", "diret", "rio", "se", "existir", "os", "makedirs", "data", "samples", "exist_ok", "true", "nome", "do", "arquivo", "filename", "data", "samples", "extrato_exemplo", "pdf", "criar", "documento", "doc", "simpledoctemplate", "filename", "pagesize", "a4", "story", "estilos", "styles", "getsamplestylesheet", "title_style", "paragraphstyle", "customtitle", "parent", "styles", "heading1", "fontsize", "textcolor", "colors", "hexcolor", "alignment", "ta_center", "cabe", "alho", "story", "append", "paragraph", "banco", "exemplo", "title_style", "story", "append", "spacer", "story", "append", "paragraph", "extrato", "de", "conta", "corrente", "styles", "heading2", "story", "append", "spacer", "informa", "es", "da", "conta", "info_data", "ag", "ncia", "conta", "cliente", "jo", "da", "silva", "cpf", "per", "odo", "info_table", "table", "info_data", "colwidths", "info_table", "setstyle", "tablestyle", "fontsize", "fontname", "helvetica", "bold", "fontname", "helvetica", "bold", "story", "append", "info_table", "story", "append", "spacer", "saldo", "anterior", "story", "append", "paragraph", "saldo", "anterior", "styles", "normal", "story", "append", "spacer", "transa", "es", "transactions", "data", "descri", "valor", "saldo", "salario", "empresa", "xyz", "pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center"]}
{"chunk_id": "76922712166a514f264e02fe", "file_path": "scripts/create_sample_pdf.py", "start_line": 15, "end_line": 94, "content": "def create_sample_statement():\n    \"\"\"Cria um extrato banc√°rio de exemplo em PDF.\"\"\"\n    \n    # Criar diret√≥rio se n√£o existir\n    os.makedirs(\"data/samples\", exist_ok=True)\n    \n    # Nome do arquivo\n    filename = \"data/samples/extrato_exemplo.pdf\"\n    \n    # Criar documento\n    doc = SimpleDocTemplate(filename, pagesize=A4)\n    story = []\n    \n    # Estilos\n    styles = getSampleStyleSheet()\n    title_style = ParagraphStyle(\n        'CustomTitle',\n        parent=styles['Heading1'],\n        fontSize=16,\n        textColor=colors.HexColor('#003366'),\n        alignment=TA_CENTER\n    )\n    \n    # Cabe√ßalho\n    story.append(Paragraph(\"BANCO EXEMPLO S.A.\", title_style))\n    story.append(Spacer(1, 12))\n    story.append(Paragraph(\"Extrato de Conta Corrente\", styles['Heading2']))\n    story.append(Spacer(1, 12))\n    \n    # Informa√ß√µes da conta\n    info_data = [\n        [\"Ag√™ncia:\", \"1234\", \"Conta:\", \"56789-0\"],\n        [\"Cliente:\", \"Jo√£o da Silva\", \"CPF:\", \"123.456.789-00\"],\n        [\"Per√≠odo:\", \"01/08/2025 a 31/08/2025\", \"\", \"\"]\n    ]\n    \n    info_table = Table(info_data, colWidths=[60, 120, 60, 120])\n    info_table.setStyle(TableStyle([\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('FONTNAME', (2, 0), (2, -1), 'Helvetica-Bold'),\n    ]))\n    \n    story.append(info_table)\n    story.append(Spacer(1, 20))\n    \n    # Saldo anterior\n    story.append(Paragraph(\"Saldo Anterior (31/07/2025): ‚Ç¨ 2.500,00\", styles['Normal']))\n    story.append(Spacer(1, 12))\n\n    # Transa√ß√µes\n    transactions = [\n        [\"Data\", \"Descri√ß√£o\", \"Valor\", \"Saldo\"],\n        [\"01/08\", \"SALARIO EMPRESA XYZ\", \"‚Ç¨ 5.000,00 C\", \"‚Ç¨ 7.500,00\"],\n        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"‚Ç¨ 150,00 D\", \"‚Ç¨ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"‚Ç¨ 287,45 D\", \"‚Ç¨ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"‚Ç¨ 89,90 D\", \"‚Ç¨ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"‚Ç¨ 23,50 D\", \"‚Ç¨ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"‚Ç¨ 156,00 D\", \"‚Ç¨ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"‚Ç¨ 234,78 D\", \"‚Ç¨ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"‚Ç¨ 39,90 D\", \"‚Ç¨ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"‚Ç¨ 500,00 C\", \"‚Ç¨ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"‚Ç¨ 200,00 D\", \"‚Ç¨ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"‚Ç¨ 120,00 D\", \"‚Ç¨ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"‚Ç¨ 80,00 C\", \"‚Ç¨ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"‚Ç¨ 450,00 D\", \"‚Ç¨ 6.328,47\"],\n        [\"28/08\", \"IFOOD\", \"‚Ç¨ 67,80 D\", \"‚Ç¨ 6.260,67\"],\n        [\"30/08\", \"RENDIMENTO POUPANCA\", \"‚Ç¨ 15,43 C\", \"‚Ç¨ 6.276,10\"],\n    ]\n\n    # Criar tabela de transa√ß√µes\n    trans_table = Table(transactions, colWidths=[50, 200, 80, 80])\n    trans_table.setStyle(TableStyle([\n        # Cabe√ßalho\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 10),\n        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n", "mtime": 1755109561.8157306, "terms": ["def", "create_sample_statement", "cria", "um", "extrato", "banc", "rio", "de", "exemplo", "em", "pdf", "criar", "diret", "rio", "se", "existir", "os", "makedirs", "data", "samples", "exist_ok", "true", "nome", "do", "arquivo", "filename", "data", "samples", "extrato_exemplo", "pdf", "criar", "documento", "doc", "simpledoctemplate", "filename", "pagesize", "a4", "story", "estilos", "styles", "getsamplestylesheet", "title_style", "paragraphstyle", "customtitle", "parent", "styles", "heading1", "fontsize", "textcolor", "colors", "hexcolor", "alignment", "ta_center", "cabe", "alho", "story", "append", "paragraph", "banco", "exemplo", "title_style", "story", "append", "spacer", "story", "append", "paragraph", "extrato", "de", "conta", "corrente", "styles", "heading2", "story", "append", "spacer", "informa", "es", "da", "conta", "info_data", "ag", "ncia", "conta", "cliente", "jo", "da", "silva", "cpf", "per", "odo", "info_table", "table", "info_data", "colwidths", "info_table", "setstyle", "tablestyle", "fontsize", "fontname", "helvetica", "bold", "fontname", "helvetica", "bold", "story", "append", "info_table", "story", "append", "spacer", "saldo", "anterior", "story", "append", "paragraph", "saldo", "anterior", "styles", "normal", "story", "append", "spacer", "transa", "es", "transactions", "data", "descri", "valor", "saldo", "salario", "empresa", "xyz", "pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center", "ifood", "rendimento", "poupanca", "criar", "tabela", "de", "transa", "es", "trans_table", "table", "transactions", "colwidths", "trans_table", "setstyle", "tablestyle", "cabe", "alho", "background", "colors", "grey", "textcolor", "colors", "whitesmoke", "fontname", "helvetica", "bold", "fontsize", "align", "center"]}
{"chunk_id": "f2000ae5c76f554f290afaa5", "file_path": "scripts/create_sample_pdf.py", "start_line": 69, "end_line": 136, "content": "        [\"02/08\", \"PIX ENVIADO MARIA SILVA\", \"‚Ç¨ 150,00 D\", \"‚Ç¨ 7.350,00\"],\n        [\"03/08\", \"SUPERMERCADO EXTRA\", \"‚Ç¨ 287,45 D\", \"‚Ç¨ 7.062,55\"],\n        [\"05/08\", \"FARMACIA POPULAR\", \"‚Ç¨ 89,90 D\", \"‚Ç¨ 6.972,65\"],\n        [\"07/08\", \"UBER TRIP\", \"‚Ç¨ 23,50 D\", \"‚Ç¨ 6.949,15\"],\n        [\"08/08\", \"RESTAURANTE SABOR\", \"‚Ç¨ 156,00 D\", \"‚Ç¨ 6.793,15\"],\n        [\"10/08\", \"CONTA LUZ\", \"‚Ç¨ 234,78 D\", \"‚Ç¨ 6.558,37\"],\n        [\"12/08\", \"NETFLIX.COM\", \"‚Ç¨ 39,90 D\", \"‚Ç¨ 6.518,47\"],\n        [\"15/08\", \"TRANSFERENCIA RECEBIDA\", \"‚Ç¨ 500,00 C\", \"‚Ç¨ 7.018,47\"],\n        [\"18/08\", \"POSTO SHELL\", \"‚Ç¨ 200,00 D\", \"‚Ç¨ 6.818,47\"],\n        [\"20/08\", \"ACADEMIA FITNESS\", \"‚Ç¨ 120,00 D\", \"‚Ç¨ 6.698,47\"],\n        [\"22/08\", \"PIX RECEBIDO PEDRO\", \"‚Ç¨ 80,00 C\", \"‚Ç¨ 6.778,47\"],\n        [\"25/08\", \"SHOPPING CENTER\", \"‚Ç¨ 450,00 D\", \"‚Ç¨ 6.328,47\"],\n        [\"28/08\", \"IFOOD\", \"‚Ç¨ 67,80 D\", \"‚Ç¨ 6.260,67\"],\n        [\"30/08\", \"RENDIMENTO POUPANCA\", \"‚Ç¨ 15,43 C\", \"‚Ç¨ 6.276,10\"],\n    ]\n\n    # Criar tabela de transa√ß√µes\n    trans_table = Table(transactions, colWidths=[50, 200, 80, 80])\n    trans_table.setStyle(TableStyle([\n        # Cabe√ßalho\n        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n        ('FONTSIZE', (0, 0), (-1, 0), 10),\n        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),\n\n        # Corpo\n        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n        ('FONTSIZE', (0, 1), (-1, -1), 9),\n        ('ALIGN', (0, 1), (0, -1), 'CENTER'),\n        ('ALIGN', (2, 1), (-1, -1), 'RIGHT'),\n\n        # Linhas alternadas\n        ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n\n        # Bordas\n        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n    ]))\n\n    story.append(trans_table)\n    story.append(Spacer(1, 20))\n\n    # Resumo\n    story.append(Paragraph(\"RESUMO DO PER√çODO\", styles['Heading3']))\n    story.append(Spacer(1, 12))\n\n    summary_data = [\n        [\"Total de Cr√©ditos:\", \"‚Ç¨ 5.595,43\"],\n        [\"Total de D√©bitos:\", \"‚Ç¨ 1.819,33\"],\n        [\"Saldo Final:\", \"‚Ç¨ 6.276,10\"]\n    ]\n\n    summary_table = Table(summary_data, colWidths=[150, 100])\n    summary_table.setStyle(TableStyle([\n        ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n        ('ALIGN', (1, 0), (1, -1), 'RIGHT'),\n        ('FONTSIZE', (0, 0), (-1, -1), 10),\n    ]))\n\n    story.append(summary_table)\n\n    # Gerar PDF\n    doc.build(story)\n    print(f\"‚úÖ PDF de exemplo criado: {filename}\")\n    return filename\n\nif __name__ == \"__main__\":\n    create_sample_statement()", "mtime": 1755109561.8157306, "terms": ["pix", "enviado", "maria", "silva", "supermercado", "extra", "farmacia", "popular", "uber", "trip", "restaurante", "sabor", "conta", "luz", "netflix", "com", "transferencia", "recebida", "posto", "shell", "academia", "fitness", "pix", "recebido", "pedro", "shopping", "center", "ifood", "rendimento", "poupanca", "criar", "tabela", "de", "transa", "es", "trans_table", "table", "transactions", "colwidths", "trans_table", "setstyle", "tablestyle", "cabe", "alho", "background", "colors", "grey", "textcolor", "colors", "whitesmoke", "fontname", "helvetica", "bold", "fontsize", "align", "center", "corpo", "fontname", "helvetica", "fontsize", "align", "center", "align", "right", "linhas", "alternadas", "rowbackgrounds", "colors", "white", "colors", "lightgrey", "bordas", "grid", "colors", "black", "story", "append", "trans_table", "story", "append", "spacer", "resumo", "story", "append", "paragraph", "resumo", "do", "per", "odo", "styles", "heading3", "story", "append", "spacer", "summary_data", "total", "de", "cr", "ditos", "total", "de", "bitos", "saldo", "final", "summary_table", "table", "summary_data", "colwidths", "summary_table", "setstyle", "tablestyle", "fontname", "helvetica", "bold", "align", "right", "fontsize", "story", "append", "summary_table", "gerar", "pdf", "doc", "build", "story", "print", "pdf", "de", "exemplo", "criado", "filename", "return", "filename", "if", "__name__", "__main__", "create_sample_statement"]}
{"chunk_id": "0e6c51198d21e942dae55042", "file_path": "src/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote principal src", "mtime": 1755104706.859261, "terms": ["pacote", "principal", "src"]}
{"chunk_id": "85a385165191d692de3ad527", "file_path": "src/utils/currency_utils.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nUtilit√°rios para formata√ß√£o e detec√ß√£o de moedas.\n\"\"\"\nimport re\nfrom typing import Tuple, Optional\n\n\nclass CurrencyUtils:\n    \"\"\"Utilit√°rios para trabalhar com moedas.\"\"\"\n    \n    # Mapeamento de s√≠mbolos de moeda\n    CURRENCY_SYMBOLS = {\n        'EUR': '‚Ç¨',\n        'USD': '$',\n        'BRL': 'R$',\n        'GBP': '¬£',\n        'JPY': '¬•',\n        'CHF': 'CHF',\n        'CAD': 'C$',\n        'AUD': 'A$'\n    }\n    \n    # Padr√µes para detectar moedas em texto\n    CURRENCY_PATTERNS = [\n        (r'‚Ç¨\\s*[\\d.,]+|[\\d.,]+\\s*‚Ç¨', 'EUR'),\n        (r'R\\$\\s*[\\d.,]+|[\\d.,]+\\s*R\\$', 'BRL'),\n        (r'\\$\\s*[\\d.,]+|[\\d.,]+\\s*\\$', 'USD'),\n        (r'¬£\\s*[\\d.,]+|[\\d.,]+\\s*¬£', 'GBP'),\n        (r'¬•\\s*[\\d.,]+|[\\d.,]+\\s*¬•', 'JPY'),\n        (r'CHF\\s*[\\d.,]+|[\\d.,]+\\s*CHF', 'CHF'),\n    ]\n    \n    @classmethod\n    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para an√°lise\n\n        Returns:\n            C√≥digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padr√£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padr√µes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por c√≥digos de moeda expl√≠citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # Padr√£o para Europa (assume EUR se n√£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o s√≠mbolo da moeda.\n        \n        Args:\n            currency_code: C√≥digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            S√≠mbolo da moeda (ex: '‚Ç¨', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        ", "mtime": 1756149301.0570138, "terms": ["utilit", "rios", "para", "formata", "detec", "de", "moedas", "import", "re", "from", "typing", "import", "tuple", "optional", "class", "currencyutils", "utilit", "rios", "para", "trabalhar", "com", "moedas", "mapeamento", "de", "mbolos", "de", "moeda", "currency_symbols", "eur", "usd", "brl", "gbp", "jpy", "chf", "chf", "cad", "aud", "padr", "es", "para", "detectar", "moedas", "em", "texto", "currency_patterns", "eur", "brl", "usd", "gbp", "jpy", "chf", "chf", "chf", "classmethod", "def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada"]}
{"chunk_id": "4960b6392ed42f335309c6c7", "file_path": "src/utils/currency_utils.py", "start_line": 8, "end_line": 87, "content": "class CurrencyUtils:\n    \"\"\"Utilit√°rios para trabalhar com moedas.\"\"\"\n    \n    # Mapeamento de s√≠mbolos de moeda\n    CURRENCY_SYMBOLS = {\n        'EUR': '‚Ç¨',\n        'USD': '$',\n        'BRL': 'R$',\n        'GBP': '¬£',\n        'JPY': '¬•',\n        'CHF': 'CHF',\n        'CAD': 'C$',\n        'AUD': 'A$'\n    }\n    \n    # Padr√µes para detectar moedas em texto\n    CURRENCY_PATTERNS = [\n        (r'‚Ç¨\\s*[\\d.,]+|[\\d.,]+\\s*‚Ç¨', 'EUR'),\n        (r'R\\$\\s*[\\d.,]+|[\\d.,]+\\s*R\\$', 'BRL'),\n        (r'\\$\\s*[\\d.,]+|[\\d.,]+\\s*\\$', 'USD'),\n        (r'¬£\\s*[\\d.,]+|[\\d.,]+\\s*¬£', 'GBP'),\n        (r'¬•\\s*[\\d.,]+|[\\d.,]+\\s*¬•', 'JPY'),\n        (r'CHF\\s*[\\d.,]+|[\\d.,]+\\s*CHF', 'CHF'),\n    ]\n    \n    @classmethod\n    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para an√°lise\n\n        Returns:\n            C√≥digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padr√£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padr√µes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por c√≥digos de moeda expl√≠citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # Padr√£o para Europa (assume EUR se n√£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o s√≠mbolo da moeda.\n        \n        Args:\n            currency_code: C√≥digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            S√≠mbolo da moeda (ex: '‚Ç¨', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: C√≥digo da moeda\n            \n        Returns:\n            Valor formatado com s√≠mbolo da moeda\n        \"\"\"", "mtime": 1756149301.0570138, "terms": ["class", "currencyutils", "utilit", "rios", "para", "trabalhar", "com", "moedas", "mapeamento", "de", "mbolos", "de", "moeda", "currency_symbols", "eur", "usd", "brl", "gbp", "jpy", "chf", "chf", "cad", "aud", "padr", "es", "para", "detectar", "moedas", "em", "texto", "currency_patterns", "eur", "brl", "usd", "gbp", "jpy", "chf", "chf", "chf", "classmethod", "def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda"]}
{"chunk_id": "329b61acf0417825d1d53afb", "file_path": "src/utils/currency_utils.py", "start_line": 34, "end_line": 113, "content": "    def detect_currency_from_text(cls, text: str) -> str:\n        \"\"\"\n        Detecta a moeda a partir de um texto.\n\n        Args:\n            text: Texto para an√°lise\n\n        Returns:\n            C√≥digo da moeda (ex: 'EUR', 'BRL', 'USD') ou 'EUR' como padr√£o\n        \"\"\"\n        if not text:\n            return 'EUR'\n\n        text = str(text).upper()\n\n        # Procura por padr√µes de moeda\n        for pattern, currency in cls.CURRENCY_PATTERNS:\n            if re.search(pattern, text, re.IGNORECASE):\n                return currency\n\n        # Procura por c√≥digos de moeda expl√≠citos (mas apenas como palavras completas)\n        for currency_code in cls.CURRENCY_SYMBOLS.keys():\n            # Usar \\b para delimitar palavras completas\n            if re.search(r'\\b' + re.escape(currency_code) + r'\\b', text):\n                return currency_code\n\n        # Padr√£o para Europa (assume EUR se n√£o encontrar nada)\n        return 'EUR'\n    \n    @classmethod\n    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o s√≠mbolo da moeda.\n        \n        Args:\n            currency_code: C√≥digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            S√≠mbolo da moeda (ex: '‚Ç¨', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: C√≥digo da moeda\n            \n        Returns:\n            Valor formatado com s√≠mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # Formata√ß√£o espec√≠fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: ‚Ç¨ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padr√£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para an√°lise\n\n        Returns:\n            C√≥digo da moeda detectada ou None se n√£o detectada\n        \"\"\"\n        import pandas as pd\n", "mtime": 1756149301.0570138, "terms": ["def", "detect_currency_from_text", "cls", "text", "str", "str", "detecta", "moeda", "partir", "de", "um", "texto", "args", "text", "texto", "para", "an", "lise", "returns", "digo", "da", "moeda", "ex", "eur", "brl", "usd", "ou", "eur", "como", "padr", "if", "not", "text", "return", "eur", "text", "str", "text", "upper", "procura", "por", "padr", "es", "de", "moeda", "for", "pattern", "currency", "in", "cls", "currency_patterns", "if", "re", "search", "pattern", "text", "re", "ignorecase", "return", "currency", "procura", "por", "digos", "de", "moeda", "expl", "citos", "mas", "apenas", "como", "palavras", "completas", "for", "currency_code", "in", "cls", "currency_symbols", "keys", "usar", "para", "delimitar", "palavras", "completas", "if", "re", "search", "re", "escape", "currency_code", "text", "return", "currency_code", "padr", "para", "europa", "assume", "eur", "se", "encontrar", "nada", "return", "eur", "classmethod", "def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd"]}
{"chunk_id": "c311f58de24690a62a8e2ff2", "file_path": "src/utils/currency_utils.py", "start_line": 64, "end_line": 133, "content": "    def get_currency_symbol(cls, currency_code: str) -> str:\n        \"\"\"\n        Retorna o s√≠mbolo da moeda.\n        \n        Args:\n            currency_code: C√≥digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            S√≠mbolo da moeda (ex: '‚Ç¨', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: C√≥digo da moeda\n            \n        Returns:\n            Valor formatado com s√≠mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # Formata√ß√£o espec√≠fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: ‚Ç¨ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padr√£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para an√°lise\n\n        Returns:\n            C√≥digo da moeda detectada ou None se n√£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padr√µes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padr√£o para aceitar mesmo que n√£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada n√£o estiver explicitamente no texto,\n        # exceto para moedas padr√£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "get_currency_symbol", "cls", "currency_code", "str", "str", "retorna", "mbolo", "da", "moeda", "args", "currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "b13538e84a281e5000cb329f", "file_path": "src/utils/currency_utils.py", "start_line": 69, "end_line": 133, "content": "            currency_code: C√≥digo da moeda (ex: 'EUR', 'BRL')\n            \n        Returns:\n            S√≠mbolo da moeda (ex: '‚Ç¨', 'R$')\n        \"\"\"\n        return cls.CURRENCY_SYMBOLS.get(currency_code.upper(), currency_code)\n    \n    @classmethod\n    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: C√≥digo da moeda\n            \n        Returns:\n            Valor formatado com s√≠mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # Formata√ß√£o espec√≠fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: ‚Ç¨ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padr√£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para an√°lise\n\n        Returns:\n            C√≥digo da moeda detectada ou None se n√£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padr√µes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padr√£o para aceitar mesmo que n√£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada n√£o estiver explicitamente no texto,\n        # exceto para moedas padr√£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["currency_code", "digo", "da", "moeda", "ex", "eur", "brl", "returns", "mbolo", "da", "moeda", "ex", "return", "cls", "currency_symbols", "get", "currency_code", "upper", "currency_code", "classmethod", "def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "a5393e059676deb20cbe8646", "file_path": "src/utils/currency_utils.py", "start_line": 77, "end_line": 133, "content": "    def format_currency(cls, amount: float, currency_code: str) -> str:\n        \"\"\"\n        Formata um valor monet√°rio com a moeda apropriada.\n        \n        Args:\n            amount: Valor a ser formatado\n            currency_code: C√≥digo da moeda\n            \n        Returns:\n            Valor formatado com s√≠mbolo da moeda\n        \"\"\"\n        symbol = cls.get_currency_symbol(currency_code)\n        \n        # Formata√ß√£o espec√≠fica por moeda\n        if currency_code == 'EUR':\n            # Formato europeu: ‚Ç¨ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        elif currency_code == 'BRL':\n            # Formato brasileiro: R$ 1.234,56\n            return f\"{symbol} {amount:,.2f}\".replace(',', 'X').replace('.', ',').replace('X', '.')\n        else:\n            # Formato padr√£o: $ 1,234.56\n            return f\"{symbol} {amount:,.2f}\"\n    \n    @classmethod\n    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para an√°lise\n\n        Returns:\n            C√≥digo da moeda detectada ou None se n√£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padr√µes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padr√£o para aceitar mesmo que n√£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada n√£o estiver explicitamente no texto,\n        # exceto para moedas padr√£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "format_currency", "cls", "amount", "float", "currency_code", "str", "str", "formata", "um", "valor", "monet", "rio", "com", "moeda", "apropriada", "args", "amount", "valor", "ser", "formatado", "currency_code", "digo", "da", "moeda", "returns", "valor", "formatado", "com", "mbolo", "da", "moeda", "symbol", "cls", "get_currency_symbol", "currency_code", "formata", "espec", "fica", "por", "moeda", "if", "currency_code", "eur", "formato", "europeu", "return", "symbol", "amount", "replace", "replace", "replace", "elif", "currency_code", "brl", "formato", "brasileiro", "return", "symbol", "amount", "replace", "replace", "replace", "else", "formato", "padr", "return", "symbol", "amount", "classmethod", "def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "a16b61e1fb968720c77218aa", "file_path": "src/utils/currency_utils.py", "start_line": 102, "end_line": 133, "content": "    def extract_currency_from_dataframe(cls, df) -> str:\n        \"\"\"\n        Extrai a moeda de um DataFrame do pandas.\n\n        Args:\n            df: DataFrame para an√°lise\n\n        Returns:\n            C√≥digo da moeda detectada ou None se n√£o detectada\n        \"\"\"\n        import pandas as pd\n\n        # Converte todo o DataFrame para string e procura por padr√µes\n        text_content = \"\"\n\n        # Analisa as primeiras 20 linhas para detectar moeda\n        for idx in range(min(20, len(df))):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.notna(col):\n                    text_content += str(col) + \" \"\n\n        currency = cls.detect_currency_from_text(text_content)\n\n        # Lista de moedas padr√£o para aceitar mesmo que n√£o estejam explicitamente no texto\n        default_currencies = ['EUR', 'BRL', 'USD', 'GBP', 'JPY', 'CHF', 'CAD', 'AUD']\n\n        # Retorna None se a moeda detectada n√£o estiver explicitamente no texto,\n        # exceto para moedas padr√£o\n        if currency and (currency not in text_content) and (currency not in default_currencies):\n            return None\n        return currency", "mtime": 1756149301.0570138, "terms": ["def", "extract_currency_from_dataframe", "cls", "df", "str", "extrai", "moeda", "de", "um", "dataframe", "do", "pandas", "args", "df", "dataframe", "para", "an", "lise", "returns", "digo", "da", "moeda", "detectada", "ou", "none", "se", "detectada", "import", "pandas", "as", "pd", "converte", "todo", "dataframe", "para", "string", "procura", "por", "padr", "es", "text_content", "analisa", "as", "primeiras", "linhas", "para", "detectar", "moeda", "for", "idx", "in", "range", "min", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "notna", "col", "text_content", "str", "col", "currency", "cls", "detect_currency_from_text", "text_content", "lista", "de", "moedas", "padr", "para", "aceitar", "mesmo", "que", "estejam", "explicitamente", "no", "texto", "default_currencies", "eur", "brl", "usd", "gbp", "jpy", "chf", "cad", "aud", "retorna", "none", "se", "moeda", "detectada", "estiver", "explicitamente", "no", "texto", "exceto", "para", "moedas", "padr", "if", "currency", "and", "currency", "not", "in", "text_content", "and", "currency", "not", "in", "default_currencies", "return", "none", "return", "currency"]}
{"chunk_id": "30cd15052504f70af79b7ded", "file_path": "src/application/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote application", "mtime": 1755104719.7803147, "terms": ["pacote", "application"]}
{"chunk_id": "1b64a54e6aca1f352194731f", "file_path": "src/application/use_cases.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nCasos de uso para an√°lise de extratos.\n\"\"\"\nfrom pathlib import Path\nfrom typing import Optional, List\n\nfrom src.domain.interfaces import (\n    StatementReader,\n    TransactionCategorizer,\n    StatementAnalyzer,\n    ReportGenerator,\n)\nfrom src.domain.models import BankStatement, AnalysisResult\n\n\nclass AnalyzeStatementUseCase:\n    \"\"\"Caso de uso para analisar extratos banc√°rios.\"\"\"\n\n    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {file_path}\")\n\n        # L√™ o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transa√ß√µes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relat√≥rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis", "mtime": 1756146923.2629967, "terms": ["casos", "de", "uso", "para", "an", "lise", "de", "extratos", "from", "pathlib", "import", "path", "from", "typing", "import", "optional", "list", "from", "src", "domain", "interfaces", "import", "statementreader", "transactioncategorizer", "statementanalyzer", "reportgenerator", "from", "src", "domain", "models", "import", "bankstatement", "analysisresult", "class", "analyzestatementusecase", "caso", "de", "uso", "para", "analisar", "extratos", "banc", "rios", "def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis"]}
{"chunk_id": "99db0101b0636bae70c43212", "file_path": "src/application/use_cases.py", "start_line": 16, "end_line": 95, "content": "class AnalyzeStatementUseCase:\n    \"\"\"Caso de uso para analisar extratos banc√°rios.\"\"\"\n\n    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {file_path}\")\n\n        # L√™ o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transa√ß√µes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relat√≥rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:", "mtime": 1756146923.2629967, "terms": ["class", "analyzestatementusecase", "caso", "de", "uso", "para", "analisar", "extratos", "banc", "rios", "def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader"]}
{"chunk_id": "b6da7a8645e9a9f291aa3a19", "file_path": "src/application/use_cases.py", "start_line": 19, "end_line": 98, "content": "    def __init__(\n        self,\n        reader: StatementReader,\n        categorizer: TransactionCategorizer,\n        analyzer: StatementAnalyzer,\n        report_generator: ReportGenerator,\n    ):\n        self.reader = reader\n        self.categorizer = categorizer\n        self.analyzer = analyzer\n        self.report_generator = report_generator\n\n    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {file_path}\")\n\n        # L√™ o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transa√ß√µes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relat√≥rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:", "mtime": 1756146923.2629967, "terms": ["def", "__init__", "self", "reader", "statementreader", "categorizer", "transactioncategorizer", "analyzer", "statementanalyzer", "report_generator", "reportgenerator", "self", "reader", "reader", "self", "categorizer", "categorizer", "self", "analyzer", "analyzer", "self", "report_generator", "report_generator", "def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers"]}
{"chunk_id": "a4911f621ec84b149fd30b1b", "file_path": "src/application/use_cases.py", "start_line": 31, "end_line": 110, "content": "    def execute(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Converte para Path\n        file_path = Path(file_path)\n        output_path = Path(output_path) if output_path else None\n\n        # Valida se o arquivo existe\n        if not file_path.exists():\n            raise FileNotFoundError(f\"Arquivo n√£o encontrado: {file_path}\")\n\n        # L√™ o extrato\n        statement = self.reader.read(file_path)\n\n        # Categoriza as transa√ß√µes\n        for i, transaction in enumerate(statement.transactions):\n            statement.transactions[i] = self.categorizer.categorize(transaction)\n\n        # Analisa o extrato\n        analysis_result = self.analyzer.analyze(statement)\n\n        # Gera o relat√≥rio\n        report = self.report_generator.generate(analysis_result, output_path)\n\n        return analysis_result, report\n\n\nclass ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor dispon√≠vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        ", "mtime": 1756146923.2629967, "terms": ["def", "execute", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "converte", "para", "path", "file_path", "path", "file_path", "output_path", "path", "output_path", "if", "output_path", "else", "none", "valida", "se", "arquivo", "existe", "if", "not", "file_path", "exists", "raise", "filenotfounderror", "arquivo", "encontrado", "file_path", "extrato", "statement", "self", "reader", "read", "file_path", "categoriza", "as", "transa", "es", "for", "transaction", "in", "enumerate", "statement", "transactions", "statement", "transactions", "self", "categorizer", "categorize", "transaction", "analisa", "extrato", "analysis_result", "self", "analyzer", "analyze", "statement", "gera", "relat", "rio", "report", "self", "report_generator", "generate", "analysis_result", "output_path", "return", "analysis_result", "report", "class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path"]}
{"chunk_id": "94a549f7038eb288649ea5dd", "file_path": "src/application/use_cases.py", "start_line": 60, "end_line": 120, "content": "class ExtractAnalyzer:\n    \"\"\"Classe de fachada para simplificar o uso do sistema.\"\"\"\n\n    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor dispon√≠vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["class", "extractanalyzer", "classe", "de", "fachada", "para", "simplificar", "uso", "do", "sistema", "def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "4c9fac649e2506b50895e22f", "file_path": "src/application/use_cases.py", "start_line": 63, "end_line": 120, "content": "    def __init__(self):\n        # Importa implementa√ß√µes concretas\n        from src.infrastructure.readers.pdf_reader import PDFStatementReader\n        from src.infrastructure.readers.excel_reader import ExcelStatementReader\n        from src.infrastructure.readers.csv_reader import CSVStatementReader\n        from src.infrastructure.categorizers.keyword_categorizer import KeywordCategorizer\n        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor dispon√≠vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "__init__", "self", "importa", "implementa", "es", "concretas", "from", "src", "infrastructure", "readers", "pdf_reader", "import", "pdfstatementreader", "from", "src", "infrastructure", "readers", "excel_reader", "import", "excelstatementreader", "from", "src", "infrastructure", "readers", "csv_reader", "import", "csvstatementreader", "from", "src", "infrastructure", "categorizers", "keyword_categorizer", "import", "keywordcategorizer", "from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "76892954e39a296c3d3b54c3", "file_path": "src/application/use_cases.py", "start_line": 69, "end_line": 120, "content": "        from src.infrastructure.analyzers.basic_analyzer import BasicStatementAnalyzer\n        from src.infrastructure.reports.text_report import TextReportGenerator\n\n        # Inicializa componentes\n        self.pdf_reader = PDFStatementReader()\n        self.excel_reader = ExcelStatementReader()\n        self.csv_reader = CSVStatementReader()\n        self.categorizer = KeywordCategorizer()\n        self.analyzer = BasicStatementAnalyzer()\n        self.text_report = TextReportGenerator()\n\n        # Lista de leitores dispon√≠veis\n        self.readers: List[StatementReader] = [\n            self.pdf_reader,\n            self.excel_reader,\n            self.csv_reader\n        ]\n\n        # Cria caso de uso com o primeiro leitor (ser√° substitu√≠do dinamicamente)\n        self.use_case = AnalyzeStatementUseCase(\n            reader=self.pdf_reader,\n            categorizer=self.categorizer,\n            analyzer=self.analyzer,\n            report_generator=self.text_report,\n        )\n\n    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor dispon√≠vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["from", "src", "infrastructure", "analyzers", "basic_analyzer", "import", "basicstatementanalyzer", "from", "src", "infrastructure", "reports", "text_report", "import", "textreportgenerator", "inicializa", "componentes", "self", "pdf_reader", "pdfstatementreader", "self", "excel_reader", "excelstatementreader", "self", "csv_reader", "csvstatementreader", "self", "categorizer", "keywordcategorizer", "self", "analyzer", "basicstatementanalyzer", "self", "text_report", "textreportgenerator", "lista", "de", "leitores", "dispon", "veis", "self", "readers", "list", "statementreader", "self", "pdf_reader", "self", "excel_reader", "self", "csv_reader", "cria", "caso", "de", "uso", "com", "primeiro", "leitor", "ser", "substitu", "do", "dinamicamente", "self", "use_case", "analyzestatementusecase", "reader", "self", "pdf_reader", "categorizer", "self", "categorizer", "analyzer", "self", "analyzer", "report_generator", "self", "text_report", "def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "77160899a6c3f34bc34d18f7", "file_path": "src/application/use_cases.py", "start_line": 95, "end_line": 120, "content": "    def _get_appropriate_reader(self, file_path: str) -> StatementReader:\n        \"\"\"Retorna o leitor apropriado para o tipo de arquivo.\"\"\"\n        file_path_obj = Path(file_path)\n        for reader in self.readers:\n            if reader.can_read(file_path_obj):\n                return reader\n        raise ValueError(f\"Nenhum leitor dispon√≠vel para o arquivo: {file_path}\")\n\n    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "_get_appropriate_reader", "self", "file_path", "str", "statementreader", "retorna", "leitor", "apropriado", "para", "tipo", "de", "arquivo", "file_path_obj", "path", "file_path", "for", "reader", "in", "self", "readers", "if", "reader", "can_read", "file_path_obj", "return", "reader", "raise", "valueerror", "nenhum", "leitor", "dispon", "vel", "para", "arquivo", "file_path", "def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "46af600437a974066ebb4db9", "file_path": "src/application/use_cases.py", "start_line": 103, "end_line": 120, "content": "    def analyze_file(\n        self,\n        file_path: str,\n        output_path: Optional[str] = None,\n    ) -> tuple:\n        # Seleciona o leitor apropriado\n        reader = self._get_appropriate_reader(file_path)\n        \n        # Atualiza o caso de uso com o leitor correto\n        self.use_case.reader = reader\n        \n        result, report = self.use_case.execute(file_path, output_path)\n        return result, report, reader.read(Path(file_path))\n\n    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "analyze_file", "self", "file_path", "str", "output_path", "optional", "str", "none", "tuple", "seleciona", "leitor", "apropriado", "reader", "self", "_get_appropriate_reader", "file_path", "atualiza", "caso", "de", "uso", "com", "leitor", "correto", "self", "use_case", "reader", "reader", "result", "report", "self", "use_case", "execute", "file_path", "output_path", "return", "result", "report", "reader", "read", "path", "file_path", "def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "6682b38dc6fe6de7bfaad305", "file_path": "src/application/use_cases.py", "start_line": 117, "end_line": 120, "content": "    def analyze_and_print(self, file_path: str):\n        result, report = self.analyze_file(file_path)\n        print(report)\n        return result", "mtime": 1756146923.2629967, "terms": ["def", "analyze_and_print", "self", "file_path", "str", "result", "report", "self", "analyze_file", "file_path", "print", "report", "return", "result"]}
{"chunk_id": "2c1d39ced02fbd133ea9c56d", "file_path": "src/infrastructure/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote infrastructure", "mtime": 1755104726.021574, "terms": ["pacote", "infrastructure"]}
{"chunk_id": "5f29afe18eda4048261c4cad", "file_path": "src/domain/interfaces.py", "start_line": 1, "end_line": 58, "content": "\"\"\"\nInterfaces (protocolos) do dom√≠nio.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional\nfrom pathlib import Path\n\nfrom src.domain.models import BankStatement, Transaction, AnalysisResult\n\n\nclass StatementReader(ABC):\n    \"\"\"Interface para leitores de extratos.\"\"\"\n    \n    @abstractmethod\n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["interfaces", "protocolos", "do", "dom", "nio", "from", "abc", "import", "abc", "abstractmethod", "from", "typing", "import", "list", "optional", "from", "pathlib", "import", "path", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "analysisresult", "class", "statementreader", "abc", "interface", "para", "leitores", "de", "extratos", "abstractmethod", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "3b7d6623996c3a6ea56a0f36", "file_path": "src/domain/interfaces.py", "start_line": 11, "end_line": 58, "content": "class StatementReader(ABC):\n    \"\"\"Interface para leitores de extratos.\"\"\"\n    \n    @abstractmethod\n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "statementreader", "abc", "interface", "para", "leitores", "de", "extratos", "abstractmethod", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "b53faf4f2b52f07b79c11190", "file_path": "src/domain/interfaces.py", "start_line": 15, "end_line": 58, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo.\"\"\"\n        pass\n    \n    @abstractmethod\n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "pass", "abstractmethod", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "69afbfd16f680e80f685263b", "file_path": "src/domain/interfaces.py", "start_line": 20, "end_line": 58, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo e retorna um extrato.\"\"\"\n        pass\n\n\nclass TransactionParser(ABC):\n    \"\"\"Interface para parsers de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "retorna", "um", "extrato", "pass", "class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "b2e5315ad43533bfa5a2f5e3", "file_path": "src/domain/interfaces.py", "start_line": 25, "end_line": 58, "content": "class TransactionParser(ABC):\n    \"\"\"Interface para parsers de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "transactionparser", "abc", "interface", "para", "parsers", "de", "transa", "es", "abstractmethod", "def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "2a186bf08e675217021f5071", "file_path": "src/domain/interfaces.py", "start_line": 29, "end_line": 58, "content": "    def parse(self, raw_data: str) -> List[Transaction]:\n        \"\"\"Faz parsing de dados brutos para transa√ß√µes.\"\"\"\n        pass\n\n\nclass StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "parse", "self", "raw_data", "str", "list", "transaction", "faz", "parsing", "de", "dados", "brutos", "para", "transa", "es", "pass", "class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "5cb39268e695e1f658ad9852", "file_path": "src/domain/interfaces.py", "start_line": 34, "end_line": 58, "content": "class StatementAnalyzer(ABC):\n    \"\"\"Interface para analisadores de extratos.\"\"\"\n    \n    @abstractmethod\n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "statementanalyzer", "abc", "interface", "para", "analisadores", "de", "extratos", "abstractmethod", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "c51894f929b875be38828537", "file_path": "src/domain/interfaces.py", "start_line": 38, "end_line": 58, "content": "    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        pass\n\n\nclass ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "pass", "class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "3fdd202cd6c9a81a48d6be9e", "file_path": "src/domain/interfaces.py", "start_line": 43, "end_line": 58, "content": "class ReportGenerator(ABC):\n    \"\"\"Interface para geradores de relat√≥rios.\"\"\"\n    \n    @abstractmethod\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "reportgenerator", "abc", "interface", "para", "geradores", "de", "relat", "rios", "abstractmethod", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "7b4d145a76a7bfdc2a0b870f", "file_path": "src/domain/interfaces.py", "start_line": 47, "end_line": 58, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio a partir da an√°lise.\"\"\"\n        pass\n\n\nclass TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "partir", "da", "an", "lise", "pass", "class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "cfafa4f4c765694c7a3fc2b3", "file_path": "src/domain/interfaces.py", "start_line": 52, "end_line": 58, "content": "class TransactionCategorizer(ABC):\n    \"\"\"Interface para categorizadores de transa√ß√µes.\"\"\"\n    \n    @abstractmethod\n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["class", "transactioncategorizer", "abc", "interface", "para", "categorizadores", "de", "transa", "es", "abstractmethod", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "825471932a7d64834b4951a1", "file_path": "src/domain/interfaces.py", "start_line": 56, "end_line": 58, "content": "    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o.\"\"\"\n        pass", "mtime": 1755104182.5194025, "terms": ["def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "pass"]}
{"chunk_id": "a727aae8ce334c5ad59fe878", "file_path": "src/domain/models.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nModelos de dom√≠nio para o sistema de an√°lise de extratos banc√°rios.\n\"\"\"\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Optional, List\nfrom uuid import uuid4\n\n\nclass TransactionType(Enum):\n    \"\"\"Tipos de transa√ß√£o banc√°ria.\"\"\"\n    DEBIT = \"DEBIT\"\n    CREDIT = \"CREDIT\"\n    \n    \nclass TransactionCategory(Enum):\n    \"\"\"Categorias de transa√ß√£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transa√ß√£o banc√°ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"", "mtime": 1756145254.817271, "terms": ["modelos", "de", "dom", "nio", "para", "sistema", "de", "an", "lise", "de", "extratos", "banc", "rios", "from", "dataclasses", "import", "dataclass", "field", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "enum", "import", "enum", "from", "typing", "import", "optional", "list", "from", "uuid", "import", "uuid4", "class", "transactiontype", "enum", "tipos", "de", "transa", "banc", "ria", "debit", "debit", "credit", "credit", "class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa"]}
{"chunk_id": "02eb22cdfe035051b5dc3029", "file_path": "src/domain/models.py", "start_line": 12, "end_line": 91, "content": "class TransactionType(Enum):\n    \"\"\"Tipos de transa√ß√£o banc√°ria.\"\"\"\n    DEBIT = \"DEBIT\"\n    CREDIT = \"CREDIT\"\n    \n    \nclass TransactionCategory(Enum):\n    \"\"\"Categorias de transa√ß√£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transa√ß√£o banc√°ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income", "mtime": 1756145254.817271, "terms": ["class", "transactiontype", "enum", "tipos", "de", "transa", "banc", "ria", "debit", "debit", "credit", "credit", "class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income"]}
{"chunk_id": "3af765cb053a3e9db2614f51", "file_path": "src/domain/models.py", "start_line": 18, "end_line": 97, "content": "class TransactionCategory(Enum):\n    \"\"\"Categorias de transa√ß√£o.\"\"\"\n    ALIMENTACAO = \"ALIMENTACAO\"\n    TRANSPORTE = \"TRANSPORTE\"\n    MORADIA = \"MORADIA\"\n    SAUDE = \"SAUDE\"\n    EDUCACAO = \"EDUCACAO\"\n    LAZER = \"LAZER\"\n    COMPRAS = \"COMPRAS\"\n    SERVICOS = \"SERVICOS\"\n    TRANSFERENCIA = \"TRANSFERENCIA\"\n    INVESTIMENTO = \"INVESTIMENTO\"\n    SALARIO = \"SALARIO\"\n    OUTROS = \"OUTROS\"\n    NAO_CATEGORIZADO = \"NAO_CATEGORIZADO\"\n\n\n@dataclass\nclass Transaction:\n    \"\"\"Representa uma transa√ß√£o banc√°ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(", "mtime": 1756145254.817271, "terms": ["class", "transactioncategory", "enum", "categorias", "de", "transa", "alimentacao", "alimentacao", "transporte", "transporte", "moradia", "moradia", "saude", "saude", "educacao", "educacao", "lazer", "lazer", "compras", "compras", "servicos", "servicos", "transferencia", "transferencia", "investimento", "investimento", "salario", "salario", "outros", "outros", "nao_categorizado", "nao_categorizado", "dataclass", "class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum"]}
{"chunk_id": "b642a92992702b39d7980892", "file_path": "src/domain/models.py", "start_line": 36, "end_line": 115, "content": "class Transaction:\n    \"\"\"Representa uma transa√ß√£o banc√°ria.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    date: datetime = field(default_factory=datetime.now)\n    description: str = \"\"\n    amount: Decimal = Decimal(\"0.00\")\n    type: TransactionType = TransactionType.DEBIT\n    category: TransactionCategory = TransactionCategory.NAO_CATEGORIZADO\n    balance_after: Optional[Decimal] = None\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        ", "mtime": 1756145254.817271, "terms": ["class", "transaction", "representa", "uma", "transa", "banc", "ria", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "date", "datetime", "field", "default_factory", "datetime", "now", "description", "str", "amount", "decimal", "decimal", "type", "transactiontype", "transactiontype", "debit", "category", "transactioncategory", "transactioncategory", "nao_categorizado", "balance_after", "optional", "decimal", "none", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction"]}
{"chunk_id": "6e170b982f2d8d015c479111", "file_path": "src/domain/models.py", "start_line": 47, "end_line": 126, "content": "    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.amount, (int, float)):\n            self.amount = Decimal(str(self.amount))\n        if self.balance_after and isinstance(self.balance_after, (int, float)):\n            self.balance_after = Decimal(str(self.balance_after))\n            \n    @property\n    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n", "mtime": 1756145254.817271, "terms": ["def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "amount", "int", "float", "self", "amount", "decimal", "str", "self", "amount", "if", "self", "balance_after", "and", "isinstance", "self", "balance_after", "int", "float", "self", "balance_after", "decimal", "str", "self", "balance_after", "property", "def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end"]}
{"chunk_id": "6312bb58d1eded2d6e03e429", "file_path": "src/domain/models.py", "start_line": 55, "end_line": 134, "content": "    def is_income(self) -> bool:\n        \"\"\"Verifica se √© uma receita.\"\"\"\n        return self.type == TransactionType.CREDIT\n    \n    @property\n    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal", "mtime": 1756145254.817271, "terms": ["def", "is_income", "self", "bool", "verifica", "se", "uma", "receita", "return", "self", "type", "transactiontype", "credit", "property", "def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal"]}
{"chunk_id": "4f2b5dae3e11597ce15c7ddf", "file_path": "src/domain/models.py", "start_line": 60, "end_line": 139, "content": "    def is_expense(self) -> bool:\n        \"\"\"Verifica se √© uma despesa.\"\"\"\n        return self.type == TransactionType.DEBIT\n\n\n@dataclass\nclass BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)", "mtime": 1756145254.817271, "terms": ["def", "is_expense", "self", "bool", "verifica", "se", "uma", "despesa", "return", "self", "type", "transactiontype", "debit", "dataclass", "class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list"]}
{"chunk_id": "f1c218e95d647a7f0c2eb75c", "file_path": "src/domain/models.py", "start_line": 66, "end_line": 140, "content": "class BankStatement:\n    \"\"\"Representa um extrato banc√°rio.\"\"\"\n    id: str = field(default_factory=lambda: str(uuid4()))\n    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["class", "bankstatement", "representa", "um", "extrato", "banc", "rio", "id", "str", "field", "default_factory", "lambda", "str", "uuid4", "bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "6917e30bb231b395bc1da3e3", "file_path": "src/domain/models.py", "start_line": 69, "end_line": 140, "content": "    bank_name: str = \"\"\n    account_number: str = \"\"\n    period_start: datetime = field(default_factory=datetime.now)\n    period_end: datetime = field(default_factory=datetime.now)\n    initial_balance: Decimal = Decimal(\"0.00\")\n    final_balance: Decimal = Decimal(\"0.00\")\n    currency: str = \"EUR\"  # Moeda padr√£o\n    transactions: List[Transaction] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n    \n    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["bank_name", "str", "account_number", "str", "period_start", "datetime", "field", "default_factory", "datetime", "now", "period_end", "datetime", "field", "default_factory", "datetime", "now", "initial_balance", "decimal", "decimal", "final_balance", "decimal", "decimal", "currency", "str", "eur", "moeda", "padr", "transactions", "list", "transaction", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict", "def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "002624d9b348343e8dea7742", "file_path": "src/domain/models.py", "start_line": 79, "end_line": 140, "content": "    def __post_init__(self):\n        \"\"\"Valida√ß√µes ap√≥s inicializa√ß√£o.\"\"\"\n        if isinstance(self.initial_balance, (int, float)):\n            self.initial_balance = Decimal(str(self.initial_balance))\n        if isinstance(self.final_balance, (int, float)):\n            self.final_balance = Decimal(str(self.final_balance))\n    \n    @property\n    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "__post_init__", "self", "valida", "es", "ap", "inicializa", "if", "isinstance", "self", "initial_balance", "int", "float", "self", "initial_balance", "decimal", "str", "self", "initial_balance", "if", "isinstance", "self", "final_balance", "int", "float", "self", "final_balance", "decimal", "str", "self", "final_balance", "property", "def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "cd17cc71ab4118d64cb3bcd0", "file_path": "src/domain/models.py", "start_line": 87, "end_line": 140, "content": "    def total_income(self) -> Decimal:\n        \"\"\"Calcula o total de receitas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_income\n        )\n    \n    @property\n    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "total_income", "self", "decimal", "calcula", "total", "de", "receitas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_income", "property", "def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "7e51bd1f1610dd3a6c12d0b3", "file_path": "src/domain/models.py", "start_line": 95, "end_line": 140, "content": "    def total_expenses(self) -> Decimal:\n        \"\"\"Calcula o total de despesas.\"\"\"\n        return sum(\n            t.amount for t in self.transactions \n            if t.is_expense\n        )\n    \n    @property\n    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "total_expenses", "self", "decimal", "calcula", "total", "de", "despesas", "return", "sum", "amount", "for", "in", "self", "transactions", "if", "is_expense", "property", "def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "a638b425f6b5b26c74587b0c", "file_path": "src/domain/models.py", "start_line": 103, "end_line": 140, "content": "    def net_flow(self) -> Decimal:\n        \"\"\"Calcula o fluxo l√≠quido (receitas - despesas).\"\"\"\n        return self.total_income - self.total_expenses\n    \n    @property\n    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "net_flow", "self", "decimal", "calcula", "fluxo", "quido", "receitas", "despesas", "return", "self", "total_income", "self", "total_expenses", "property", "def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "775dc246da993a28bbeaabdf", "file_path": "src/domain/models.py", "start_line": 108, "end_line": 140, "content": "    def transaction_count(self) -> int:\n        \"\"\"Retorna o n√∫mero de transa√ß√µes.\"\"\"\n        return len(self.transactions)\n    \n    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "transaction_count", "self", "int", "retorna", "mero", "de", "transa", "es", "return", "len", "self", "transactions", "def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "2de4ed98b84aeebb3fc36b28", "file_path": "src/domain/models.py", "start_line": 112, "end_line": 140, "content": "    def add_transaction(self, transaction: Transaction) -> None:\n        \"\"\"Adiciona uma transa√ß√£o ao extrato.\"\"\"\n        self.transactions.append(transaction)\n        \n    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "add_transaction", "self", "transaction", "transaction", "none", "adiciona", "uma", "transa", "ao", "extrato", "self", "transactions", "append", "transaction", "def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "c81e29d10c2e82c5ed1540a2", "file_path": "src/domain/models.py", "start_line": 116, "end_line": 140, "content": "    def get_transactions_by_category(self, category: TransactionCategory) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes de uma categoria espec√≠fica.\"\"\"\n        return [t for t in self.transactions if t.category == category]\n    \n    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "get_transactions_by_category", "self", "category", "transactioncategory", "list", "transaction", "retorna", "transa", "es", "de", "uma", "categoria", "espec", "fica", "return", "for", "in", "self", "transactions", "if", "category", "category", "def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "96e0d30f0eb2149842409b29", "file_path": "src/domain/models.py", "start_line": 120, "end_line": 140, "content": "    def get_transactions_by_date_range(self, start: datetime, end: datetime) -> List[Transaction]:\n        \"\"\"Retorna transa√ß√µes em um per√≠odo espec√≠fico.\"\"\"\n        return [\n            t for t in self.transactions \n            if start <= t.date <= end\n        ]\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["def", "get_transactions_by_date_range", "self", "start", "datetime", "end", "datetime", "list", "transaction", "retorna", "transa", "es", "em", "um", "per", "odo", "espec", "fico", "return", "for", "in", "self", "transactions", "if", "start", "date", "end", "dataclass", "class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "91421a6358d776d42b07513f", "file_path": "src/domain/models.py", "start_line": 129, "end_line": 140, "content": "class AnalysisResult:\n    \"\"\"Resultado da an√°lise de um extrato.\"\"\"\n    statement_id: str\n    total_income: Decimal\n    total_expenses: Decimal\n    net_flow: Decimal\n    currency: str  # Moeda do extrato\n    categories_summary: dict[TransactionCategory, Decimal]\n    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["class", "analysisresult", "resultado", "da", "an", "lise", "de", "um", "extrato", "statement_id", "str", "total_income", "decimal", "total_expenses", "decimal", "net_flow", "decimal", "currency", "str", "moeda", "do", "extrato", "categories_summary", "dict", "transactioncategory", "decimal", "monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "72aba891735258e9f9339e0f", "file_path": "src/domain/models.py", "start_line": 137, "end_line": 140, "content": "    monthly_summary: dict[str, dict[str, Decimal]]\n    alerts: List[str] = field(default_factory=list)\n    insights: List[str] = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)", "mtime": 1756145254.817271, "terms": ["monthly_summary", "dict", "str", "dict", "str", "decimal", "alerts", "list", "str", "field", "default_factory", "list", "insights", "list", "str", "field", "default_factory", "list", "metadata", "dict", "field", "default_factory", "dict"]}
{"chunk_id": "28534cd06ce6a551fbbcd474", "file_path": "src/domain/__init__.py", "start_line": 1, "end_line": 1, "content": "# Pacote domain", "mtime": 1755104713.7980626, "terms": ["pacote", "domain"]}
{"chunk_id": "a2f746f75e19897bcc4c787d", "file_path": "src/domain/exceptions.py", "start_line": 1, "end_line": 33, "content": "\"\"\"\nExce√ß√µes customizadas do dom√≠nio.\n\"\"\"\n\n\nclass DomainException(Exception):\n    \"\"\"Exce√ß√£o base do dom√≠nio.\"\"\"\n    pass\n\n\nclass InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transa√ß√£o √© inv√°lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato √© inv√°lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["exce", "es", "customizadas", "do", "dom", "nio", "class", "domainexception", "exception", "exce", "base", "do", "dom", "nio", "pass", "class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "2db100339f253381998313da", "file_path": "src/domain/exceptions.py", "start_line": 6, "end_line": 33, "content": "class DomainException(Exception):\n    \"\"\"Exce√ß√£o base do dom√≠nio.\"\"\"\n    pass\n\n\nclass InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transa√ß√£o √© inv√°lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato √© inv√°lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "domainexception", "exception", "exce", "base", "do", "dom", "nio", "pass", "class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "fe5d78b34f4564f7090e4fcb", "file_path": "src/domain/exceptions.py", "start_line": 11, "end_line": 33, "content": "class InvalidTransactionError(DomainException):\n    \"\"\"Erro quando uma transa√ß√£o √© inv√°lida.\"\"\"\n    pass\n\n\nclass InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato √© inv√°lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "invalidtransactionerror", "domainexception", "erro", "quando", "uma", "transa", "inv", "lida", "pass", "class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "98aedbb6370172bd7ee2a68a", "file_path": "src/domain/exceptions.py", "start_line": 16, "end_line": 33, "content": "class InvalidStatementError(DomainException):\n    \"\"\"Erro quando um extrato √© inv√°lido.\"\"\"\n    pass\n\n\nclass ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "invalidstatementerror", "domainexception", "erro", "quando", "um", "extrato", "inv", "lido", "pass", "class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "07b5667fa20356982563afc3", "file_path": "src/domain/exceptions.py", "start_line": 21, "end_line": 33, "content": "class ParsingError(DomainException):\n    \"\"\"Erro ao fazer parsing de dados.\"\"\"\n    pass\n\n\nclass FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "parsingerror", "domainexception", "erro", "ao", "fazer", "parsing", "de", "dados", "pass", "class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "7734690c1b228eb15e6f43fc", "file_path": "src/domain/exceptions.py", "start_line": 26, "end_line": 33, "content": "class FileNotSupportedError(DomainException):\n    \"\"\"Erro quando o formato do arquivo n√£o √© suportado.\"\"\"\n    pass\n\n\nclass AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "filenotsupportederror", "domainexception", "erro", "quando", "formato", "do", "arquivo", "suportado", "pass", "class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "5bc45aa5c03f565013079aba", "file_path": "src/domain/exceptions.py", "start_line": 31, "end_line": 33, "content": "class AnalysisError(DomainException):\n    \"\"\"Erro durante an√°lise de dados.\"\"\"\n    pass", "mtime": 1755104150.5030491, "terms": ["class", "analysiserror", "domainexception", "erro", "durante", "an", "lise", "de", "dados", "pass"]}
{"chunk_id": "11e0c82412b3e2aa030321d9", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o do analisador b√°sico de extratos.\n\"\"\"\nfrom collections import defaultdict\nfrom decimal import Decimal\nfrom typing import Dict\n\nfrom src.domain.models import BankStatement, AnalysisResult, TransactionCategory\nfrom src.domain.interfaces import StatementAnalyzer\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass BasicStatementAnalyzer(StatementAnalyzer):\n    \"\"\"Analisador b√°sico de extratos banc√°rios.\"\"\"\n    \n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:", "mtime": 1756145358.5171318, "terms": ["implementa", "do", "analisador", "sico", "de", "extratos", "from", "collections", "import", "defaultdict", "from", "decimal", "import", "decimal", "from", "typing", "import", "dict", "from", "src", "domain", "models", "import", "bankstatement", "analysisresult", "transactioncategory", "from", "src", "domain", "interfaces", "import", "statementanalyzer", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "basicstatementanalyzer", "statementanalyzer", "analisador", "sico", "de", "extratos", "banc", "rios", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str"]}
{"chunk_id": "e04394cd51653cc185079166", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 13, "end_line": 92, "content": "class BasicStatementAnalyzer(StatementAnalyzer):\n    \"\"\"Analisador b√°sico de extratos banc√°rios.\"\"\"\n    \n    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions ", "mtime": 1756145358.5171318, "terms": ["class", "basicstatementanalyzer", "statementanalyzer", "analisador", "sico", "de", "extratos", "banc", "rios", "def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions"]}
{"chunk_id": "35c4c02be3e4d783b4668ac4", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 16, "end_line": 95, "content": "    def analyze(self, statement: BankStatement) -> AnalysisResult:\n        \"\"\"Analisa um extrato e retorna resultados.\"\"\"\n        # Calcula totais\n        total_income = statement.total_income\n        total_expenses = statement.total_expenses\n        net_flow = statement.net_flow\n        \n        # Resumo por categoria\n        categories_summary = self._calculate_categories_summary(statement)\n        \n        # Resumo mensal\n        monthly_summary = self._calculate_monthly_summary(statement)\n        \n        # Gera alertas\n        alerts = self._generate_alerts(statement, categories_summary)\n        \n        # Gera insights\n        insights = self._generate_insights(statement, categories_summary)\n        \n        return AnalysisResult(\n            statement_id=statement.id,\n            total_income=total_income,\n            total_expenses=total_expenses,\n            net_flow=net_flow,\n            currency=statement.currency,  # Propaga a moeda do extrato\n            categories_summary=categories_summary,\n            monthly_summary=monthly_summary,\n            alerts=alerts,\n            insights=insights,\n            metadata={\n                'transaction_count': statement.transaction_count,\n                'period_days': (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            }\n        )\n    \n    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:", "mtime": 1756145358.5171318, "terms": ["def", "analyze", "self", "statement", "bankstatement", "analysisresult", "analisa", "um", "extrato", "retorna", "resultados", "calcula", "totais", "total_income", "statement", "total_income", "total_expenses", "statement", "total_expenses", "net_flow", "statement", "net_flow", "resumo", "por", "categoria", "categories_summary", "self", "_calculate_categories_summary", "statement", "resumo", "mensal", "monthly_summary", "self", "_calculate_monthly_summary", "statement", "gera", "alertas", "alerts", "self", "_generate_alerts", "statement", "categories_summary", "gera", "insights", "insights", "self", "_generate_insights", "statement", "categories_summary", "return", "analysisresult", "statement_id", "statement", "id", "total_income", "total_income", "total_expenses", "total_expenses", "net_flow", "net_flow", "currency", "statement", "currency", "propaga", "moeda", "do", "extrato", "categories_summary", "categories_summary", "monthly_summary", "monthly_summary", "alerts", "alerts", "insights", "insights", "metadata", "transaction_count", "statement", "transaction_count", "period_days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions"]}
{"chunk_id": "6aaecd880e25b0e86fe5a6e4", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 51, "end_line": 130, "content": "    def _calculate_categories_summary(self, statement: BankStatement) -> Dict[TransactionCategory, Decimal]:\n        \"\"\"Calcula resumo de gastos por categoria.\"\"\"\n        summary = defaultdict(Decimal)\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense:\n                summary[transaction.category] += transaction.amount\n        \n        # Converte para dict normal e ordena por valor\n        return dict(sorted(summary.items(), key=lambda x: x[1], reverse=True))\n    \n    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"‚ö†Ô∏è {uncategorized} transa√ß√µes n√£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias espec√≠ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"‚ö†Ô∏è Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transa√ß√µes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"‚ö†Ô∏è Transa√ß√£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]", "mtime": 1756145358.5171318, "terms": ["def", "_calculate_categories_summary", "self", "statement", "bankstatement", "dict", "transactioncategory", "decimal", "calcula", "resumo", "de", "gastos", "por", "categoria", "summary", "defaultdict", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "summary", "transaction", "category", "transaction", "amount", "converte", "para", "dict", "normal", "ordena", "por", "valor", "return", "dict", "sorted", "summary", "items", "key", "lambda", "reverse", "true", "def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category"]}
{"chunk_id": "4a04bbb0052bd6bbd81c6cff", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 62, "end_line": 141, "content": "    def _calculate_monthly_summary(self, statement: BankStatement) -> Dict[str, Dict[str, Decimal]]:\n        \"\"\"Calcula resumo mensal de receitas e despesas.\"\"\"\n        monthly = defaultdict(lambda: {'income': Decimal('0'), 'expenses': Decimal('0')})\n        \n        for transaction in statement.transactions:\n            month_key = transaction.date.strftime('%Y-%m')\n            \n            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"‚ö†Ô∏è {uncategorized} transa√ß√µes n√£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias espec√≠ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"‚ö†Ô∏è Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transa√ß√µes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"‚ö†Ô∏è Transa√ß√£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"üí° Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre m√©dia di√°ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days", "mtime": 1756145358.5171318, "terms": ["def", "_calculate_monthly_summary", "self", "statement", "bankstatement", "dict", "str", "dict", "str", "decimal", "calcula", "resumo", "mensal", "de", "receitas", "despesas", "monthly", "defaultdict", "lambda", "income", "decimal", "expenses", "decimal", "for", "transaction", "in", "statement", "transactions", "month_key", "transaction", "date", "strftime", "if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days"]}
{"chunk_id": "528c3986c01fa4da96995a22", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 69, "end_line": 148, "content": "            if transaction.is_income:\n                monthly[month_key]['income'] += transaction.amount\n            else:\n                monthly[month_key]['expenses'] += transaction.amount\n        \n        # Calcula saldo mensal\n        for month_data in monthly.values():\n            month_data['balance'] = month_data['income'] - month_data['expenses']\n        \n        return dict(monthly)\n    \n    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"‚ö†Ô∏è {uncategorized} transa√ß√µes n√£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias espec√≠ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"‚ö†Ô∏è Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transa√ß√µes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"‚ö†Ô∏è Transa√ß√£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"üí° Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre m√©dia di√°ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"üí° M√©dia di√°ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padr√£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()", "mtime": 1756145358.5171318, "terms": ["if", "transaction", "is_income", "monthly", "month_key", "income", "transaction", "amount", "else", "monthly", "month_key", "expenses", "transaction", "amount", "calcula", "saldo", "mensal", "for", "month_data", "in", "monthly", "values", "month_data", "balance", "month_data", "income", "month_data", "expenses", "return", "dict", "monthly", "def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort"]}
{"chunk_id": "853c66e52e71bdda355fbf4a", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 80, "end_line": 159, "content": "    def _generate_alerts(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera alertas baseados na an√°lise.\"\"\"\n        alerts = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Alerta de saldo negativo\n        if statement.net_flow < 0:\n            deficit = abs(statement.net_flow)\n            alerts.append(f\"‚ö†Ô∏è Aten√ß√£o: Despesas superaram receitas em {currency_symbol} {deficit:.2f}\")\n        \n        # Alerta de muitas transa√ß√µes n√£o categorizadas\n        uncategorized = sum(\n            1 for t in statement.transactions \n            if t.category == TransactionCategory.NAO_CATEGORIZADO\n        )\n        if uncategorized > len(statement.transactions) * 0.3:\n            alerts.append(f\"‚ö†Ô∏è {uncategorized} transa√ß√µes n√£o foram categorizadas automaticamente\")\n        \n        # Alerta de gastos altos em categorias espec√≠ficas\n        total_expenses = statement.total_expenses\n        if total_expenses > 0:\n            for category, amount in categories_summary.items():\n                percentage = (amount / total_expenses) * 100\n                if percentage > 40:\n                    alerts.append(\n                        f\"‚ö†Ô∏è Gastos com {category.value} representam {percentage:.1f}% do total\"\n                    )\n        \n        # Alerta de transa√ß√µes de alto valor\n        avg_expense = (\n            total_expenses / len([t for t in statement.transactions if t.is_expense])\n            if any(t.is_expense for t in statement.transactions) else Decimal('0')\n        )\n        \n        for transaction in statement.transactions:\n            if transaction.is_expense and transaction.amount > avg_expense * 3:\n                alerts.append(\n                    f\"‚ö†Ô∏è Transa√ß√£o de alto valor: {transaction.description[:50]} - {currency_symbol} {transaction.amount:.2f}\"\n                )\n        \n        return alerts[:5]  # Limita a 5 alertas mais importantes\n    \n    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"üí° Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre m√©dia di√°ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"üí° M√©dia di√°ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padr√£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"üí° {percentage:.0f}% das suas despesas s√£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequ√™ncia de transa√ß√µes", "mtime": 1756145358.5171318, "terms": ["def", "_generate_alerts", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "alertas", "baseados", "na", "an", "lise", "alerts", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "alerta", "de", "saldo", "negativo", "if", "statement", "net_flow", "deficit", "abs", "statement", "net_flow", "alerts", "append", "aten", "despesas", "superaram", "receitas", "em", "currency_symbol", "deficit", "alerta", "de", "muitas", "transa", "es", "categorizadas", "uncategorized", "sum", "for", "in", "statement", "transactions", "if", "category", "transactioncategory", "nao_categorizado", "if", "uncategorized", "len", "statement", "transactions", "alerts", "append", "uncategorized", "transa", "es", "foram", "categorizadas", "automaticamente", "alerta", "de", "gastos", "altos", "em", "categorias", "espec", "ficas", "total_expenses", "statement", "total_expenses", "if", "total_expenses", "for", "category", "amount", "in", "categories_summary", "items", "percentage", "amount", "total_expenses", "if", "percentage", "alerts", "append", "gastos", "com", "category", "value", "representam", "percentage", "do", "total", "alerta", "de", "transa", "es", "de", "alto", "valor", "avg_expense", "total_expenses", "len", "for", "in", "statement", "transactions", "if", "is_expense", "if", "any", "is_expense", "for", "in", "statement", "transactions", "else", "decimal", "for", "transaction", "in", "statement", "transactions", "if", "transaction", "is_expense", "and", "transaction", "amount", "avg_expense", "alerts", "append", "transa", "de", "alto", "valor", "transaction", "description", "currency_symbol", "transaction", "amount", "return", "alerts", "limita", "alertas", "mais", "importantes", "def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es"]}
{"chunk_id": "857dc31280824f1cf682406e", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 122, "end_line": 189, "content": "    def _generate_insights(self, statement: BankStatement, categories_summary: Dict) -> list[str]:\n        \"\"\"Gera insights sobre os gastos.\"\"\"\n        insights = []\n        currency_symbol = CurrencyUtils.get_currency_symbol(statement.currency)\n        \n        # Insight sobre categoria com maior gasto\n        if categories_summary:\n            top_category = list(categories_summary.keys())[0]\n            top_amount = categories_summary[top_category]\n            percentage = (top_amount / statement.total_expenses * 100) if statement.total_expenses > 0 else 0\n            \n            insights.append(\n                f\"üí° Maior categoria de gastos: {top_category.value} ({currency_symbol} {top_amount:.2f} - {percentage:.1f}%)\"\n            )\n        \n        # Insight sobre m√©dia di√°ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"üí° M√©dia di√°ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padr√£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"üí° {percentage:.0f}% das suas despesas s√£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequ√™ncia de transa√ß√µes\n        if (statement.transaction_count > 0 and\n            statement.period_end and statement.period_start and\n            statement.period_end > statement.period_start):\n            days = (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            if days > 0:\n                trans_per_day = statement.transaction_count / days\n                insights.append(\n                    f\"üí° M√©dia de {trans_per_day:.1f} transa√ß√µes por dia\"\n                )\n        \n        # Insight sobre economia potencial\n        if categories_summary:\n            # Identifica categorias com potencial de economia\n            discretionary = [\n                TransactionCategory.LAZER,\n                TransactionCategory.COMPRAS,\n                TransactionCategory.ALIMENTACAO\n            ]\n            \n            potential_savings = Decimal('0')\n            for cat in discretionary:\n                if cat in categories_summary:\n                    potential_savings += categories_summary[cat] * Decimal('0.2')  # 20% de economia\n            \n            if potential_savings > 0:\n                insights.append(\n                    f\"üí° Potencial de economia: {currency_symbol} {potential_savings:.2f} reduzindo 20% em gastos discricion√°rios\"\n                )\n        \n        return insights[:5]  # Limita a 5 insights mais relevantes", "mtime": 1756145358.5171318, "terms": ["def", "_generate_insights", "self", "statement", "bankstatement", "categories_summary", "dict", "list", "str", "gera", "insights", "sobre", "os", "gastos", "insights", "currency_symbol", "currencyutils", "get_currency_symbol", "statement", "currency", "insight", "sobre", "categoria", "com", "maior", "gasto", "if", "categories_summary", "top_category", "list", "categories_summary", "keys", "top_amount", "categories_summary", "top_category", "percentage", "top_amount", "statement", "total_expenses", "if", "statement", "total_expenses", "else", "insights", "append", "maior", "categoria", "de", "gastos", "top_category", "value", "currency_symbol", "top_amount", "percentage", "insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es", "if", "statement", "transaction_count", "and", "statement", "period_end", "and", "statement", "period_start", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "if", "days", "trans_per_day", "statement", "transaction_count", "days", "insights", "append", "dia", "de", "trans_per_day", "transa", "es", "por", "dia", "insight", "sobre", "economia", "potencial", "if", "categories_summary", "identifica", "categorias", "com", "potencial", "de", "economia", "discretionary", "transactioncategory", "lazer", "transactioncategory", "compras", "transactioncategory", "alimentacao", "potential_savings", "decimal", "for", "cat", "in", "discretionary", "if", "cat", "in", "categories_summary", "potential_savings", "categories_summary", "cat", "decimal", "de", "economia", "if", "potential_savings", "insights", "append", "potencial", "de", "economia", "currency_symbol", "potential_savings", "reduzindo", "em", "gastos", "discricion", "rios", "return", "insights", "limita", "insights", "mais", "relevantes"]}
{"chunk_id": "bb27cfbbdc1ba88352cabfc7", "file_path": "src/infrastructure/analyzers/basic_analyzer.py", "start_line": 137, "end_line": 189, "content": "        # Insight sobre m√©dia di√°ria de gastos\n        if statement.period_end is not None and statement.period_start is not None and statement.period_end > statement.period_start:\n            days = (statement.period_end - statement.period_start).days\n            if days > 0:\n                daily_avg = statement.total_expenses / days\n                insights.append(f\"üí° M√©dia di√°ria de gastos: {currency_symbol} {daily_avg:.2f}\")\n        \n        # Insight sobre padr√£o de gastos\n        expense_transactions = [t for t in statement.transactions if t.is_expense]\n        if len(expense_transactions) > 10:\n            amounts = [t.amount for t in expense_transactions]\n            amounts.sort()\n            median_idx = len(amounts) // 2\n            median = amounts[median_idx]\n            \n            small_transactions = sum(1 for a in amounts if a < median)\n            percentage = (small_transactions / len(amounts)) * 100\n            \n            insights.append(\n                f\"üí° {percentage:.0f}% das suas despesas s√£o menores que {currency_symbol} {median:.2f}\"\n            )\n        \n        # Insight sobre frequ√™ncia de transa√ß√µes\n        if (statement.transaction_count > 0 and\n            statement.period_end and statement.period_start and\n            statement.period_end > statement.period_start):\n            days = (statement.period_end - statement.period_start).days if statement.period_end and statement.period_start else 0\n            if days > 0:\n                trans_per_day = statement.transaction_count / days\n                insights.append(\n                    f\"üí° M√©dia de {trans_per_day:.1f} transa√ß√µes por dia\"\n                )\n        \n        # Insight sobre economia potencial\n        if categories_summary:\n            # Identifica categorias com potencial de economia\n            discretionary = [\n                TransactionCategory.LAZER,\n                TransactionCategory.COMPRAS,\n                TransactionCategory.ALIMENTACAO\n            ]\n            \n            potential_savings = Decimal('0')\n            for cat in discretionary:\n                if cat in categories_summary:\n                    potential_savings += categories_summary[cat] * Decimal('0.2')  # 20% de economia\n            \n            if potential_savings > 0:\n                insights.append(\n                    f\"üí° Potencial de economia: {currency_symbol} {potential_savings:.2f} reduzindo 20% em gastos discricion√°rios\"\n                )\n        \n        return insights[:5]  # Limita a 5 insights mais relevantes", "mtime": 1756145358.5171318, "terms": ["insight", "sobre", "dia", "di", "ria", "de", "gastos", "if", "statement", "period_end", "is", "not", "none", "and", "statement", "period_start", "is", "not", "none", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "days", "daily_avg", "statement", "total_expenses", "days", "insights", "append", "dia", "di", "ria", "de", "gastos", "currency_symbol", "daily_avg", "insight", "sobre", "padr", "de", "gastos", "expense_transactions", "for", "in", "statement", "transactions", "if", "is_expense", "if", "len", "expense_transactions", "amounts", "amount", "for", "in", "expense_transactions", "amounts", "sort", "median_idx", "len", "amounts", "median", "amounts", "median_idx", "small_transactions", "sum", "for", "in", "amounts", "if", "median", "percentage", "small_transactions", "len", "amounts", "insights", "append", "percentage", "das", "suas", "despesas", "menores", "que", "currency_symbol", "median", "insight", "sobre", "frequ", "ncia", "de", "transa", "es", "if", "statement", "transaction_count", "and", "statement", "period_end", "and", "statement", "period_start", "and", "statement", "period_end", "statement", "period_start", "days", "statement", "period_end", "statement", "period_start", "days", "if", "statement", "period_end", "and", "statement", "period_start", "else", "if", "days", "trans_per_day", "statement", "transaction_count", "days", "insights", "append", "dia", "de", "trans_per_day", "transa", "es", "por", "dia", "insight", "sobre", "economia", "potencial", "if", "categories_summary", "identifica", "categorias", "com", "potencial", "de", "economia", "discretionary", "transactioncategory", "lazer", "transactioncategory", "compras", "transactioncategory", "alimentacao", "potential_savings", "decimal", "for", "cat", "in", "discretionary", "if", "cat", "in", "categories_summary", "potential_savings", "categories_summary", "cat", "decimal", "de", "economia", "if", "potential_savings", "insights", "append", "potencial", "de", "economia", "currency_symbol", "potential_savings", "reduzindo", "em", "gastos", "discricion", "rios", "return", "insights", "limita", "insights", "mais", "relevantes"]}
{"chunk_id": "a4947b1460588a7c3753dd87", "file_path": "src/infrastructure/analyzers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote analyzers", "mtime": 1755104473.5524144, "terms": ["inicializa", "pacote", "analyzers"]}
{"chunk_id": "6bfbc7d0fbdf0fd3aba3abe0", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o de leitor de extratos em PDF.\n\"\"\"\nimport re\nimport json\nimport os\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple\nimport pdfplumber\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\n\n\nclass PDFStatementReader(StatementReader):\n    # Load configuration from a JSON file\n    CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'pdf_reader_config.json')\n\n    with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:\n        config = json.load(config_file)\n\n    date_patterns = [\n        r\"\\d{2}/\\d{2}/\\d{4}\",\n        r\"\\d{2}-\\d{2}-\\d{4}\",\n        r\"\\d{4}-\\d{2}-\\d{2}\",\n    ]\n\n    amount_patterns = [\n        r\"-?\\d{1,3}(?:\\.\\d{3})*,\\d{2}\",  # e.g. 1.234,56 or -1.234,56\n        r\"-?\\d+,\\d{2}\",  # e.g. 1234,56 or -1234,56\n        r\"-?\\d+\\.\\d{2}\",  # e.g. 1234.56 or -1234.56\n    ]\n\n    # Use config for bank names\n    bank_name_patterns = config.get('bank_name_patterns', [])\n\n    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)", "mtime": 1755732670.6391072, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "pdf", "import", "re", "import", "json", "import", "os", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "optional", "tuple", "import", "pdfplumber", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "class", "pdfstatementreader", "statementreader", "load", "configuration", "from", "json", "file", "config_path", "os", "path", "join", "os", "path", "dirname", "__file__", "pdf_reader_config", "json", "with", "open", "config_path", "encoding", "utf", "as", "config_file", "config", "json", "load", "config_file", "date_patterns", "amount_patterns", "or", "or", "or", "use", "config", "for", "bank", "names", "bank_name_patterns", "config", "get", "bank_name_patterns", "def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group"]}
{"chunk_id": "02fb0a2aff8a2e24a5f64251", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 18, "end_line": 97, "content": "class PDFStatementReader(StatementReader):\n    # Load configuration from a JSON file\n    CONFIG_PATH = os.path.join(os.path.dirname(__file__), 'pdf_reader_config.json')\n\n    with open(CONFIG_PATH, 'r', encoding='utf-8') as config_file:\n        config = json.load(config_file)\n\n    date_patterns = [\n        r\"\\d{2}/\\d{2}/\\d{4}\",\n        r\"\\d{2}-\\d{2}-\\d{4}\",\n        r\"\\d{4}-\\d{2}-\\d{2}\",\n    ]\n\n    amount_patterns = [\n        r\"-?\\d{1,3}(?:\\.\\d{3})*,\\d{2}\",  # e.g. 1.234,56 or -1.234,56\n        r\"-?\\d+,\\d{2}\",  # e.g. 1234,56 or -1234,56\n        r\"-?\\d+\\.\\d{2}\",  # e.g. 1234.56 or -1234.56\n    ]\n\n    # Use config for bank names\n    bank_name_patterns = config.get('bank_name_patterns', [])\n\n    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"", "mtime": 1755732670.6391072, "terms": ["class", "pdfstatementreader", "statementreader", "load", "configuration", "from", "json", "file", "config_path", "os", "path", "join", "os", "path", "dirname", "__file__", "pdf_reader_config", "json", "with", "open", "config_path", "encoding", "utf", "as", "config_file", "config", "json", "load", "config_file", "date_patterns", "amount_patterns", "or", "or", "or", "use", "config", "for", "bank", "names", "bank_name_patterns", "config", "get", "bank_name_patterns", "def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period"]}
{"chunk_id": "0562c7c670757814c69d824f", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 40, "end_line": 119, "content": "    def __init__(self):\n        self.currency = self.config.get('currency', 'EUR')\n        self.credit_pattern = self.config.get('credit_pattern', r'\\+?\\d+(?:[.,]\\d+)?')\n        self.debit_pattern = self.config.get('debit_pattern', r'-?\\d+(?:[.,]\\d+)?')\n\n    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')", "mtime": 1755732670.6391072, "terms": ["def", "__init__", "self", "self", "currency", "self", "config", "get", "currency", "eur", "self", "credit_pattern", "self", "config", "get", "credit_pattern", "self", "debit_pattern", "self", "config", "get", "debit_pattern", "def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal"]}
{"chunk_id": "e77460a1a703cb9441fec400", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 45, "end_line": 124, "content": "    def can_read(self, file_path: Path) -> bool:\n        return file_path.suffix.lower() == '.pdf'\n\n    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',", "mtime": 1755732670.6391072, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "return", "file_path", "suffix", "lower", "pdf", "def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency"]}
{"chunk_id": "40170ff61e67321ba5f7ed42", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 48, "end_line": 127, "content": "    def read(self, file_path: Path) -> BankStatement:\n        try:\n            text = self._extract_text(file_path)\n            bank_name = self._extract_bank_name(text)\n            account_number = self._extract_account_number(text)\n            period = self._extract_period(text)\n            initial_balance = self._extract_initial_balance(text)\n            final_balance = self._extract_final_balance(text)\n            transactions = self._extract_transactions(text)\n\n            # Ajuste para passar period_start e period_end em vez de period\n            return BankStatement(\n                bank_name=bank_name,\n                account_number=account_number,\n                period_start=period[0] if period else None,\n                period_end=period[1] if period else None,\n                initial_balance=initial_balance,\n                final_balance=final_balance,\n                transactions=transactions\n            )\n        except Exception as e:\n            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)", "mtime": 1755732670.6391072, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "try", "text", "self", "_extract_text", "file_path", "bank_name", "self", "_extract_bank_name", "text", "account_number", "self", "_extract_account_number", "text", "period", "self", "_extract_period", "text", "initial_balance", "self", "_extract_initial_balance", "text", "final_balance", "self", "_extract_final_balance", "text", "transactions", "self", "_extract_transactions", "text", "ajuste", "para", "passar", "period_start", "period_end", "em", "vez", "de", "period", "return", "bankstatement", "bank_name", "bank_name", "account_number", "account_number", "period_start", "period", "if", "period", "else", "none", "period_end", "period", "if", "period", "else", "none", "initial_balance", "initial_balance", "final_balance", "final_balance", "transactions", "transactions", "except", "exception", "as", "raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase"]}
{"chunk_id": "512af0cd4baeee17ca40aba0", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 69, "end_line": 148, "content": "            raise ParsingError(f\"Error reading PDF file: {e}\")\n\n    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)", "mtime": 1755732670.6391072, "terms": ["raise", "parsingerror", "error", "reading", "pdf", "file", "def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line"]}
{"chunk_id": "86efea49cddc93376d5531e2", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 71, "end_line": 150, "content": "    def _extract_text(self, file_path: Path) -> str:\n        with pdfplumber.open(file_path) as pdf:\n            text = '\\n'.join(page.extract_text() for page in pdf.pages if page.extract_text())\n        return text\n\n    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break", "mtime": 1755732670.6391072, "terms": ["def", "_extract_text", "self", "file_path", "path", "str", "with", "pdfplumber", "open", "file_path", "as", "pdf", "text", "join", "page", "extract_text", "for", "page", "in", "pdf", "pages", "if", "page", "extract_text", "return", "text", "def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break"]}
{"chunk_id": "f72963c14baf6a3fbf7371f6", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 76, "end_line": 155, "content": "    def _extract_bank_name(self, text: str) -> str:\n        for pattern in self.bank_name_patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(0)\n        return \"Unknown Bank\"\n\n    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_bank_name", "self", "text", "str", "str", "for", "pattern", "in", "self", "bank_name_patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "unknown", "bank", "def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns"]}
{"chunk_id": "c386d0a1dd02ca46b1998e3f", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 83, "end_line": 162, "content": "    def _extract_account_number(self, text: str) -> str:\n        # Basic pattern, but could be adapted for European format if needed\n        patterns = [\n            r'account[:\\s]+(\\w+[-/]?\\w*)',\n            r'acc[:\\s]+(\\w+[-/]?\\w*)',\n            r'iban[:\\s]+([A-Z0-9]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return match.group(1)\n        return \"\"\n\n    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)", "mtime": 1755732670.6391072, "terms": ["def", "_extract_account_number", "self", "text", "str", "str", "basic", "pattern", "but", "could", "be", "adapted", "for", "european", "format", "if", "needed", "patterns", "account", "acc", "iban", "z0", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "match", "group", "return", "def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group"]}
{"chunk_id": "650b8eb517b343f13897bede", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 96, "end_line": 175, "content": "    def _extract_period(self, text: str) -> Optional[Tuple[datetime, datetime]]:\n        # European date period pattern, e.g. \"Period: 01.01.2024 - 31.01.2024\"\n        period_pattern = r'period[:\\s]+(\\d{2}[./-]\\d{2}[./-]\\d{4})\\s*[-a√†]\\s*(\\d{2}[./-]\\d{2}[./-]\\d{4})'\n        match = re.search(period_pattern, text, re.IGNORECASE)\n        if match:\n            fmt_guess = '%d.%m.%Y' if '.' in match.group(1) else '%d/%m/%Y'\n            try:\n                start = datetime.strptime(match.group(1), fmt_guess)\n                end = datetime.strptime(match.group(2), fmt_guess)\n                return start, end\n            except Exception:\n                return None\n        return None\n\n    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n", "mtime": 1755732670.6391072, "terms": ["def", "_extract_period", "self", "text", "str", "optional", "tuple", "datetime", "datetime", "european", "date", "period", "pattern", "period", "period_pattern", "period", "match", "re", "search", "period_pattern", "text", "re", "ignorecase", "if", "match", "fmt_guess", "if", "in", "match", "group", "else", "try", "start", "datetime", "strptime", "match", "group", "fmt_guess", "end", "datetime", "strptime", "match", "group", "fmt_guess", "return", "start", "end", "except", "exception", "return", "none", "return", "none", "def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str"]}
{"chunk_id": "d3892b8697bf77be238bf4d9", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 110, "end_line": 189, "content": "    def _extract_initial_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'opening\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+start[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_initial_balance", "self", "text", "str", "decimal", "patterns", "rf", "opening", "balance", "self", "currency", "rf", "balance", "start", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try"]}
{"chunk_id": "ffd97ccb0770949e3b7cc75c", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 121, "end_line": 200, "content": "    def _extract_final_balance(self, text: str) -> Decimal:\n        patterns = [\n            rf'closing\\s+balance[:\\s]+{self.currency}\\s*([\\d.,]+)',\n            rf'balance\\s+end[:\\s]+{self.currency}\\s*([\\d.,]+)',\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return self._parse_amount(match.group(1))\n        return Decimal('0.00')\n\n    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,", "mtime": 1755732670.6391072, "terms": ["def", "_extract_final_balance", "self", "text", "str", "decimal", "patterns", "rf", "closing", "balance", "self", "currency", "rf", "balance", "end", "self", "currency", "for", "pattern", "in", "patterns", "match", "re", "search", "pattern", "text", "re", "ignorecase", "if", "match", "return", "self", "_parse_amount", "match", "group", "return", "decimal", "def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj"]}
{"chunk_id": "c7492d2c8d9f68e0d7dfb54c", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 132, "end_line": 211, "content": "    def _extract_transactions(self, text: str) -> List[Transaction]:\n        transactions = []\n        lines = text.splitlines()\n        for line in lines:\n            transaction = self._parse_transaction_line(line)\n            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:", "mtime": 1755732670.6391072, "terms": ["def", "_extract_transactions", "self", "text", "str", "list", "transaction", "transactions", "lines", "text", "splitlines", "for", "line", "in", "lines", "transaction", "self", "_parse_transaction_line", "line", "if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception"]}
{"chunk_id": "857d8a633a79b1281a1c5834", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 137, "end_line": 212, "content": "            if transaction:\n                transactions.append(transaction)\n        return transactions\n\n    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["if", "transaction", "transactions", "append", "transaction", "return", "transactions", "def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "7e8aedb547e5325e6f6194e4", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 141, "end_line": 212, "content": "    def _parse_transaction_line(self, line: str) -> Optional[Transaction]:\n        # Skip short lines or headers\n        if len(line.strip()) < 10:\n            return None\n\n        date_match = None\n        for pattern in self.date_patterns:\n            date_match = re.search(pattern, line)\n            if date_match:\n                break\n        if not date_match:\n            return None\n\n        amount_match = None\n        for pattern in self.amount_patterns:\n            amount_match = re.search(pattern, line)\n            if amount_match:\n                break\n        if not amount_match:\n            return None\n\n        amount_str = amount_match.group(0)\n\n        # Check for negative sign within 3 characters before the amount in the line\n        start_index = amount_match.start()\n        negative_sign_found = False\n        for i in range(max(0, start_index - 3), start_index):\n            if line[i] == '-':\n                negative_sign_found = True\n                break\n        if negative_sign_found:\n            amount_str = '-' + amount_str\n\n        amount = self._parse_amount(amount_str)\n\n        # Extract description (remove date and amount)\n        description = line.replace(date_match.group(0), '').replace(amount_match.group(0), '').strip()\n\n        # Determine credit or debit based on sign of amount\n        if amount < 0:\n            transaction_type = TransactionType.DEBIT\n        else:\n            transaction_type = TransactionType.CREDIT\n\n        # Parse date to datetime object\n        try:\n            date_str = date_match.group(0)\n            for fmt in ['%d/%m/%Y', '%d-%m-%Y', '%Y-%m-%d']:\n                try:\n                    date_obj = datetime.strptime(date_str, fmt)\n                    break\n                except ValueError:\n                    continue\n            else:\n                date_obj = datetime.now()\n        except Exception:\n            date_obj = datetime.now()\n\n        return Transaction(\n            date=date_obj,\n            description=description,\n            amount=amount,  # Preserve the sign of the amount\n            type=transaction_type\n        )\n\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_transaction_line", "self", "line", "str", "optional", "transaction", "skip", "short", "lines", "or", "headers", "if", "len", "line", "strip", "return", "none", "date_match", "none", "for", "pattern", "in", "self", "date_patterns", "date_match", "re", "search", "pattern", "line", "if", "date_match", "break", "if", "not", "date_match", "return", "none", "amount_match", "none", "for", "pattern", "in", "self", "amount_patterns", "amount_match", "re", "search", "pattern", "line", "if", "amount_match", "break", "if", "not", "amount_match", "return", "none", "amount_str", "amount_match", "group", "check", "for", "negative", "sign", "within", "characters", "before", "the", "amount", "in", "the", "line", "start_index", "amount_match", "start", "negative_sign_found", "false", "for", "in", "range", "max", "start_index", "start_index", "if", "line", "negative_sign_found", "true", "break", "if", "negative_sign_found", "amount_str", "amount_str", "amount", "self", "_parse_amount", "amount_str", "extract", "description", "remove", "date", "and", "amount", "description", "line", "replace", "date_match", "group", "replace", "amount_match", "group", "strip", "determine", "credit", "or", "debit", "based", "on", "sign", "of", "amount", "if", "amount", "transaction_type", "transactiontype", "debit", "else", "transaction_type", "transactiontype", "credit", "parse", "date", "to", "datetime", "object", "try", "date_str", "date_match", "group", "for", "fmt", "in", "try", "date_obj", "datetime", "strptime", "date_str", "fmt", "break", "except", "valueerror", "continue", "else", "date_obj", "datetime", "now", "except", "exception", "date_obj", "datetime", "now", "return", "transaction", "date", "date_obj", "description", "description", "amount", "amount", "preserve", "the", "sign", "of", "the", "amount", "type", "transaction_type", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "886425a3229408866fd5171a", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 205, "end_line": 212, "content": "\n    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "6e6ad31219a555542490abbe", "file_path": "src/infrastructure/readers/pdf_reader.py", "start_line": 206, "end_line": 212, "content": "    def _parse_amount(self, amount_str: str) -> Decimal:\n        # Normalize European format: remove thousand separator '.', replace decimal ',' with '.'\n        normalized = amount_str.replace('.', '').replace(',', '.').replace(' ', '')\n        try:\n            return Decimal(normalized)\n        except Exception:\n            return Decimal('0.0')", "mtime": 1755732670.6391072, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "normalize", "european", "format", "remove", "thousand", "separator", "replace", "decimal", "with", "normalized", "amount_str", "replace", "replace", "replace", "try", "return", "decimal", "normalized", "except", "exception", "return", "decimal"]}
{"chunk_id": "45a1a79d9b12cdcc33deaf8c", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o de leitor de extratos em Excel.\n\"\"\"\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List\nimport pandas as pd\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass ExcelStatementReader(StatementReader):\n    \"\"\"Leitor de extratos banc√°rios em formato Excel.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados est√£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:", "mtime": 1756149205.9433913, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "excel", "import", "re", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "import", "pandas", "as", "pd", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "excelstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "excel", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols"]}
{"chunk_id": "e1f205c036b1affc591a6df0", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 17, "end_line": 96, "content": "class ExcelStatementReader(StatementReader):\n    \"\"\"Leitor de extratos banc√°rios em formato Excel.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados est√£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")", "mtime": 1756149205.9433913, "terms": ["class", "excelstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "excel", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel"]}
{"chunk_id": "b15eee4a8309b99690ebd347", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 20, "end_line": 99, "content": "    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados est√£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:", "mtime": 1756149205.9433913, "terms": ["def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try"]}
{"chunk_id": "80a85e027da04108f95aa195", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 23, "end_line": 102, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo Excel.\"\"\"\n        return file_path.suffix.lower() in ['.xlsx', '.xls']\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados est√£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)", "mtime": 1756149205.9433913, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "excel", "return", "file_path", "suffix", "lower", "in", "xlsx", "xls", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str"]}
{"chunk_id": "b1148243b65451d5067fc66d", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 27, "end_line": 106, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo Excel e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo Excel\n            excel_file = pd.ExcelFile(file_path)\n            \n            # Assume que os dados est√£o na primeira aba\n            df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])\n            \n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n            \n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo Excel: {str(e)}\")\n    \n    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n", "mtime": 1756149205.9433913, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "excel", "retorna", "um", "extrato", "try", "arquivo", "excel", "excel_file", "pd", "excelfile", "file_path", "assume", "que", "os", "dados", "est", "na", "primeira", "aba", "df", "pd", "read_excel", "file_path", "sheet_name", "excel_file", "sheet_names", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "excel", "str", "def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip"]}
{"chunk_id": "28ce5ce9bd2e054215d95bbf", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 62, "end_line": 141, "content": "    def _extract_transactions(self, df) -> List[Transaction]:\n        \"\"\"Extrai transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos Excel\n        date_col = None\n        description_col = None\n        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n\n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n\n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type\n                )\n\n                transactions.append(transaction)\n\n            except Exception:\n                # Ignora transa√ß√µes que n√£o podem ser processadas\n                continue\n\n        return transactions\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:", "mtime": 1756149205.9433913, "terms": ["def", "_extract_transactions", "self", "df", "list", "transaction", "extrai", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "excel", "date_col", "none", "description_col", "none", "amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "transactions", "append", "transaction", "except", "exception", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try"]}
{"chunk_id": "418bd1f219ba4bd3e77a083c", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 69, "end_line": 148, "content": "        amount_col = None\n\n        # Normaliza os nomes das colunas para facilitar a busca\n        normalized_columns = [str(col).strip().lower() for col in df.columns]\n\n        # Define listas de poss√≠veis nomes para cada coluna\n        possible_date_cols = ['data mov.', 'data mov', 'data', 'date', 'data transacao', 'transaction date']\n        possible_description_cols = ['descricao', 'description', 'descri√ß√£o']\n        possible_amount_cols = ['valor', 'amount', 'value', 'montante']\n\n        # Encontra as colunas correspondentes\n        for col_name in possible_date_cols:\n            if col_name in normalized_columns:\n                date_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_description_cols:\n            if col_name in normalized_columns:\n                description_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        for col_name in possible_amount_cols:\n            if col_name in normalized_columns:\n                amount_col = df.columns[normalized_columns.index(col_name)]\n                break\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no Excel\")\n\n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n\n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n\n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n\n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type\n                )\n\n                transactions.append(transaction)\n\n            except Exception:\n                # Ignora transa√ß√µes que n√£o podem ser processadas\n                continue\n\n        return transactions\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"N√£o foi poss√≠vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:", "mtime": 1756149205.9433913, "terms": ["amount_col", "none", "normaliza", "os", "nomes", "das", "colunas", "para", "facilitar", "busca", "normalized_columns", "str", "col", "strip", "lower", "for", "col", "in", "df", "columns", "define", "listas", "de", "poss", "veis", "nomes", "para", "cada", "coluna", "possible_date_cols", "data", "mov", "data", "mov", "data", "date", "data", "transacao", "transaction", "date", "possible_description_cols", "descricao", "description", "descri", "possible_amount_cols", "valor", "amount", "value", "montante", "encontra", "as", "colunas", "correspondentes", "for", "col_name", "in", "possible_date_cols", "if", "col_name", "in", "normalized_columns", "date_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_description_cols", "if", "col_name", "in", "normalized_columns", "description_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "for", "col_name", "in", "possible_amount_cols", "if", "col_name", "in", "normalized_columns", "amount_col", "df", "columns", "normalized_columns", "index", "col_name", "break", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "excel", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "transactions", "append", "transaction", "except", "exception", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal"]}
{"chunk_id": "2d41aa89cecc394a08a4cedd", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 127, "end_line": 206, "content": "    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Formatos de data comuns\n        date_formats = [\n            '%d/%m/%Y',\n            '%d/%m/%y',\n            '%d-%m-%Y',\n            '%d-%m-%y',\n            '%Y-%m-%d',\n            '%d.%m.%Y',\n            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"N√£o foi poss√≠vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui v√≠rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres n√£o num√©ricos exceto ponto, sinal de menos e d√≠gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padr√µes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura por padr√µes de n√∫mero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # Padr√£o gen√©rico para n√∫meros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):", "mtime": 1756149205.9433913, "terms": ["def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "formatos", "de", "data", "comuns", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value"]}
{"chunk_id": "e7b4a89c86c0c027c3be5156", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 137, "end_line": 216, "content": "            '%d.%m.%y'\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n        \n        raise ValueError(f\"N√£o foi poss√≠vel parsear a data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui v√≠rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres n√£o num√©ricos exceto ponto, sinal de menos e d√≠gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padr√µes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura por padr√µes de n√∫mero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # Padr√£o gen√©rico para n√∫meros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    ", "mtime": 1756149205.9433913, "terms": ["for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "raise", "valueerror", "foi", "poss", "vel", "parsear", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions"]}
{"chunk_id": "d2e4a33489301d8d5e57a2c5", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 148, "end_line": 227, "content": "    def _parse_amount(self, amount_str: str) -> Decimal:\n        \"\"\"Faz parsing de uma string de valor.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        amount_str = amount_str.strip().replace(\" \", \"\")\n        \n        # Substitui v√≠rgula por ponto para decimais\n        amount_str = amount_str.replace(\",\", \".\")\n        \n        # Remove caracteres n√£o num√©ricos exceto ponto, sinal de menos e d√≠gitos\n        amount_str = re.sub(r'[^\\d.-]', '', amount_str)\n        \n        if amount_str == '':\n            return Decimal('0.00')\n            \n        return Decimal(amount_str)\n    \n    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padr√µes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura por padr√µes de n√∫mero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # Padr√£o gen√©rico para n√∫meros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:", "mtime": 1756149205.9433913, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "decimal", "faz", "parsing", "de", "uma", "string", "de", "valor", "remove", "espa", "os", "caracteres", "especiais", "amount_str", "amount_str", "strip", "replace", "substitui", "rgula", "por", "ponto", "para", "decimais", "amount_str", "amount_str", "replace", "remove", "caracteres", "num", "ricos", "exceto", "ponto", "sinal", "de", "menos", "gitos", "amount_str", "re", "sub", "amount_str", "if", "amount_str", "return", "decimal", "return", "decimal", "amount_str", "def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row"]}
{"chunk_id": "5386d83cb7fbdc0471ac4df0", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 164, "end_line": 243, "content": "    def _extract_bank_name(self, df) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura por padr√µes de nome de banco\n        bank_patterns = [\n            r'BPI.*',\n            r'Caixa.*',\n            r'Santander.*',\n            r'BBVA.*',\n            r'ING.*',\n            r'Deutsche.*',\n            r'BNP.*',\n            r'Credit.*',\n            r'UniCredit.*',\n            r'Intesa.*',\n            r'Sabadell.*',\n            r'CGD.*',\n            r'BCP.*',\n            r'Novo.*'\n        ]\n        \n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                for pattern in bank_patterns:\n                    if re.search(pattern, cell_value, re.IGNORECASE):\n                        return re.search(pattern, cell_value, re.IGNORECASE).group()\n        \n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura por padr√µes de n√∫mero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # Padr√£o gen√©rico para n√∫meros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())", "mtime": 1756149205.9433913, "terms": ["def", "_extract_bank_name", "self", "df", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "por", "padr", "es", "de", "nome", "de", "banco", "bank_patterns", "bpi", "caixa", "santander", "bbva", "ing", "deutsche", "bnp", "credit", "unicredit", "intesa", "sabadell", "cgd", "bcp", "novo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "for", "pattern", "in", "bank_patterns", "if", "re", "search", "pattern", "cell_value", "re", "ignorecase", "return", "re", "search", "pattern", "cell_value", "re", "ignorecase", "group", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group"]}
{"chunk_id": "ff4ce092b154e0c9f46a0746", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 195, "end_line": 265, "content": "    def _extract_account_number(self, df) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura por padr√µes de n√∫mero de conta\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                # Padr√£o gen√©rico para n√∫meros de conta\n                if re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value):\n                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_account_number", "self", "df", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "por", "padr", "es", "de", "mero", "de", "conta", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "padr", "gen", "rico", "para", "meros", "de", "conta", "if", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "ec3e2c2fe20176ce668403e3", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 205, "end_line": 265, "content": "                    return re.search(r'\\b\\d{4}\\s*\\d{4}\\s*\\d{4}\\s*\\d{4}\\b', cell_value).group()\n                elif re.search(r'\\b\\d{20,}\\b', cell_value):\n                    return re.search(r'\\b\\d{20,}\\b', cell_value).group()\n        \n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["return", "re", "search", "cell_value", "group", "elif", "re", "search", "cell_value", "return", "re", "search", "cell_value", "group", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "9b336731716c4c4435ea13a4", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 211, "end_line": 265, "content": "    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data inicial das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "inicial", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "b7976bf597ba8e352032140b", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 217, "end_line": 265, "content": "    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data final das transa√ß√µes.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "final", "das", "transa", "es", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "4a5a453ce6fe11ad887ee755", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 223, "end_line": 265, "content": "    def _extract_initial_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo\n        for idx, row in df.iterrows():\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value and 'Dispon' in cell_value:\n                    # Procura o valor do saldo nas pr√≥ximas linhas\n                    for search_idx in range(idx + 1, min(idx + 5, len(df))):\n                        search_row = df.iloc[search_idx]\n                        for search_col in search_row:\n                            if pd.isna(search_col):\n                                continue\n                            search_value = str(search_col)\n                            # Padr√£o para valores monet√°rios\n                            match = re.search(r'[\\d.,]+', search_value.replace(\" \", \"\"))\n                            if match:\n                                try:\n                                    return self._parse_amount(match.group())\n                                except:\n                                    continue\n        return Decimal('0.00')\n    \n    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_initial_balance", "self", "df", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "for", "idx", "row", "in", "df", "iterrows", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "and", "dispon", "in", "cell_value", "procura", "valor", "do", "saldo", "nas", "pr", "ximas", "linhas", "for", "search_idx", "in", "range", "idx", "min", "idx", "len", "df", "search_row", "df", "iloc", "search_idx", "for", "search_col", "in", "search_row", "if", "pd", "isna", "search_col", "continue", "search_value", "str", "search_col", "padr", "para", "valores", "monet", "rios", "match", "re", "search", "search_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "d01d9636e620fad3f648f017", "file_path": "src/infrastructure/readers/excel_reader.py", "start_line": 248, "end_line": 265, "content": "    def _extract_final_balance(self, df) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura por padr√µes de saldo final nas √∫ltimas linhas\n        for idx in range(len(df) - 1, max(0, len(df) - 10), -1):\n            row = df.iloc[idx]\n            for col in row:\n                if pd.isna(col):\n                    continue\n                cell_value = str(col)\n                if 'Saldo' in cell_value:\n                    # Procura o valor do saldo\n                    match = re.search(r'[\\d.,]+', cell_value.replace(\" \", \"\"))\n                    if match:\n                        try:\n                            return self._parse_amount(match.group())\n                        except:\n                            continue\n        return Decimal('0.00')", "mtime": 1756149205.9433913, "terms": ["def", "_extract_final_balance", "self", "df", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "por", "padr", "es", "de", "saldo", "final", "nas", "ltimas", "linhas", "for", "idx", "in", "range", "len", "df", "max", "len", "df", "row", "df", "iloc", "idx", "for", "col", "in", "row", "if", "pd", "isna", "col", "continue", "cell_value", "str", "col", "if", "saldo", "in", "cell_value", "procura", "valor", "do", "saldo", "match", "re", "search", "cell_value", "replace", "if", "match", "try", "return", "self", "_parse_amount", "match", "group", "except", "continue", "return", "decimal"]}
{"chunk_id": "d422869d4bf3b9df32d045c2", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o de leitor de extratos em CSV.\n\"\"\"\nimport re\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom pathlib import Path\nfrom typing import List\nimport pandas as pd\n\nfrom src.domain.models import BankStatement, Transaction, TransactionType\nfrom src.domain.interfaces import StatementReader\nfrom src.domain.exceptions import FileNotSupportedError, ParsingError\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass CSVStatementReader(StatementReader):\n    \"\"\"Leitor de extratos banc√°rios em formato CSV.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o", "mtime": 1756149193.6473725, "terms": ["implementa", "de", "leitor", "de", "extratos", "em", "csv", "import", "re", "from", "datetime", "import", "datetime", "from", "decimal", "import", "decimal", "from", "pathlib", "import", "path", "from", "typing", "import", "list", "import", "pandas", "as", "pd", "from", "src", "domain", "models", "import", "bankstatement", "transaction", "transactiontype", "from", "src", "domain", "interfaces", "import", "statementreader", "from", "src", "domain", "exceptions", "import", "filenotsupportederror", "parsingerror", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "csvstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "csv", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri"]}
{"chunk_id": "0fdf58c9f5435fb96712c80e", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 17, "end_line": 96, "content": "class CSVStatementReader(StatementReader):\n    \"\"\"Leitor de extratos banc√°rios em formato CSV.\"\"\"\n    \n    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,", "mtime": 1756149193.6473725, "terms": ["class", "csvstatementreader", "statementreader", "leitor", "de", "extratos", "banc", "rios", "em", "formato", "csv", "def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description"]}
{"chunk_id": "cd9c5fde7b0e6a4ae8890351", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 20, "end_line": 99, "content": "    def __init__(self):\n        self.currency = \"EUR\"  # Ser√° detectado automaticamente\n        \n    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after", "mtime": 1756149193.6473725, "terms": ["def", "__init__", "self", "self", "currency", "eur", "ser", "detectado", "automaticamente", "def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after"]}
{"chunk_id": "8cc38c9e27567b885c430706", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 23, "end_line": 102, "content": "    def can_read(self, file_path: Path) -> bool:\n        \"\"\"Verifica se pode ler o arquivo CSV.\"\"\"\n        if isinstance(file_path, str):\n            file_path = Path(file_path)\n        return file_path.suffix.lower() == '.csv'\n    \n    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)", "mtime": 1756149193.6473725, "terms": ["def", "can_read", "self", "file_path", "path", "bool", "verifica", "se", "pode", "ler", "arquivo", "csv", "if", "isinstance", "file_path", "str", "file_path", "path", "file_path", "return", "file_path", "suffix", "lower", "csv", "def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction"]}
{"chunk_id": "1278e0950053031262cb84d1", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 29, "end_line": 108, "content": "    def read(self, file_path: Path) -> BankStatement:\n        \"\"\"L√™ o arquivo CSV e retorna um extrato.\"\"\"\n        try:\n            # L√™ o arquivo CSV\n            df = pd.read_csv(file_path, encoding='utf-8')\n\n            # Detecta a moeda automaticamente\n            detected_currency = CurrencyUtils.extract_currency_from_dataframe(df)\n            if not detected_currency:\n                detected_currency = \"EUR\"\n            self.currency = detected_currency\n\n            # Extrai as transa√ß√µes do DataFrame\n            transactions = self._extract_transactions(df)\n\n            # Cria o extrato banc√°rio\n            statement = BankStatement(\n                bank_name=self._extract_bank_name(df),\n                account_number=self._extract_account_number(df),\n                period_start=self._extract_start_date(transactions),\n                period_end=self._extract_end_date(transactions),\n                initial_balance=self._extract_initial_balance(df),\n                final_balance=self._extract_final_balance(df),\n                currency=self.currency,  # Usa a moeda detectada\n                transactions=transactions\n            )\n\n            return statement\n\n        except Exception as e:\n            raise ParsingError(f\"Erro ao ler o arquivo CSV: {str(e)}\")\n\n    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transa√ß√µes que n√£o podem ser processadas\n                continue\n                \n        return transactions", "mtime": 1756149193.6473725, "terms": ["def", "read", "self", "file_path", "path", "bankstatement", "arquivo", "csv", "retorna", "um", "extrato", "try", "arquivo", "csv", "df", "pd", "read_csv", "file_path", "encoding", "utf", "detecta", "moeda", "automaticamente", "detected_currency", "currencyutils", "extract_currency_from_dataframe", "df", "if", "not", "detected_currency", "detected_currency", "eur", "self", "currency", "detected_currency", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "self", "_extract_transactions", "df", "cria", "extrato", "banc", "rio", "statement", "bankstatement", "bank_name", "self", "_extract_bank_name", "df", "account_number", "self", "_extract_account_number", "df", "period_start", "self", "_extract_start_date", "transactions", "period_end", "self", "_extract_end_date", "transactions", "initial_balance", "self", "_extract_initial_balance", "df", "final_balance", "self", "_extract_final_balance", "df", "currency", "self", "currency", "usa", "moeda", "detectada", "transactions", "transactions", "return", "statement", "except", "exception", "as", "raise", "parsingerror", "erro", "ao", "ler", "arquivo", "csv", "str", "def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions"]}
{"chunk_id": "0bae68605443c9868e7fd246", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 61, "end_line": 140, "content": "    def _extract_transactions(self, df: pd.DataFrame) -> List[Transaction]:\n        \"\"\"Extrai as transa√ß√µes do DataFrame.\"\"\"\n        transactions = []\n\n        # Procura colunas comuns em extratos CSV\n        date_col = self._find_column(df, ['data', 'date', 'data transacao', 'transaction date'])\n        description_col = self._find_column(df, ['descricao', 'description', 'descri√ß√£o'])\n        amount_col = self._find_column(df, ['valor', 'amount', 'value', 'montante'])\n        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transa√ß√µes que n√£o podem ser processadas\n                continue\n                \n        return transactions\n    \n    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para compara√ß√£o (remove acentos e converte para min√∫sculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para min√∫sculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:", "mtime": 1756149193.6473725, "terms": ["def", "_extract_transactions", "self", "df", "pd", "dataframe", "list", "transaction", "extrai", "as", "transa", "es", "do", "dataframe", "transactions", "procura", "colunas", "comuns", "em", "extratos", "csv", "date_col", "self", "_find_column", "df", "data", "date", "data", "transacao", "transaction", "date", "description_col", "self", "_find_column", "df", "descricao", "description", "descri", "amount_col", "self", "_find_column", "df", "valor", "amount", "value", "montante", "balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats"]}
{"chunk_id": "fd0f72348b0ccc40362a9996", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 69, "end_line": 148, "content": "        balance_col = self._find_column(df, ['saldo', 'balance', 'saldo ap√≥s', 'balance after'])\n\n        if not date_col or not description_col or not amount_col:\n            raise ParsingError(\"N√£o foi poss√≠vel identificar as colunas necess√°rias no CSV\")\n        \n        for _, row in df.iterrows():\n            try:\n                # Extrai e converte a data\n                date_str = str(row[date_col]).strip()\n                date = self._parse_date(date_str)\n                \n                # Extrai a descri√ß√£o\n                description = str(row[description_col]).strip()\n                \n                # Extrai e converte o valor\n                amount_str = str(row[amount_col]).strip()\n                amount, transaction_type = self._parse_amount(amount_str)\n                \n                # Extrai o saldo ap√≥s a transa√ß√£o, se dispon√≠vel\n                balance_after = None\n                if balance_col and balance_col in df.columns:\n                    balance_str = str(row[balance_col]).strip()\n                    balance_after = self._parse_balance(balance_str)\n                \n                # Cria a transa√ß√£o\n                transaction = Transaction(\n                    date=date,\n                    description=description,\n                    amount=amount,\n                    type=transaction_type,\n                    balance_after=balance_after\n                )\n                \n                transactions.append(transaction)\n                \n            except Exception as e:\n                # Ignora transa√ß√µes que n√£o podem ser processadas\n                continue\n                \n        return transactions\n    \n    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para compara√ß√£o (remove acentos e converte para min√∫sculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para min√∫sculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser autom√°tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()", "mtime": 1756149193.6473725, "terms": ["balance_col", "self", "_find_column", "df", "saldo", "balance", "saldo", "ap", "balance", "after", "if", "not", "date_col", "or", "not", "description_col", "or", "not", "amount_col", "raise", "parsingerror", "foi", "poss", "vel", "identificar", "as", "colunas", "necess", "rias", "no", "csv", "for", "row", "in", "df", "iterrows", "try", "extrai", "converte", "data", "date_str", "str", "row", "date_col", "strip", "date", "self", "_parse_date", "date_str", "extrai", "descri", "description", "str", "row", "description_col", "strip", "extrai", "converte", "valor", "amount_str", "str", "row", "amount_col", "strip", "amount", "transaction_type", "self", "_parse_amount", "amount_str", "extrai", "saldo", "ap", "transa", "se", "dispon", "vel", "balance_after", "none", "if", "balance_col", "and", "balance_col", "in", "df", "columns", "balance_str", "str", "row", "balance_col", "strip", "balance_after", "self", "_parse_balance", "balance_str", "cria", "transa", "transaction", "transaction", "date", "date", "description", "description", "amount", "amount", "type", "transaction_type", "balance_after", "balance_after", "transactions", "append", "transaction", "except", "exception", "as", "ignora", "transa", "es", "que", "podem", "ser", "processadas", "continue", "return", "transactions", "def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime"]}
{"chunk_id": "8c45954875717ff61c771342", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 110, "end_line": 189, "content": "    def _find_column(self, df: pd.DataFrame, possible_names: List[str]) -> str:\n        \"\"\"Encontra uma coluna pelo nome (case insensitive).\"\"\"\n        columns_lower = [self._normalize_text(col) for col in df.columns]\n        for name in possible_names:\n            normalized_name = self._normalize_text(name)\n            if normalized_name in columns_lower:\n                # Retorna o nome original da coluna\n                return df.columns[columns_lower.index(normalized_name)]\n        return None\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para compara√ß√£o (remove acentos e converte para min√∫sculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para min√∫sculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser autom√°tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monet√°rio e determina o tipo de transa√ß√£o.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se √© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui v√≠rgula por ponto se necess√°rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transa√ß√£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"", "mtime": 1756149193.6473725, "terms": ["def", "_find_column", "self", "df", "pd", "dataframe", "possible_names", "list", "str", "str", "encontra", "uma", "coluna", "pelo", "nome", "case", "insensitive", "columns_lower", "self", "_normalize_text", "col", "for", "col", "in", "df", "columns", "for", "name", "in", "possible_names", "normalized_name", "self", "_normalize_text", "name", "if", "normalized_name", "in", "columns_lower", "retorna", "nome", "original", "da", "coluna", "return", "df", "columns", "columns_lower", "index", "normalized_name", "return", "none", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo"]}
{"chunk_id": "cfcbdfd774cf5c083777568b", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 120, "end_line": 199, "content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto para compara√ß√£o (remove acentos e converte para min√∫sculo).\"\"\"\n        import unicodedata\n        # Remove acentos\n        text = unicodedata.normalize('NFD', text)\n        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n        # Converte para min√∫sculo\n        return text.lower()\n    \n    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser autom√°tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monet√°rio e determina o tipo de transa√ß√£o.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se √© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui v√≠rgula por ponto se necess√°rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transa√ß√£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espa√ßos e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal", "mtime": 1756149193.6473725, "terms": ["def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "para", "compara", "remove", "acentos", "converte", "para", "min", "sculo", "import", "unicodedata", "remove", "acentos", "text", "unicodedata", "normalize", "nfd", "text", "text", "join", "for", "in", "text", "if", "unicodedata", "category", "mn", "converte", "para", "min", "sculo", "return", "text", "lower", "def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal"]}
{"chunk_id": "41affa5b9f4c94187b9f603a", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 129, "end_line": 208, "content": "    def _parse_date(self, date_str: str) -> datetime:\n        \"\"\"Faz parsing de uma string de data.\"\"\"\n        # Tenta diferentes formatos de data\n        date_formats = [\n            \"%d/%m/%Y\",\n            \"%d-%m-%Y\",\n            \"%Y-%m-%d\",\n            \"%d/%m/%y\",\n            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser autom√°tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monet√°rio e determina o tipo de transa√ß√£o.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se √© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui v√≠rgula por ponto se necess√°rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transa√ß√£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espa√ßos e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)", "mtime": 1756149193.6473725, "terms": ["def", "_parse_date", "self", "date_str", "str", "datetime", "faz", "parsing", "de", "uma", "string", "de", "data", "tenta", "diferentes", "formatos", "de", "data", "date_formats", "for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned"]}
{"chunk_id": "706ca28da05a1c0fcefbe800", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 137, "end_line": 216, "content": "            \"%d-%m-%y\"\n        ]\n        \n        for fmt in date_formats:\n            try:\n                return datetime.strptime(date_str, fmt)\n            except ValueError:\n                continue\n                \n        # Se nenhum formato funcionar, tenta usar o parser autom√°tico\n        try:\n            return pd.to_datetime(date_str).to_pydatetime()\n        except:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing da data: {date_str}\")\n    \n    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monet√°rio e determina o tipo de transa√ß√£o.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se √© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui v√≠rgula por ponto se necess√°rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transa√ß√£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espa√ßos e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabe√ßalhos\n        return \"Banco Desconhecido\"\n    ", "mtime": 1756149193.6473725, "terms": ["for", "fmt", "in", "date_formats", "try", "return", "datetime", "strptime", "date_str", "fmt", "except", "valueerror", "continue", "se", "nenhum", "formato", "funcionar", "tenta", "usar", "parser", "autom", "tico", "try", "return", "pd", "to_datetime", "date_str", "to_pydatetime", "except", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "da", "data", "date_str", "def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido"]}
{"chunk_id": "0fdd627c64322a85fda9c062", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 152, "end_line": 231, "content": "    def _parse_amount(self, amount_str: str) -> tuple:\n        \"\"\"Faz parsing de um valor monet√°rio e determina o tipo de transa√ß√£o.\"\"\"\n        # Remove espa√ßos e caracteres especiais\n        cleaned = amount_str.strip().replace(' ', '')\n        \n        # Verifica se √© um valor negativo\n        is_negative = False\n        if cleaned.startswith('-') or cleaned.startswith('(') and cleaned.endswith(')'):\n            is_negative = True\n            cleaned = cleaned.replace('-', '').replace('(', '').replace(')', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal (substitui v√≠rgula por ponto se necess√°rio)\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            amount = Decimal(cleaned)\n            if is_negative:\n                amount = -amount\n                \n            # Determina o tipo de transa√ß√£o\n            transaction_type = TransactionType.CREDIT if amount >= 0 else TransactionType.DEBIT\n            amount = abs(amount)\n            \n            return amount, transaction_type\n            \n        except Exception as e:\n            raise ParsingError(f\"N√£o foi poss√≠vel fazer parsing do valor: {amount_str}\")\n    \n    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espa√ßos e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabe√ßalhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o n√∫mero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'n√∫mero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor n√£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:", "mtime": 1756149193.6473725, "terms": ["def", "_parse_amount", "self", "amount_str", "str", "tuple", "faz", "parsing", "de", "um", "valor", "monet", "rio", "determina", "tipo", "de", "transa", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "amount_str", "strip", "replace", "verifica", "se", "um", "valor", "negativo", "is_negative", "false", "if", "cleaned", "startswith", "or", "cleaned", "startswith", "and", "cleaned", "endswith", "is_negative", "true", "cleaned", "cleaned", "replace", "replace", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "substitui", "rgula", "por", "ponto", "se", "necess", "rio", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "amount", "decimal", "cleaned", "if", "is_negative", "amount", "amount", "determina", "tipo", "de", "transa", "transaction_type", "transactiontype", "credit", "if", "amount", "else", "transactiontype", "debit", "amount", "abs", "amount", "return", "amount", "transaction_type", "except", "exception", "as", "raise", "parsingerror", "foi", "poss", "vel", "fazer", "parsing", "do", "valor", "amount_str", "def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions"]}
{"chunk_id": "f1b03036683714c4a560f948", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 188, "end_line": 267, "content": "    def _parse_balance(self, balance_str: str) -> Decimal:\n        \"\"\"Faz parsing de um saldo.\"\"\"\n        if not balance_str or balance_str.lower() in ['nan', 'none', 'null', '']:\n            return None\n            \n        # Remove espa√ßos e caracteres especiais\n        cleaned = balance_str.strip().replace(' ', '')\n        \n        # Remove s√≠mbolos de moeda e outros caracteres n√£o num√©ricos\n        cleaned = re.sub(r'[^\\d,.\\-]', '', cleaned)\n        \n        # Normaliza o formato decimal\n        if ',' in cleaned and '.' in cleaned:\n            # Formato 1.234,56\n            cleaned = cleaned.replace('.', '').replace(',', '.')\n        elif ',' in cleaned:\n            # Formato 1234,56\n            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabe√ßalhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o n√∫mero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'n√∫mero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor n√£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)", "mtime": 1756149193.6473725, "terms": ["def", "_parse_balance", "self", "balance_str", "str", "decimal", "faz", "parsing", "de", "um", "saldo", "if", "not", "balance_str", "or", "balance_str", "lower", "in", "nan", "none", "null", "return", "none", "remove", "espa", "os", "caracteres", "especiais", "cleaned", "balance_str", "strip", "replace", "remove", "mbolos", "de", "moeda", "outros", "caracteres", "num", "ricos", "cleaned", "re", "sub", "cleaned", "normaliza", "formato", "decimal", "if", "in", "cleaned", "and", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "replace", "elif", "in", "cleaned", "formato", "cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type"]}
{"chunk_id": "67af249621160ad751f044f7", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 205, "end_line": 271, "content": "            cleaned = cleaned.replace(',', '.')\n        \n        try:\n            return Decimal(cleaned)\n        except:\n            return None\n    \n    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabe√ßalhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o n√∫mero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'n√∫mero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor n√£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["cleaned", "cleaned", "replace", "try", "return", "decimal", "cleaned", "except", "return", "none", "def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "1760e6faf3019d67a91a39f0", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 212, "end_line": 271, "content": "    def _extract_bank_name(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o nome do banco do DataFrame.\"\"\"\n        # Procura em metadados ou cabe√ßalhos\n        return \"Banco Desconhecido\"\n    \n    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o n√∫mero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'n√∫mero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor n√£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_bank_name", "self", "df", "pd", "dataframe", "str", "extrai", "nome", "do", "banco", "do", "dataframe", "procura", "em", "metadados", "ou", "cabe", "alhos", "return", "banco", "desconhecido", "def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "4ffe76cf130c5e4490f7d402", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 217, "end_line": 271, "content": "    def _extract_account_number(self, df: pd.DataFrame) -> str:\n        \"\"\"Extrai o n√∫mero da conta do DataFrame.\"\"\"\n        # Procura colunas que possam conter o n√∫mero da conta\n        account_col = self._find_column(df, ['conta', 'account', 'n√∫mero conta', 'account number'])\n        if account_col:\n            # Retorna o primeiro valor n√£o nulo\n            for _, row in df.iterrows():\n                value = str(row[account_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    return value\n        return \"Conta Desconhecida\"\n    \n    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_account_number", "self", "df", "pd", "dataframe", "str", "extrai", "mero", "da", "conta", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "mero", "da", "conta", "account_col", "self", "_find_column", "df", "conta", "account", "mero", "conta", "account", "number", "if", "account_col", "retorna", "primeiro", "valor", "nulo", "for", "row", "in", "df", "iterrows", "value", "str", "row", "account_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "return", "value", "return", "conta", "desconhecida", "def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "3dbd5bee9138a6949d2ace56", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 229, "end_line": 271, "content": "    def _extract_start_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de in√≠cio do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return min(t.date for t in transactions)\n    \n    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_start_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "in", "cio", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "min", "date", "for", "in", "transactions", "def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "7315831eddd00c8b746d111c", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 235, "end_line": 271, "content": "    def _extract_end_date(self, transactions: List[Transaction]) -> datetime:\n        \"\"\"Extrai a data de fim do per√≠odo.\"\"\"\n        if not transactions:\n            return datetime.now()\n        return max(t.date for t in transactions)\n    \n    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_end_date", "self", "transactions", "list", "transaction", "datetime", "extrai", "data", "de", "fim", "do", "per", "odo", "if", "not", "transactions", "return", "datetime", "now", "return", "max", "date", "for", "in", "transactions", "def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "2e31dea4bc03ca1d1bba6e29", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 241, "end_line": 271, "content": "    def _extract_initial_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo inicial do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo inicial\n        initial_balance_col = self._find_column(df, ['saldo inicial', 'initial balance', 'opening balance'])\n        if initial_balance_col:\n            for _, row in df.iterrows():\n                value = str(row[initial_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")\n    \n    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_initial_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "inicial", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "inicial", "initial_balance_col", "self", "_find_column", "df", "saldo", "inicial", "initial", "balance", "opening", "balance", "if", "initial_balance_col", "for", "row", "in", "df", "iterrows", "value", "str", "row", "initial_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal", "def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "8a2a358f3768bf9d20c90f5d", "file_path": "src/infrastructure/readers/csv_reader.py", "start_line": 256, "end_line": 271, "content": "    def _extract_final_balance(self, df: pd.DataFrame) -> Decimal:\n        \"\"\"Extrai o saldo final do DataFrame.\"\"\"\n        # Procura colunas que possam conter o saldo final\n        final_balance_col = self._find_column(df, ['saldo final', 'final balance', 'closing balance', 'saldo'])\n        if final_balance_col:\n            # Pega o √∫ltimo valor n√£o nulo\n            for i in range(len(df) - 1, -1, -1):\n                row = df.iloc[i]\n                value = str(row[final_balance_col]).strip()\n                if value and value.lower() not in ['nan', 'none', 'null', '']:\n                    try:\n                        _, _ = self._parse_amount(value)  # _parse_amount returns (amount, transaction_type)\n                        return Decimal(str(value).replace(',', '.'))\n                    except:\n                        continue\n        return Decimal(\"0.00\")", "mtime": 1756149193.6473725, "terms": ["def", "_extract_final_balance", "self", "df", "pd", "dataframe", "decimal", "extrai", "saldo", "final", "do", "dataframe", "procura", "colunas", "que", "possam", "conter", "saldo", "final", "final_balance_col", "self", "_find_column", "df", "saldo", "final", "final", "balance", "closing", "balance", "saldo", "if", "final_balance_col", "pega", "ltimo", "valor", "nulo", "for", "in", "range", "len", "df", "row", "df", "iloc", "value", "str", "row", "final_balance_col", "strip", "if", "value", "and", "value", "lower", "not", "in", "nan", "none", "null", "try", "self", "_parse_amount", "value", "_parse_amount", "returns", "amount", "transaction_type", "return", "decimal", "str", "value", "replace", "except", "continue", "return", "decimal"]}
{"chunk_id": "88c908446cd1c7eeac45779b", "file_path": "src/infrastructure/readers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote readers", "mtime": 1755104259.9851887, "terms": ["inicializa", "pacote", "readers"]}
{"chunk_id": "9b46726ecdae2fd6c2937b18", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o de categorizador simples de transa√ß√µes baseado em palavras-chave.\n\"\"\"\nimport re\nfrom typing import Dict, List\n\nfrom src.domain.models import Transaction, TransactionCategory\nfrom src.domain.interfaces import TransactionCategorizer\n\n\nclass KeywordCategorizer(TransactionCategorizer):\n    \"\"\"Categorizador de transa√ß√µes baseado em palavras-chave.\"\"\"\n    \n    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'a√ßougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'caf√©',\n                'cantina', 'refeitorio', 'almo√ßo', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            ", "mtime": 1755104396.4543626, "terms": ["implementa", "de", "categorizador", "simples", "de", "transa", "es", "baseado", "em", "palavras", "chave", "import", "re", "from", "typing", "import", "dict", "list", "from", "src", "domain", "models", "import", "transaction", "transactioncategory", "from", "src", "domain", "interfaces", "import", "transactioncategorizer", "class", "keywordcategorizer", "transactioncategorizer", "categorizador", "de", "transa", "es", "baseado", "em", "palavras", "chave", "def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq"]}
{"chunk_id": "bde37f22281763134615338e", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 11, "end_line": 90, "content": "class KeywordCategorizer(TransactionCategorizer):\n    \"\"\"Categorizador de transa√ß√µes baseado em palavras-chave.\"\"\"\n    \n    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'a√ßougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'caf√©',\n                'cantina', 'refeitorio', 'almo√ßo', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'", "mtime": 1755104396.4543626, "terms": ["class", "keywordcategorizer", "transactioncategorizer", "categorizador", "de", "transa", "es", "baseado", "em", "palavras", "chave", "def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio"]}
{"chunk_id": "e24f28be669915e2c9c33f20", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 14, "end_line": 93, "content": "    def __init__(self):\n        self.keywords = self._load_keywords()\n    \n    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'a√ßougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'caf√©',\n                'cantina', 'refeitorio', 'almo√ßo', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    ", "mtime": 1755104396.4543626, "terms": ["def", "__init__", "self", "self", "keywords", "self", "_load_keywords", "def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio"]}
{"chunk_id": "c2f233d8c1755b4803eeb7d6", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 17, "end_line": 96, "content": "    def _load_keywords(self) -> Dict[TransactionCategory, List[str]]:\n        \"\"\"Define palavras-chave para cada categoria.\"\"\"\n        return {\n            TransactionCategory.ALIMENTACAO: [\n                'restaurante', 'lanchonete', 'padaria', 'mercado', 'supermercado',\n                'a√ßougue', 'feira', 'ifood', 'uber eats', 'rappi', 'delivery',\n                'pizza', 'hamburguer', 'sushi', 'churrasco', 'bar', 'caf√©',\n                'cantina', 'refeitorio', 'almo√ßo', 'jantar', 'lanche'\n            ],\n            \n            TransactionCategory.TRANSPORTE: [\n                'uber', '99', 'cabify', 'taxi', 'combustivel', 'gasolina',\n                'etanol', 'alcool', 'posto', 'estacionamento', 'pedagio',\n                'onibus', 'metro', 'trem', 'passagem', 'viagem', 'rodoviaria',\n                'aeroporto', 'aviao', 'voo', 'locadora', 'aluguel carro'\n            ],\n            \n            TransactionCategory.MORADIA: [\n                'aluguel', 'condominio', 'iptu', 'luz', 'energia', 'agua',\n                'gas', 'internet', 'telefone', 'celular', 'vivo', 'claro',\n                'tim', 'oi', 'net', 'sky', 'manutencao', 'reforma', 'obra',\n                'material construcao', 'pintura', 'eletricista', 'encanador'\n            ],\n            \n            TransactionCategory.SAUDE: [\n                'farmacia', 'drogaria', 'medicamento', 'remedio', 'hospital',\n                'clinica', 'medico', 'consulta', 'exame', 'laboratorio',\n                'plano saude', 'unimed', 'amil', 'sulamerica', 'bradesco saude',\n                'dentista', 'ortodontia', 'fisioterapia', 'psicologia'\n            ],\n            \n            TransactionCategory.EDUCACAO: [\n                'escola', 'faculdade', 'universidade', 'curso', 'livro',\n                'livraria', 'apostila', 'material escolar', 'papelaria',\n                'mensalidade', 'matricula', 'udemy', 'coursera', 'alura',\n                'ingles', 'idioma', 'pos graduacao', 'mba', 'mestrado'\n            ],\n            \n            TransactionCategory.LAZER: [\n                'cinema', 'teatro', 'show', 'festa', 'evento', 'ingresso',\n                'netflix', 'spotify', 'amazon prime', 'disney', 'hbo',\n                'game', 'jogo', 'steam', 'playstation', 'xbox', 'nintendo',\n                'viagem', 'hotel', 'pousada', 'airbnb', 'turismo'\n            ],\n            \n            TransactionCategory.COMPRAS: [\n                'shopping', 'loja', 'roupa', 'calcado', 'tenis', 'sapato',\n                'acessorio', 'relogio', 'oculos', 'bolsa', 'mochila',\n                'eletronico', 'celular', 'notebook', 'computador', 'tv',\n                'amazon', 'mercado livre', 'americanas', 'magazine', 'casas bahia'\n            ],\n            \n            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    \n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o baseado em sua descri√ß√£o.\"\"\"\n        description_lower = transaction.description.lower()", "mtime": 1755104396.4543626, "terms": ["def", "_load_keywords", "self", "dict", "transactioncategory", "list", "str", "define", "palavras", "chave", "para", "cada", "categoria", "return", "transactioncategory", "alimentacao", "restaurante", "lanchonete", "padaria", "mercado", "supermercado", "ougue", "feira", "ifood", "uber", "eats", "rappi", "delivery", "pizza", "hamburguer", "sushi", "churrasco", "bar", "caf", "cantina", "refeitorio", "almo", "jantar", "lanche", "transactioncategory", "transporte", "uber", "cabify", "taxi", "combustivel", "gasolina", "etanol", "alcool", "posto", "estacionamento", "pedagio", "onibus", "metro", "trem", "passagem", "viagem", "rodoviaria", "aeroporto", "aviao", "voo", "locadora", "aluguel", "carro", "transactioncategory", "moradia", "aluguel", "condominio", "iptu", "luz", "energia", "agua", "gas", "internet", "telefone", "celular", "vivo", "claro", "tim", "oi", "net", "sky", "manutencao", "reforma", "obra", "material", "construcao", "pintura", "eletricista", "encanador", "transactioncategory", "saude", "farmacia", "drogaria", "medicamento", "remedio", "hospital", "clinica", "medico", "consulta", "exame", "laboratorio", "plano", "saude", "unimed", "amil", "sulamerica", "bradesco", "saude", "dentista", "ortodontia", "fisioterapia", "psicologia", "transactioncategory", "educacao", "escola", "faculdade", "universidade", "curso", "livro", "livraria", "apostila", "material", "escolar", "papelaria", "mensalidade", "matricula", "udemy", "coursera", "alura", "ingles", "idioma", "pos", "graduacao", "mba", "mestrado", "transactioncategory", "lazer", "cinema", "teatro", "show", "festa", "evento", "ingresso", "netflix", "spotify", "amazon", "prime", "disney", "hbo", "game", "jogo", "steam", "playstation", "xbox", "nintendo", "viagem", "hotel", "pousada", "airbnb", "turismo", "transactioncategory", "compras", "shopping", "loja", "roupa", "calcado", "tenis", "sapato", "acessorio", "relogio", "oculos", "bolsa", "mochila", "eletronico", "celular", "notebook", "computador", "tv", "amazon", "mercado", "livre", "americanas", "magazine", "casas", "bahia", "transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower"]}
{"chunk_id": "aad7b9e5ed33e83656139292", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 69, "end_line": 134, "content": "            TransactionCategory.SERVICOS: [\n                'cartorio', 'correios', 'sedex', 'pac', 'seguro', 'advogado',\n                'contador', 'mecanico', 'oficina', 'lavanderia', 'costura',\n                'salao', 'cabeleireiro', 'barbearia', 'manicure', 'estetica',\n                'academia', 'personal', 'crossfit', 'pilates', 'yoga'\n            ],\n            \n            TransactionCategory.TRANSFERENCIA: [\n                'transferencia', 'ted', 'doc', 'pix', 'deposito', 'saque',\n                'envio', 'recebido', 'transf', 'dep', 'saq'\n            ],\n            \n            TransactionCategory.INVESTIMENTO: [\n                'investimento', 'aplicacao', 'resgate', 'rendimento', 'cdb',\n                'lci', 'lca', 'tesouro', 'poupanca', 'fundo', 'acao',\n                'bolsa', 'b3', 'corretora', 'xp', 'rico', 'clear', 'nuinvest'\n            ],\n            \n            TransactionCategory.SALARIO: [\n                'salario', 'pagamento', 'vencimento', 'remuneracao', 'holerite',\n                'adiantamento', '13o', 'decimo terceiro', 'ferias', 'rescisao',\n                'inss', 'fgts', 'vale', 'beneficio', 'auxilio'\n            ]\n        }\n    \n    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o baseado em sua descri√ß√£o.\"\"\"\n        description_lower = transaction.description.lower()\n        \n        # Remove acentos para melhor matching\n        description_normalized = self._normalize_text(description_lower)\n        \n        # Tenta encontrar categoria por palavras-chave\n        for category, keywords in self.keywords.items():\n            for keyword in keywords:\n                keyword_normalized = self._normalize_text(keyword.lower())\n                if keyword_normalized in description_normalized:\n                    transaction.category = category\n                    return transaction\n        \n        # Se n√£o encontrou categoria, mant√©m como n√£o categorizado\n        transaction.category = TransactionCategory.NAO_CATEGORIZADO\n        return transaction\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            '√°': 'a', '√†': 'a', '√£': 'a', '√¢': 'a', '√§': 'a',\n            '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',\n            '√≠': 'i', '√¨': 'i', '√Æ': 'i', '√Ø': 'i',\n            '√≥': 'o', '√≤': 'o', '√µ': 'o', '√¥': 'o', '√∂': 'o',\n            '√∫': 'u', '√π': 'u', '√ª': 'u', '√º': 'u',\n            '√ß': 'c', '√±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espa√ßos e n√∫meros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espa√ßos m√∫ltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["transactioncategory", "servicos", "cartorio", "correios", "sedex", "pac", "seguro", "advogado", "contador", "mecanico", "oficina", "lavanderia", "costura", "salao", "cabeleireiro", "barbearia", "manicure", "estetica", "academia", "personal", "crossfit", "pilates", "yoga", "transactioncategory", "transferencia", "transferencia", "ted", "doc", "pix", "deposito", "saque", "envio", "recebido", "transf", "dep", "saq", "transactioncategory", "investimento", "investimento", "aplicacao", "resgate", "rendimento", "cdb", "lci", "lca", "tesouro", "poupanca", "fundo", "acao", "bolsa", "b3", "corretora", "xp", "rico", "clear", "nuinvest", "transactioncategory", "salario", "salario", "pagamento", "vencimento", "remuneracao", "holerite", "adiantamento", "decimo", "terceiro", "ferias", "rescisao", "inss", "fgts", "vale", "beneficio", "auxilio", "def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower", "remove", "acentos", "para", "melhor", "matching", "description_normalized", "self", "_normalize_text", "description_lower", "tenta", "encontrar", "categoria", "por", "palavras", "chave", "for", "category", "keywords", "in", "self", "keywords", "items", "for", "keyword", "in", "keywords", "keyword_normalized", "self", "_normalize_text", "keyword", "lower", "if", "keyword_normalized", "in", "description_normalized", "transaction", "category", "category", "return", "transaction", "se", "encontrou", "categoria", "mant", "como", "categorizado", "transaction", "category", "transactioncategory", "nao_categorizado", "return", "transaction", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "594ac875fd1f65f74cc96d99", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 94, "end_line": 134, "content": "    def categorize(self, transaction: Transaction) -> Transaction:\n        \"\"\"Categoriza uma transa√ß√£o baseado em sua descri√ß√£o.\"\"\"\n        description_lower = transaction.description.lower()\n        \n        # Remove acentos para melhor matching\n        description_normalized = self._normalize_text(description_lower)\n        \n        # Tenta encontrar categoria por palavras-chave\n        for category, keywords in self.keywords.items():\n            for keyword in keywords:\n                keyword_normalized = self._normalize_text(keyword.lower())\n                if keyword_normalized in description_normalized:\n                    transaction.category = category\n                    return transaction\n        \n        # Se n√£o encontrou categoria, mant√©m como n√£o categorizado\n        transaction.category = TransactionCategory.NAO_CATEGORIZADO\n        return transaction\n    \n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            '√°': 'a', '√†': 'a', '√£': 'a', '√¢': 'a', '√§': 'a',\n            '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',\n            '√≠': 'i', '√¨': 'i', '√Æ': 'i', '√Ø': 'i',\n            '√≥': 'o', '√≤': 'o', '√µ': 'o', '√¥': 'o', '√∂': 'o',\n            '√∫': 'u', '√π': 'u', '√ª': 'u', '√º': 'u',\n            '√ß': 'c', '√±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espa√ßos e n√∫meros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espa√ßos m√∫ltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["def", "categorize", "self", "transaction", "transaction", "transaction", "categoriza", "uma", "transa", "baseado", "em", "sua", "descri", "description_lower", "transaction", "description", "lower", "remove", "acentos", "para", "melhor", "matching", "description_normalized", "self", "_normalize_text", "description_lower", "tenta", "encontrar", "categoria", "por", "palavras", "chave", "for", "category", "keywords", "in", "self", "keywords", "items", "for", "keyword", "in", "keywords", "keyword_normalized", "self", "_normalize_text", "keyword", "lower", "if", "keyword_normalized", "in", "description_normalized", "transaction", "category", "category", "return", "transaction", "se", "encontrou", "categoria", "mant", "como", "categorizado", "transaction", "category", "transactioncategory", "nao_categorizado", "return", "transaction", "def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "323262c310b31012122c5eaa", "file_path": "src/infrastructure/categorizers/keyword_categorizer.py", "start_line": 113, "end_line": 134, "content": "    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normaliza texto removendo acentos e caracteres especiais.\"\"\"\n        # Remove acentos comuns\n        replacements = {\n            '√°': 'a', '√†': 'a', '√£': 'a', '√¢': 'a', '√§': 'a',\n            '√©': 'e', '√®': 'e', '√™': 'e', '√´': 'e',\n            '√≠': 'i', '√¨': 'i', '√Æ': 'i', '√Ø': 'i',\n            '√≥': 'o', '√≤': 'o', '√µ': 'o', '√¥': 'o', '√∂': 'o',\n            '√∫': 'u', '√π': 'u', '√ª': 'u', '√º': 'u',\n            '√ß': 'c', '√±': 'n'\n        }\n        \n        for old, new in replacements.items():\n            text = text.replace(old, new)\n        \n        # Remove caracteres especiais mantendo espa√ßos e n√∫meros\n        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n        \n        # Remove espa√ßos m√∫ltiplos\n        text = ' '.join(text.split())\n        \n        return text", "mtime": 1755104396.4543626, "terms": ["def", "_normalize_text", "self", "text", "str", "str", "normaliza", "texto", "removendo", "acentos", "caracteres", "especiais", "remove", "acentos", "comuns", "replacements", "for", "old", "new", "in", "replacements", "items", "text", "text", "replace", "old", "new", "remove", "caracteres", "especiais", "mantendo", "espa", "os", "meros", "text", "re", "sub", "z0", "text", "remove", "espa", "os", "ltiplos", "text", "join", "text", "split", "return", "text"]}
{"chunk_id": "978b6e5656128a30ed538e88", "file_path": "src/infrastructure/categorizers/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote categorizers", "mtime": 1755104402.6686974, "terms": ["inicializa", "pacote", "categorizers"]}
{"chunk_id": "98ff3dce7c4ebd7b8a5dea25", "file_path": "src/infrastructure/reports/__init__.py", "start_line": 1, "end_line": 1, "content": "# Inicializa o pacote reports", "mtime": 1755104542.934863, "terms": ["inicializa", "pacote", "reports"]}
{"chunk_id": "356a881d60a3f3868d5af108", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 1, "end_line": 80, "content": "\"\"\"\nImplementa√ß√£o de gerador de relat√≥rios em texto.\n\"\"\"\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom src.domain.models import AnalysisResult, TransactionCategory\nfrom src.domain.interfaces import ReportGenerator\nfrom src.utils.currency_utils import CurrencyUtils\n\n\nclass TextReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relat√≥rios em formato texto.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em texto a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELAT√ìRIO DE AN√ÅLISE DE EXTRATO BANC√ÅRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do Per√≠odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'M√™s':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:", "mtime": 1756145388.113131, "terms": ["implementa", "de", "gerador", "de", "relat", "rios", "em", "texto", "from", "datetime", "import", "datetime", "from", "pathlib", "import", "path", "from", "typing", "import", "optional", "from", "src", "domain", "models", "import", "analysisresult", "transactioncategory", "from", "src", "domain", "interfaces", "import", "reportgenerator", "from", "src", "utils", "currency_utils", "import", "currencyutils", "class", "textreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "texto", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts"]}
{"chunk_id": "25a43c5f07d75499aa53532c", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 13, "end_line": 92, "content": "class TextReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relat√≥rios em formato texto.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em texto a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELAT√ìRIO DE AN√ÅLISE DE EXTRATO BANC√ÅRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do Per√≠odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'M√™s':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDA√á√ïES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados", "mtime": 1756145388.113131, "terms": ["class", "textreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "texto", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados"]}
{"chunk_id": "06af7ee77570163b6856b2d5", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 16, "end_line": 95, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em texto a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELAT√ìRIO DE AN√ÅLISE DE EXTRATO BANC√ÅRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do Per√≠odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'M√™s':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDA√á√ïES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMA√á√ïES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)", "mtime": 1756145388.113131, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "texto", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append"]}
{"chunk_id": "e961a0d0187c39d3b3e5d64d", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 19, "end_line": 98, "content": "        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"RELAT√ìRIO DE AN√ÅLISE DE EXTRATO BANC√ÅRIO\".center(80))\n        report_lines.append(\"=\" * 80)\n        report_lines.append(f\"Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"RESUMO FINANCEIRO\")\n        report_lines.append(\"-\" * 40)\n        report_lines.append(f\"Total de Receitas:  {format_currency(analysis.total_income):>12}\")\n        report_lines.append(f\"Total de Despesas:  {format_currency(analysis.total_expenses):>12}\")\n        report_lines.append(f\"Saldo do Per√≠odo:   {format_currency(analysis.net_flow):>12}\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"DESPESAS POR CATEGORIA\")\n            report_lines.append(\"-\" * 40)\n\n            total_categorized = sum(analysis.categories_summary.values())\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.title()\n                report_lines.append(\n                    f\"{category_name:<25} {format_currency(amount):>10} ({percentage:>5.1f}%)\"\n                )\n\n            report_lines.append(\"-\" * 40)\n            report_lines.append(f\"{'Total Categorizado:':<25} {format_currency(total_categorized):>10}\")\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"RESUMO MENSAL\")\n            report_lines.append(\"-\" * 60)\n            report_lines.append(f\"{'M√™s':<10} {'Receitas':>15} {'Despesas':>15} {'Saldo':>15}\")\n            report_lines.append(\"-\" * 60)\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"{month_name:<10} \"\n                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDA√á√ïES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMA√á√ïES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"Total de transa√ß√µes: {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:", "mtime": 1756145388.113131, "terms": ["def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "center", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "total_categorized", "sum", "analysis", "categories_summary", "values", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "report_lines", "append", "total", "categorizado", "format_currency", "total_categorized", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata"]}
{"chunk_id": "021bbe2bbad462c53be4f95b", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 69, "end_line": 148, "content": "                    f\"{format_currency(data['income']):>15} \"\n                    f\"{format_currency(data['expenses']):>15} \"\n                    f\"{format_currency(data['balance']):>15}\"\n                )\n\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"ALERTAS\")\n            report_lines.append(\"-\" * 40)\n            for alert in analysis.alerts:\n                report_lines.append(f\"  {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"INSIGHTS E RECOMENDA√á√ïES\")\n            report_lines.append(\"-\" * 40)\n            for insight in analysis.insights:\n                report_lines.append(f\"  {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"INFORMA√á√ïES ADICIONAIS\")\n            report_lines.append(\"-\" * 40)\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"Total de transa√ß√µes: {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"Per√≠odo analisado: {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n\n        # Rodap√©\n        report_lines.append(\"=\" * 80)\n        report_lines.append(\"Fim do Relat√≥rio\")\n        report_lines.append(\"=\" * 80)\n\n        report_content = \"\\n\".join(report_lines)\n\n        # Salva o arquivo se especificado\n        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content\n\n\nclass MarkdownReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relat√≥rios em formato Markdown.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em Markdown a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"# Relat√≥rio de An√°lise de Extrato Banc√°rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## üí∞ Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do Per√≠odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## üìä Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")", "mtime": 1756145388.113131, "terms": ["format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append", "rodap", "report_lines", "append", "report_lines", "append", "fim", "do", "relat", "rio", "report_lines", "append", "report_content", "join", "report_lines", "salva", "arquivo", "se", "especificado", "if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content", "class", "markdownreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "markdown", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual"]}
{"chunk_id": "bd3876feaf5680c1ff0c7e52", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 116, "end_line": 195, "content": "class MarkdownReportGenerator(ReportGenerator):\n    \"\"\"Gerador de relat√≥rios em formato Markdown.\"\"\"\n\n    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em Markdown a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"# Relat√≥rio de An√°lise de Extrato Banc√°rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## üí∞ Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do Per√≠odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## üìä Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## üìÖ Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| M√™s | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## ‚ö†Ô∏è Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## üí° Insights e Recomenda√ß√µes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## ‚ÑπÔ∏è Informa√ß√µes Adicionais\")\n            report_lines.append(\"\")", "mtime": 1756145388.113131, "terms": ["class", "markdownreportgenerator", "reportgenerator", "gerador", "de", "relat", "rios", "em", "formato", "markdown", "def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append"]}
{"chunk_id": "3436bdec7971a576f1ee9c8c", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 119, "end_line": 198, "content": "    def generate(self, analysis: AnalysisResult, output_path: Optional[Path] = None) -> str:\n        \"\"\"Gera relat√≥rio em Markdown a partir da an√°lise.\"\"\"\n        \n        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"# Relat√≥rio de An√°lise de Extrato Banc√°rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## üí∞ Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do Per√≠odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## üìä Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## üìÖ Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| M√™s | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## ‚ö†Ô∏è Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## üí° Insights e Recomenda√ß√µes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## ‚ÑπÔ∏è Informa√ß√µes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transa√ß√µes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:", "mtime": 1756145388.113131, "terms": ["def", "generate", "self", "analysis", "analysisresult", "output_path", "optional", "path", "none", "str", "gera", "relat", "rio", "em", "markdown", "partir", "da", "an", "lise", "def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata"]}
{"chunk_id": "b75f59ffce86d5b3f271826a", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 122, "end_line": 201, "content": "        def format_currency(value: float) -> str:\n            \"\"\"Formata valor com a moeda correta.\"\"\"\n            return CurrencyUtils.format_currency(value, analysis.currency)\n\n        report_lines = []\n\n        # Cabe√ßalho\n        report_lines.append(\"# Relat√≥rio de An√°lise de Extrato Banc√°rio\")\n        report_lines.append(\"\")\n        report_lines.append(f\"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n        report_lines.append(\"\")\n\n        # Resumo Financeiro\n        report_lines.append(\"## üí∞ Resumo Financeiro\")\n        report_lines.append(\"\")\n        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do Per√≠odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## üìä Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## üìÖ Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| M√™s | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## ‚ö†Ô∏è Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## üí° Insights e Recomenda√ß√µes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## ‚ÑπÔ∏è Informa√ß√µes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transa√ß√µes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"- **Per√≠odo analisado:** {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n", "mtime": 1756145388.113131, "terms": ["def", "format_currency", "value", "float", "str", "formata", "valor", "com", "moeda", "correta", "return", "currencyutils", "format_currency", "value", "analysis", "currency", "report_lines", "cabe", "alho", "report_lines", "append", "relat", "rio", "de", "an", "lise", "de", "extrato", "banc", "rio", "report_lines", "append", "report_lines", "append", "gerado", "em", "datetime", "now", "strftime", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "resumo", "financeiro", "report_lines", "append", "report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append"]}
{"chunk_id": "b382c859a155634fa4112aa0", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 137, "end_line": 208, "content": "        report_lines.append(\"| Item | Valor |\")\n        report_lines.append(\"|------|-------|\")\n        report_lines.append(f\"| Total de Receitas | {format_currency(analysis.total_income)} |\")\n        report_lines.append(f\"| Total de Despesas | {format_currency(analysis.total_expenses)} |\")\n        report_lines.append(f\"| **Saldo do Per√≠odo** | **{format_currency(analysis.net_flow)}** |\")\n        report_lines.append(\"\")\n\n        # An√°lise por Categoria\n        if analysis.categories_summary:\n            report_lines.append(\"## üìä Despesas por Categoria\")\n            report_lines.append(\"\")\n            report_lines.append(\"| Categoria | Valor | Percentual |\")\n            report_lines.append(\"|-----------|-------|------------|\")\n\n            for category, amount in analysis.categories_summary.items():\n                percentage = (amount / analysis.total_expenses * 100) if analysis.total_expenses > 0 else 0\n                category_name = category.value.replace('_', ' ').title()\n                report_lines.append(\n                    f\"| {category_name} | {format_currency(amount)} | {percentage:.1f}% |\"\n                )\n            report_lines.append(\"\")\n\n        # Resumo Mensal\n        if analysis.monthly_summary:\n            report_lines.append(\"## üìÖ Resumo Mensal\")\n            report_lines.append(\"\")\n            report_lines.append(\"| M√™s | Receitas | Despesas | Saldo |\")\n            report_lines.append(\"|-----|----------|----------|-------|\")\n\n            for month, data in sorted(analysis.monthly_summary.items()):\n                month_name = datetime.strptime(month, '%Y-%m').strftime('%b/%Y')\n                report_lines.append(\n                    f\"| {month_name} | \"\n                    f\"{format_currency(data['income'])} | \"\n                    f\"{format_currency(data['expenses'])} | \"\n                    f\"{format_currency(data['balance'])} |\"\n                )\n            report_lines.append(\"\")\n\n        # Alertas\n        if analysis.alerts:\n            report_lines.append(\"## ‚ö†Ô∏è Alertas\")\n            report_lines.append(\"\")\n            for alert in analysis.alerts:\n                report_lines.append(f\"- {alert}\")\n            report_lines.append(\"\")\n\n        # Insights\n        if analysis.insights:\n            report_lines.append(\"## üí° Insights e Recomenda√ß√µes\")\n            report_lines.append(\"\")\n            for insight in analysis.insights:\n                report_lines.append(f\"- {insight}\")\n            report_lines.append(\"\")\n\n        # Metadados\n        if analysis.metadata:\n            report_lines.append(\"## ‚ÑπÔ∏è Informa√ß√µes Adicionais\")\n            report_lines.append(\"\")\n            if 'transaction_count' in analysis.metadata:\n                report_lines.append(f\"- **Total de transa√ß√µes:** {analysis.metadata['transaction_count']}\")\n            if 'period_days' in analysis.metadata:\n                report_lines.append(f\"- **Per√≠odo analisado:** {analysis.metadata['period_days']} dias\")\n            report_lines.append(\"\")\n\n        report_content = \"\\n\".join(report_lines)\n\n        # Salva o arquivo se especificado\n        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content", "mtime": 1756145388.113131, "terms": ["report_lines", "append", "item", "valor", "report_lines", "append", "report_lines", "append", "total", "de", "receitas", "format_currency", "analysis", "total_income", "report_lines", "append", "total", "de", "despesas", "format_currency", "analysis", "total_expenses", "report_lines", "append", "saldo", "do", "per", "odo", "format_currency", "analysis", "net_flow", "report_lines", "append", "an", "lise", "por", "categoria", "if", "analysis", "categories_summary", "report_lines", "append", "despesas", "por", "categoria", "report_lines", "append", "report_lines", "append", "categoria", "valor", "percentual", "report_lines", "append", "for", "category", "amount", "in", "analysis", "categories_summary", "items", "percentage", "amount", "analysis", "total_expenses", "if", "analysis", "total_expenses", "else", "category_name", "category", "value", "replace", "title", "report_lines", "append", "category_name", "format_currency", "amount", "percentage", "report_lines", "append", "resumo", "mensal", "if", "analysis", "monthly_summary", "report_lines", "append", "resumo", "mensal", "report_lines", "append", "report_lines", "append", "receitas", "despesas", "saldo", "report_lines", "append", "for", "month", "data", "in", "sorted", "analysis", "monthly_summary", "items", "month_name", "datetime", "strptime", "month", "strftime", "report_lines", "append", "month_name", "format_currency", "data", "income", "format_currency", "data", "expenses", "format_currency", "data", "balance", "report_lines", "append", "alertas", "if", "analysis", "alerts", "report_lines", "append", "alertas", "report_lines", "append", "for", "alert", "in", "analysis", "alerts", "report_lines", "append", "alert", "report_lines", "append", "insights", "if", "analysis", "insights", "report_lines", "append", "insights", "recomenda", "es", "report_lines", "append", "for", "insight", "in", "analysis", "insights", "report_lines", "append", "insight", "report_lines", "append", "metadados", "if", "analysis", "metadata", "report_lines", "append", "informa", "es", "adicionais", "report_lines", "append", "if", "transaction_count", "in", "analysis", "metadata", "report_lines", "append", "total", "de", "transa", "es", "analysis", "metadata", "transaction_count", "if", "period_days", "in", "analysis", "metadata", "report_lines", "append", "per", "odo", "analisado", "analysis", "metadata", "period_days", "dias", "report_lines", "append", "report_content", "join", "report_lines", "salva", "arquivo", "se", "especificado", "if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content"]}
{"chunk_id": "e43f9c9a3286d4317f524144", "file_path": "src/infrastructure/reports/text_report.py", "start_line": 205, "end_line": 208, "content": "        if output_path:\n            output_path.write_text(report_content, encoding='utf-8')\n\n        return report_content", "mtime": 1756145388.113131, "terms": ["if", "output_path", "output_path", "write_text", "report_content", "encoding", "utf", "return", "report_content"]}
{"chunk_id": "1d453cc663a82c33098b9305", "file_path": "mcp_system/embeddings/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Embeddings Module\n\"\"\"\nM√≥dulo de embeddings sem√¢nticos para busca avan√ßada de c√≥digo.\n\"\"\"\n\nfrom .semantic_search import *", "mtime": 1755701269.9533663, "terms": ["mcp", "system", "embeddings", "module", "dulo", "de", "embeddings", "sem", "nticos", "para", "busca", "avan", "ada", "de", "digo", "from", "semantic_search", "import"]}
{"chunk_id": "f33e9104b86d272a7e2afc9a", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 1, "end_line": 80, "content": "# src/embeddings/semantic_search.py\n\"\"\"\nSistema de busca sem√¢ntica usando embeddings locais\nIntegra√ß√£o com sentence-transformers para embeddings eficientes\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport json\nimport numpy as np\nimport hashlib\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nimport pathlib\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    HAS_SENTENCE_TRANSFORMERS = True\nexcept ImportError:\n    HAS_SENTENCE_TRANSFORMERS = False\n    SentenceTransformer = None\n\n@dataclass\nclass EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: n√£o carregar o modelo no __init__\n        # Se sentence-transformers n√£o estiver dispon√≠vel, os m√©todos far√£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"", "mtime": 1756159582.9433653, "terms": ["src", "embeddings", "semantic_search", "py", "sistema", "de", "busca", "sem", "ntica", "usando", "embeddings", "locais", "integra", "com", "sentence", "transformers", "para", "embeddings", "eficientes", "from", "__future__", "import", "annotations", "import", "os", "import", "json", "import", "numpy", "as", "np", "import", "hashlib", "from", "pathlib", "import", "path", "from", "typing", "import", "dict", "list", "tuple", "optional", "any", "from", "dataclasses", "import", "dataclass", "import", "pathlib", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "sentence_transformers", "import", "sentencetransformer", "has_sentence_transformers", "true", "except", "importerror", "has_sentence_transformers", "false", "sentencetransformer", "none", "dataclass", "class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray"]}
{"chunk_id": "16e5d8070d1eaddb4b003a54", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 28, "end_line": 107, "content": "class EmbeddingResult:\n    chunk_id: str\n    similarity_score: float\n    bm25_score: float\n    combined_score: float\n    content: str\n    file_path: str\n\nclass SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: n√£o carregar o modelo no __init__\n        # Se sentence-transformers n√£o estiver dispon√≠vel, os m√©todos far√£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                ", "mtime": 1756159582.9433653, "terms": ["class", "embeddingresult", "chunk_id", "str", "similarity_score", "float", "bm25_score", "float", "combined_score", "float", "content", "str", "file_path", "str", "class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load"]}
{"chunk_id": "f356fbce42cd73f0ff72e1d4", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 36, "end_line": 115, "content": "class SemanticSearchEngine:\n    \"\"\"\n    Sistema de busca sem√¢ntica que combina embeddings com BM25\n    para busca h√≠brida otimizada\n    \"\"\"\n    \n    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: n√£o carregar o modelo no __init__\n        # Se sentence-transformers n√£o estiver dispon√≠vel, os m√©todos far√£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys", "mtime": 1756159582.9433653, "terms": ["class", "semanticsearchengine", "sistema", "de", "busca", "sem", "ntica", "que", "combina", "embeddings", "com", "bm25", "para", "busca", "brida", "otimizada", "def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys"]}
{"chunk_id": "ad95d53b51a170160d5bf8bc", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 42, "end_line": 121, "content": "    def __init__(self, cache_dir: str = str(CURRENT_DIR.parent / \".mcp_index\"), model_name: str = \"all-MiniLM-L6-v2\"):\n        self.cache_dir = Path(cache_dir)\n        self.embeddings_dir = self.cache_dir / \"embeddings\"\n        self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n        self.model_name = model_name\n        self.model = None\n        self.embeddings_cache: Dict[str, np.ndarray] = {}\n        self.metadata_cache: Dict[str, Dict] = {}\n\n        # Lazy-load: n√£o carregar o modelo no __init__\n        # Se sentence-transformers n√£o estiver dispon√≠vel, os m√©todos far√£o fallback silencioso\n\n    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content", "mtime": 1756159582.9433653, "terms": ["def", "__init__", "self", "cache_dir", "str", "str", "current_dir", "parent", "mcp_index", "model_name", "str", "all", "minilm", "l6", "v2", "self", "cache_dir", "path", "cache_dir", "self", "embeddings_dir", "self", "cache_dir", "embeddings", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "model_name", "model_name", "self", "model", "none", "self", "embeddings_cache", "dict", "str", "np", "ndarray", "self", "metadata_cache", "dict", "str", "dict", "lazy", "load", "carregar", "modelo", "no", "__init__", "se", "sentence", "transformers", "estiver", "dispon", "vel", "os", "todos", "far", "fallback", "silencioso", "def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content"]}
{"chunk_id": "eb3cf6f084fc7029e4e4f382", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 55, "end_line": 134, "content": "    def _initialize_model(self):\n        \"\"\"Inicializa o modelo de embeddings de forma lazy\"\"\"\n        if self.model is None and HAS_SENTENCE_TRANSFORMERS:\n            import sys\n            sys.stderr.write(f\"üîÑ Carregando modelo de embeddings: {self.model_name}\\n\")\n            try:\n                self.model = SentenceTransformer(self.model_name)\n                sys.stderr.write(f\"‚úÖ Modelo {self.model_name} carregado com sucesso\\n\")\n            except Exception as e:\n                sys.stderr.write(f\"‚ùå Erro ao carregar modelo: {e}\\n\")\n                self.model = None\n    \n    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:", "mtime": 1756159582.9433653, "terms": ["def", "_initialize_model", "self", "inicializa", "modelo", "de", "embeddings", "de", "forma", "lazy", "if", "self", "model", "is", "none", "and", "has_sentence_transformers", "import", "sys", "sys", "stderr", "write", "carregando", "modelo", "de", "embeddings", "self", "model_name", "try", "self", "model", "sentencetransformer", "self", "model_name", "sys", "stderr", "write", "modelo", "self", "model_name", "carregado", "com", "sucesso", "except", "exception", "as", "sys", "stderr", "write", "erro", "ao", "carregar", "modelo", "self", "model", "none", "def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as"]}
{"chunk_id": "de4f156e5fe9ac181b58059a", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 67, "end_line": 146, "content": "    def _get_embedding_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para embeddings de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None", "mtime": 1756159582.9433653, "terms": ["def", "_get_embedding_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "embeddings", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none"]}
{"chunk_id": "ee7737b2f63e82c7590f4881", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 69, "end_line": 148, "content": "        return self.embeddings_dir / f\"{chunk_id}.npy\"\n    \n    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],", "mtime": 1756159582.9433653, "terms": ["return", "self", "embeddings_dir", "chunk_id", "npy", "def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray"]}
{"chunk_id": "dde80ca94ef8afafe26fdfdf", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 71, "end_line": 150, "content": "    def _get_metadata_cache_path(self, chunk_id: str) -> Path:\n        \"\"\"Gera caminho do cache para metadados de um chunk\"\"\"\n        return self.embeddings_dir / f\"{chunk_id}_meta.json\"\n    \n    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"", "mtime": 1756159582.9433653, "terms": ["def", "_get_metadata_cache_path", "self", "chunk_id", "str", "path", "gera", "caminho", "do", "cache", "para", "metadados", "de", "um", "chunk", "return", "self", "embeddings_dir", "chunk_id", "_meta", "json", "def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float"]}
{"chunk_id": "6c08fb627e8e7e4958a6e4a0", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 75, "end_line": 154, "content": "    def _hash_content(self, content: str) -> str:\n        \"\"\"Gera hash do conte√∫do para cache invalidation\"\"\"\n        return hashlib.md5(content.encode('utf-8')).hexdigest()\n    \n    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n\n        Args:\n            query: Texto da consulta", "mtime": 1756159582.9433653, "terms": ["def", "_hash_content", "self", "content", "str", "str", "gera", "hash", "do", "conte", "do", "para", "cache", "invalidation", "return", "hashlib", "md5", "content", "encode", "utf", "hexdigest", "def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta"]}
{"chunk_id": "a4e1cbf78511cdaef8128408", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 79, "end_line": 158, "content": "    def get_embedding(self, chunk_id: str, content: str, force_regenerate: bool = False) -> Optional[np.ndarray]:\n        \"\"\"\n        Obt√©m embedding para um chunk, usando cache quando poss√≠vel\n\n        Args:\n            chunk_id: Identificador √∫nico do chunk\n            content: Conte√∫do do chunk para gerar embedding\n            force_regenerate: Se True, for√ßa regenera√ß√£o do embedding\n\n        Returns:\n            Array numpy com o embedding ou None se erro\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return None\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return None\n\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        content_hash = self._hash_content(content)\n        \n        # Verifica cache se n√£o for√ßar regenera√ß√£o\n        if not force_regenerate and cache_path.exists() and metadata_path.exists():\n            try:\n                with open(metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                \n                # Verifica se conte√∫do n√£o mudou\n                if metadata.get('content_hash') == content_hash:\n                    embedding = np.load(cache_path)\n                    self.embeddings_cache[chunk_id] = embedding\n                    self.metadata_cache[chunk_id] = metadata\n                    return embedding\n            except Exception as e:\n                import sys\n                sys.stderr.write(f\"‚ö†Ô∏è  Erro ao ler cache de embedding para {chunk_id}: {e}\\n\")\n        \n        # Gera novo embedding\n        try:\n            # Limita conte√∫do para n√£o sobrecarregar o modelo\n            content_truncated = content[:2000] if len(content) > 2000 else content\n            embedding = self.model.encode([content_truncated])[0]\n            \n            # Salva no cache\n            np.save(cache_path, embedding)\n            metadata = {\n                'chunk_id': chunk_id,\n                'content_hash': content_hash,\n                'model_name': self.model_name,\n                'content_length': len(content),\n                'truncated': len(content) > 2000\n            }\n            \n            with open(metadata_path, 'w', encoding='utf-8') as f:\n                json.dump(metadata, f, indent=2)\n            \n            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n\n        Returns:", "mtime": 1756159582.9433653, "terms": ["def", "get_embedding", "self", "chunk_id", "str", "content", "str", "force_regenerate", "bool", "false", "optional", "np", "ndarray", "obt", "embedding", "para", "um", "chunk", "usando", "cache", "quando", "poss", "vel", "args", "chunk_id", "identificador", "nico", "do", "chunk", "content", "conte", "do", "do", "chunk", "para", "gerar", "embedding", "force_regenerate", "se", "true", "for", "regenera", "do", "embedding", "returns", "array", "numpy", "com", "embedding", "ou", "none", "se", "erro", "if", "not", "has_sentence_transformers", "return", "none", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "none", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "content_hash", "self", "_hash_content", "content", "verifica", "cache", "se", "for", "ar", "regenera", "if", "not", "force_regenerate", "and", "cache_path", "exists", "and", "metadata_path", "exists", "try", "with", "open", "metadata_path", "encoding", "utf", "as", "metadata", "json", "load", "verifica", "se", "conte", "do", "mudou", "if", "metadata", "get", "content_hash", "content_hash", "embedding", "np", "load", "cache_path", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "ler", "cache", "de", "embedding", "para", "chunk_id", "gera", "novo", "embedding", "try", "limita", "conte", "do", "para", "sobrecarregar", "modelo", "content_truncated", "content", "if", "len", "content", "else", "content", "embedding", "self", "model", "encode", "content_truncated", "salva", "no", "cache", "np", "save", "cache_path", "embedding", "metadata", "chunk_id", "chunk_id", "content_hash", "content_hash", "model_name", "self", "model_name", "content_length", "len", "content", "truncated", "len", "content", "with", "open", "metadata_path", "encoding", "utf", "as", "json", "dump", "metadata", "indent", "atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns"]}
{"chunk_id": "6c638b70a72f80d09b7a0134", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 137, "end_line": 216, "content": "            # Atualiza cache em mem√≥ria\n            self.embeddings_cache[chunk_id] = embedding\n            self.metadata_cache[chunk_id] = metadata\n            \n            return embedding\n            \n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro ao gerar embedding para {chunk_id}: {e}\\n\")\n            return None\n    \n    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n\n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return []\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return []\n\n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n\n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n\n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n\n            return similarities[:top_k]\n\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca sem√¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,", "mtime": 1756159582.9433653, "terms": ["atualiza", "cache", "em", "mem", "ria", "self", "embeddings_cache", "chunk_id", "embedding", "self", "metadata_cache", "chunk_id", "metadata", "return", "embedding", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "ao", "gerar", "embedding", "para", "chunk_id", "return", "none", "def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "return", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score"]}
{"chunk_id": "dfcb1dbe94678fd02f73e4ed", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 148, "end_line": 227, "content": "    def search_similar(self, query: str, chunk_embeddings: Dict[str, np.ndarray],\n                      top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"\n        Busca chunks similares semanticamente √† query\n\n        Args:\n            query: Texto da consulta\n            chunk_embeddings: Dict de chunk_id -> embedding\n            top_k: N√∫mero m√°ximo de resultados\n\n        Returns:\n            Lista de (chunk_id, similarity_score) ordenada por similaridade\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS:\n            return []\n        if self.model is None:\n            self._initialize_model()\n            if self.model is None:\n                return []\n\n        try:\n            # Gera embedding da query\n            query_embedding = self.model.encode([query])[0]\n\n            # Calcula similaridade com todos os chunks\n            similarities = []\n            for chunk_id, chunk_embedding in chunk_embeddings.items():\n                # Similaridade coseno\n                similarity = np.dot(query_embedding, chunk_embedding) / (\n                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)\n                )\n                similarities.append((chunk_id, float(similarity)))\n\n            # Ordena por similaridade descendente\n            similarities.sort(key=lambda x: x[1], reverse=True)\n\n            return similarities[:top_k]\n\n        except Exception as e:\n            import sys\n            sys.stderr.write(f\"‚ùå Erro na busca sem√¢ntica: {e}\\n\")\n            return []\n    \n    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []", "mtime": 1756159582.9433653, "terms": ["def", "search_similar", "self", "query", "str", "chunk_embeddings", "dict", "str", "np", "ndarray", "top_k", "int", "list", "tuple", "str", "float", "busca", "chunks", "similares", "semanticamente", "query", "args", "query", "texto", "da", "consulta", "chunk_embeddings", "dict", "de", "chunk_id", "embedding", "top_k", "mero", "ximo", "de", "resultados", "returns", "lista", "de", "chunk_id", "similarity_score", "ordenada", "por", "similaridade", "if", "not", "has_sentence_transformers", "return", "if", "self", "model", "is", "none", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "try", "gera", "embedding", "da", "query", "query_embedding", "self", "model", "encode", "query", "calcula", "similaridade", "com", "todos", "os", "chunks", "similarities", "for", "chunk_id", "chunk_embedding", "in", "chunk_embeddings", "items", "similaridade", "coseno", "similarity", "np", "dot", "query_embedding", "chunk_embedding", "np", "linalg", "norm", "query_embedding", "np", "linalg", "norm", "chunk_embedding", "similarities", "append", "chunk_id", "float", "similarity", "ordena", "por", "similaridade", "descendente", "similarities", "sort", "key", "lambda", "reverse", "true", "return", "similarities", "top_k", "except", "exception", "as", "import", "sys", "sys", "stderr", "write", "erro", "na", "busca", "sem", "ntica", "return", "def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return"]}
{"chunk_id": "3b86c3ec936843eb966239af", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 191, "end_line": 270, "content": "    def hybrid_search(self, query: str, bm25_results: List[Dict], \n                     chunk_data: Dict[str, Dict], \n                     semantic_weight: float = 0.3, \n                     top_k: int = 10) -> List[EmbeddingResult]:\n        \"\"\"\n        Combina resultados BM25 com busca sem√¢ntica\n        \n        Args:\n            query: Consulta original\n            bm25_results: Resultados da busca BM25\n            chunk_data: Dados completos dos chunks (chunk_id -> data)\n            semantic_weight: Peso da similaridade sem√¢ntica (0-1)\n            top_k: N√∫mero de resultados finais\n            \n        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # Obt√©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor sele√ß√£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca sem√¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        ", "mtime": 1756159582.9433653, "terms": ["def", "hybrid_search", "self", "query", "str", "bm25_results", "list", "dict", "chunk_data", "dict", "str", "dict", "semantic_weight", "float", "top_k", "int", "list", "embeddingresult", "combina", "resultados", "bm25", "com", "busca", "sem", "ntica", "args", "query", "consulta", "original", "bm25_results", "resultados", "da", "busca", "bm25", "chunk_data", "dados", "completos", "dos", "chunks", "chunk_id", "data", "semantic_weight", "peso", "da", "similaridade", "sem", "ntica", "top_k", "mero", "de", "resultados", "finais", "returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path"]}
{"chunk_id": "9461bfdfb0ea529e4db7fda5", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 205, "end_line": 284, "content": "        Returns:\n            Lista de EmbeddingResult ordenada por score combinado\n        \"\"\"\n        if not HAS_SENTENCE_TRANSFORMERS or not bm25_results:\n            # Fallback para apenas BM25\n            results = []\n            for result in bm25_results[:top_k]:\n                chunk_id = result['chunk_id']\n                chunk = chunk_data.get(chunk_id, {})\n                results.append(EmbeddingResult(\n                    chunk_id=chunk_id,\n                    similarity_score=0.0,\n                    bm25_score=result.get('score', 0.0),\n                    combined_score=result.get('score', 0.0),\n                    content=chunk.get('content', ''),\n                    file_path=chunk.get('file_path', '')\n                ))\n            return results\n        \n        # Garante que modelo est√° carregado\n        self._initialize_model()\n        if self.model is None:\n            return []\n        \n        # Obt√©m embeddings para chunks relevantes\n        chunk_embeddings = {}\n        for result in bm25_results[:top_k * 2]:  # Processa mais chunks para melhor sele√ß√£o\n            chunk_id = result['chunk_id']\n            chunk = chunk_data.get(chunk_id, {})\n            content = chunk.get('content', '')\n            \n            if content:\n                embedding = self.get_embedding(chunk_id, content)\n                if embedding is not None:\n                    chunk_embeddings[chunk_id] = embedding\n        \n        # Busca sem√¢ntica\n        semantic_results = self.search_similar(query, chunk_embeddings, top_k * 2)\n        semantic_scores = {chunk_id: score for chunk_id, score in semantic_results}\n        \n        # Normaliza scores BM25\n        bm25_scores = {r['chunk_id']: r.get('score', 0.0) for r in bm25_results}\n        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1.0\n        normalized_bm25 = {cid: score/max_bm25 for cid, score in bm25_scores.items()}\n        \n        # Combina scores\n        combined_results = []\n        all_chunk_ids = set(bm25_scores.keys()) | set(semantic_scores.keys())\n        \n        for chunk_id in all_chunk_ids:\n            bm25_score = normalized_bm25.get(chunk_id, 0.0)\n            semantic_score = semantic_scores.get(chunk_id, 0.0)\n            \n            # Score combinado: (1-w)*BM25 + w*Semantic\n            combined_score = (1 - semantic_weight) * bm25_score + semantic_weight * semantic_score\n            \n            chunk = chunk_data.get(chunk_id, {})\n            combined_results.append(EmbeddingResult(\n                chunk_id=chunk_id,\n                similarity_score=semantic_score,\n                bm25_score=bm25_score,\n                combined_score=combined_score,\n                content=chunk.get('content', ''),\n                file_path=chunk.get('file_path', '')\n            ))\n        \n        # Ordena por score combinado\n        combined_results.sort(key=lambda x: x.combined_score, reverse=True)\n        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk espec√≠fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()", "mtime": 1756159582.9433653, "terms": ["returns", "lista", "de", "embeddingresult", "ordenada", "por", "score", "combinado", "if", "not", "has_sentence_transformers", "or", "not", "bm25_results", "fallback", "para", "apenas", "bm25", "results", "for", "result", "in", "bm25_results", "top_k", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "bm25_score", "result", "get", "score", "combined_score", "result", "get", "score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "return", "results", "garante", "que", "modelo", "est", "carregado", "self", "_initialize_model", "if", "self", "model", "is", "none", "return", "obt", "embeddings", "para", "chunks", "relevantes", "chunk_embeddings", "for", "result", "in", "bm25_results", "top_k", "processa", "mais", "chunks", "para", "melhor", "sele", "chunk_id", "result", "chunk_id", "chunk", "chunk_data", "get", "chunk_id", "content", "chunk", "get", "content", "if", "content", "embedding", "self", "get_embedding", "chunk_id", "content", "if", "embedding", "is", "not", "none", "chunk_embeddings", "chunk_id", "embedding", "busca", "sem", "ntica", "semantic_results", "self", "search_similar", "query", "chunk_embeddings", "top_k", "semantic_scores", "chunk_id", "score", "for", "chunk_id", "score", "in", "semantic_results", "normaliza", "scores", "bm25", "bm25_scores", "chunk_id", "get", "score", "for", "in", "bm25_results", "max_bm25", "max", "bm25_scores", "values", "if", "bm25_scores", "values", "else", "normalized_bm25", "cid", "score", "max_bm25", "for", "cid", "score", "in", "bm25_scores", "items", "combina", "scores", "combined_results", "all_chunk_ids", "set", "bm25_scores", "keys", "set", "semantic_scores", "keys", "for", "chunk_id", "in", "all_chunk_ids", "bm25_score", "normalized_bm25", "get", "chunk_id", "semantic_score", "semantic_scores", "get", "chunk_id", "score", "combinado", "bm25", "semantic", "combined_score", "semantic_weight", "bm25_score", "semantic_weight", "semantic_score", "chunk", "chunk_data", "get", "chunk_id", "combined_results", "append", "embeddingresult", "chunk_id", "chunk_id", "similarity_score", "semantic_score", "bm25_score", "bm25_score", "combined_score", "combined_score", "content", "chunk", "get", "content", "file_path", "chunk", "get", "file_path", "ordena", "por", "score", "combinado", "combined_results", "sort", "key", "lambda", "combined_score", "reverse", "true", "return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink"]}
{"chunk_id": "88d435b73dc3de43f9321f60", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 273, "end_line": 313, "content": "        \n        return combined_results[:top_k]\n    \n    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk espec√≠fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em mem√≥ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["return", "combined_results", "top_k", "def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "a36cb181613158755970a97f", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 276, "end_line": 313, "content": "    def invalidate_cache(self, chunk_id: str):\n        \"\"\"Remove embedding do cache para um chunk espec√≠fico\"\"\"\n        cache_path = self._get_embedding_cache_path(chunk_id)\n        metadata_path = self._get_metadata_cache_path(chunk_id)\n        \n        if cache_path.exists():\n            cache_path.unlink()\n        if metadata_path.exists():\n            metadata_path.unlink()\n            \n        # Remove do cache em mem√≥ria\n        self.embeddings_cache.pop(chunk_id, None)\n        self.metadata_cache.pop(chunk_id, None)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "invalidate_cache", "self", "chunk_id", "str", "remove", "embedding", "do", "cache", "para", "um", "chunk", "espec", "fico", "cache_path", "self", "_get_embedding_cache_path", "chunk_id", "metadata_path", "self", "_get_metadata_cache_path", "chunk_id", "if", "cache_path", "exists", "cache_path", "unlink", "if", "metadata_path", "exists", "metadata_path", "unlink", "remove", "do", "cache", "em", "mem", "ria", "self", "embeddings_cache", "pop", "chunk_id", "none", "self", "metadata_cache", "pop", "chunk_id", "none", "def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "65018965f1f33654849aab73", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 290, "end_line": 313, "content": "    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Retorna estat√≠sticas do cache de embeddings\"\"\"\n        embedding_files = list(self.embeddings_dir.glob(\"*.npy\"))\n        metadata_files = list(self.embeddings_dir.glob(\"*_meta.json\"))\n        \n        total_size = sum(f.stat().st_size for f in embedding_files + metadata_files)\n        \n        return {\n            'enabled': HAS_SENTENCE_TRANSFORMERS and self.model is not None,\n            'model_name': self.model_name,\n            'cached_embeddings': len(embedding_files),\n            'cache_size_mb': round(total_size / (1024 * 1024), 2),\n            'in_memory_cache': len(self.embeddings_cache)\n        }\n    \n    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "get_cache_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "de", "embeddings", "embedding_files", "list", "self", "embeddings_dir", "glob", "npy", "metadata_files", "list", "self", "embeddings_dir", "glob", "_meta", "json", "total_size", "sum", "stat", "st_size", "for", "in", "embedding_files", "metadata_files", "return", "enabled", "has_sentence_transformers", "and", "self", "model", "is", "not", "none", "model_name", "self", "model_name", "cached_embeddings", "len", "embedding_files", "cache_size_mb", "round", "total_size", "in_memory_cache", "len", "self", "embeddings_cache", "def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "03511a5eb7501204553fbd7d", "file_path": "mcp_system/embeddings/semantic_search.py", "start_line": 305, "end_line": 313, "content": "    def clear_cache(self):\n        \"\"\"Limpa todo o cache de embeddings\"\"\"\n        import shutil\n        if self.embeddings_dir.exists():\n            shutil.rmtree(self.embeddings_dir)\n            self.embeddings_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.embeddings_cache.clear()\n        self.metadata_cache.clear()", "mtime": 1756159582.9433653, "terms": ["def", "clear_cache", "self", "limpa", "todo", "cache", "de", "embeddings", "import", "shutil", "if", "self", "embeddings_dir", "exists", "shutil", "rmtree", "self", "embeddings_dir", "self", "embeddings_dir", "mkdir", "parents", "true", "exist_ok", "true", "self", "embeddings_cache", "clear", "self", "metadata_cache", "clear"]}
{"chunk_id": "c2cded8a1a0c55659753faf0", "file_path": "mcp_system/cache/search_cache.py", "start_line": 1, "end_line": 80, "content": "#!/usr/bin/env python3\n\"\"\"\nM√≥dulo para gerenciamento de cache de resultados de busca\n\"\"\"\nimport json\nimport hashlib\nimport time\nfrom typing import Any, Dict, Optional\nimport pathlib\n\n# Removendo imports n√£o utilizados que podem causar problemas\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\nclass SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()", "mtime": 1756155043.6534107, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gerenciamento", "de", "cache", "de", "resultados", "de", "busca", "import", "json", "import", "hashlib", "import", "time", "from", "typing", "import", "any", "dict", "optional", "import", "pathlib", "removendo", "imports", "utilizados", "que", "podem", "causar", "problemas", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "4ce7917802936d4d062ceeab", "file_path": "mcp_system/cache/search_cache.py", "start_line": 16, "end_line": 95, "content": "class SearchCache:\n    \"\"\"Sistema de cache para resultados de busca\"\"\"\n\n    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio", "mtime": 1756155043.6534107, "terms": ["class", "searchcache", "sistema", "de", "cache", "para", "resultados", "de", "busca", "def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio"]}
{"chunk_id": "3af3a9d92cf861865651f60e", "file_path": "mcp_system/cache/search_cache.py", "start_line": 19, "end_line": 98, "content": "    def __init__(self, max_size: int = 100, ttl_seconds: int = 3600):\n        \"\"\"\n        Inicializa o cache de buscas.\n\n        Args:\n            max_size: N√∫mero m√°ximo de entradas no cache\n            ttl_seconds: Tempo de vida das entradas em segundos\n        \"\"\"\n        self.max_size = max_size\n        self.ttl_seconds = ttl_seconds\n        self.cache: Dict[str, Dict[str, Any]] = {}\n        self.access_order: Dict[str, float] = {}  # Para LRU\n\n    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:", "mtime": 1756155043.6534107, "terms": ["def", "__init__", "self", "max_size", "int", "ttl_seconds", "int", "inicializa", "cache", "de", "buscas", "args", "max_size", "mero", "ximo", "de", "entradas", "no", "cache", "ttl_seconds", "tempo", "de", "vida", "das", "entradas", "em", "segundos", "self", "max_size", "max_size", "self", "ttl_seconds", "ttl_seconds", "self", "cache", "dict", "str", "dict", "str", "any", "self", "access_order", "dict", "str", "float", "para", "lru", "def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order"]}
{"chunk_id": "e4950e5693b80d741bb5ab62", "file_path": "mcp_system/cache/search_cache.py", "start_line": 32, "end_line": 111, "content": "    def _generate_key(self, query: str, **kwargs) -> str:\n        \"\"\"\n        Gera uma chave √∫nica para uma consulta e seus par√¢metros.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Chave hash √∫nica para o cache\n        \"\"\"\n        # Criar um dicion√°rio ordenado com todos os par√¢metros\n        params = {\"query\": query, **kwargs}\n\n        # Ordenar as chaves para garantir consist√™ncia\n        sorted_params = {k: params[k] for k in sorted(params.keys())}\n\n        # Converter para JSON e gerar hash\n        params_str = json.dumps(sorted_params, sort_keys=True, default=str)\n        return hashlib.md5(params_str.encode()).hexdigest()\n\n    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n", "mtime": 1756155043.6534107, "terms": ["def", "_generate_key", "self", "query", "str", "kwargs", "str", "gera", "uma", "chave", "nica", "para", "uma", "consulta", "seus", "par", "metros", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "chave", "hash", "nica", "para", "cache", "criar", "um", "dicion", "rio", "ordenado", "com", "todos", "os", "par", "metros", "params", "query", "query", "kwargs", "ordenar", "as", "chaves", "para", "garantir", "consist", "ncia", "sorted_params", "params", "for", "in", "sorted", "params", "keys", "converter", "para", "json", "gerar", "hash", "params_str", "json", "dumps", "sorted_params", "sort_keys", "true", "default", "str", "return", "hashlib", "md5", "params_str", "encode", "hexdigest", "def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time"]}
{"chunk_id": "a85b1314fc4a5fe1e9dbeecd", "file_path": "mcp_system/cache/search_cache.py", "start_line": 53, "end_line": 132, "content": "    def get(self, query: str, **kwargs) -> Optional[Any]:\n        \"\"\"\n        Recupera resultados do cache se dispon√≠veis e v√°lidos.\n\n        Args:\n            query: Texto da consulta\n            **kwargs: Outros par√¢metros da consulta\n\n        Returns:\n            Resultados em cache ou None se n√£o dispon√≠veis/inv√°lidos\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Verificar se a chave existe no cache\n        if key not in self.cache:\n            return None\n\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)", "mtime": 1756155043.6534107, "terms": ["def", "get", "self", "query", "str", "kwargs", "optional", "any", "recupera", "resultados", "do", "cache", "se", "dispon", "veis", "lidos", "args", "query", "texto", "da", "consulta", "kwargs", "outros", "par", "metros", "da", "consulta", "returns", "resultados", "em", "cache", "ou", "none", "se", "dispon", "veis", "inv", "lidos", "key", "self", "_generate_key", "query", "kwargs", "verificar", "se", "chave", "existe", "no", "cache", "if", "key", "not", "in", "self", "cache", "return", "none", "verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key"]}
{"chunk_id": "8dbd353be41fcf6167fb5e5a", "file_path": "mcp_system/cache/search_cache.py", "start_line": 69, "end_line": 148, "content": "\n        # Verificar se a entrada expirou\n        entry = self.cache[key]\n        if time.time() - entry[\"timestamp\"] > self.ttl_seconds:\n            # Remover entrada expirada\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n            return None\n\n        # Atualizar ordem de acesso para LRU\n        self.access_order[key] = time.time()\n\n        return entry[\"results\"]\n\n    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n", "mtime": 1756155043.6534107, "terms": ["verificar", "se", "entrada", "expirou", "entry", "self", "cache", "key", "if", "time", "time", "entry", "timestamp", "self", "ttl_seconds", "remover", "entrada", "expirada", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "return", "none", "atualizar", "ordem", "de", "acesso", "para", "lru", "self", "access_order", "key", "time", "time", "return", "entry", "results", "def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache"]}
{"chunk_id": "41301c4e9afe3ef1d0779512", "file_path": "mcp_system/cache/search_cache.py", "start_line": 84, "end_line": 163, "content": "    def set(self, query: str, results: Any, **kwargs) -> None:\n        \"\"\"\n        Armazena resultados no cache.\n\n        Args:\n            query: Texto da consulta\n            results: Resultados para armazenar\n            **kwargs: Outros par√¢metros da consulta\n        \"\"\"\n        key = self._generate_key(query, **kwargs)\n\n        # Remover entradas antigas se o cache estiver cheio\n        if len(self.cache) >= self.max_size:\n            # Encontrar a entrada LRU (menor timestamp de acesso)\n            if self.access_order:\n                lru_key = min(self.access_order.keys(), key=lambda k: self.access_order[k])\n                del self.cache[lru_key]\n                del self.access_order[lru_key]\n\n        # Armazenar resultados\n        self.cache[key] = {\n            \"results\": results,\n            \"timestamp\": time.time()\n        }\n\n        # Atualizar ordem de acesso\n        self.access_order[key] = time.time()\n\n    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }", "mtime": 1756155043.6534107, "terms": ["def", "set", "self", "query", "str", "results", "any", "kwargs", "none", "armazena", "resultados", "no", "cache", "args", "query", "texto", "da", "consulta", "results", "resultados", "para", "armazenar", "kwargs", "outros", "par", "metros", "da", "consulta", "key", "self", "_generate_key", "query", "kwargs", "remover", "entradas", "antigas", "se", "cache", "estiver", "cheio", "if", "len", "self", "cache", "self", "max_size", "encontrar", "entrada", "lru", "menor", "timestamp", "de", "acesso", "if", "self", "access_order", "lru_key", "min", "self", "access_order", "keys", "key", "lambda", "self", "access_order", "del", "self", "cache", "lru_key", "del", "self", "access_order", "lru_key", "armazenar", "resultados", "self", "cache", "key", "results", "results", "timestamp", "time", "time", "atualizar", "ordem", "de", "acesso", "self", "access_order", "key", "time", "time", "def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds"]}
{"chunk_id": "162a44b0d38ed2bc4fc558b0", "file_path": "mcp_system/cache/search_cache.py", "start_line": 112, "end_line": 167, "content": "    def invalidate_file(self, file_path: str) -> None:\n        \"\"\"\n        Invalida todas as entradas do cache relacionadas a um arquivo espec√≠fico.\n\n        Args:\n            file_path: Caminho do arquivo que foi modificado\n        \"\"\"\n        keys_to_remove = []\n\n        # Identificar entradas relacionadas ao arquivo\n        for key, entry in self.cache.items():\n            cached_results = entry.get(\"results\", [])\n\n            # Verificar se os resultados cont√™m refer√™ncias ao arquivo\n            if isinstance(cached_results, list):\n                for result in cached_results:\n                    if isinstance(result, dict) and result.get(\"file_path\") == file_path:\n                        keys_to_remove.append(key)\n                        break\n            elif isinstance(cached_results, dict) and cached_results.get(\"file\") == file_path:\n                keys_to_remove.append(key)\n\n        # Remover entradas relacionadas ao arquivo\n        for key in keys_to_remove:\n            del self.cache[key]\n            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "invalidate_file", "self", "file_path", "str", "none", "invalida", "todas", "as", "entradas", "do", "cache", "relacionadas", "um", "arquivo", "espec", "fico", "args", "file_path", "caminho", "do", "arquivo", "que", "foi", "modificado", "keys_to_remove", "identificar", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "entry", "in", "self", "cache", "items", "cached_results", "entry", "get", "results", "verificar", "se", "os", "resultados", "cont", "refer", "ncias", "ao", "arquivo", "if", "isinstance", "cached_results", "list", "for", "result", "in", "cached_results", "if", "isinstance", "result", "dict", "and", "result", "get", "file_path", "file_path", "keys_to_remove", "append", "key", "break", "elif", "isinstance", "cached_results", "dict", "and", "cached_results", "get", "file", "file_path", "keys_to_remove", "append", "key", "remover", "entradas", "relacionadas", "ao", "arquivo", "for", "key", "in", "keys_to_remove", "del", "self", "cache", "key", "if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "0924278789039792ac131142", "file_path": "mcp_system/cache/search_cache.py", "start_line": 137, "end_line": 167, "content": "            if key in self.access_order:\n                del self.access_order[key]\n\n    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["if", "key", "in", "self", "access_order", "del", "self", "access_order", "key", "def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "a226be655a743135c6e772fa", "file_path": "mcp_system/cache/search_cache.py", "start_line": 140, "end_line": 167, "content": "    def clear(self) -> None:\n        \"\"\"Limpa todo o cache.\"\"\"\n        self.cache.clear()\n        self.access_order.clear()\n\n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "clear", "self", "none", "limpa", "todo", "cache", "self", "cache", "clear", "self", "access_order", "clear", "def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "1de80eddef7e1d6890a15169", "file_path": "mcp_system/cache/search_cache.py", "start_line": 145, "end_line": 167, "content": "    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Retorna estat√≠sticas do cache.\n\n        Returns:\n            Dicion√°rio com estat√≠sticas do cache\n        \"\"\"\n        current_time = time.time()\n        expired_count = sum(\n            1 for entry in self.cache.values()\n            if current_time - entry[\"timestamp\"] > self.ttl_seconds\n        )\n\n        return {\n            \"total_entries\": len(self.cache),\n            \"max_size\": self.max_size,\n            \"expired_entries\": expired_count,\n            \"ttl_seconds\": self.ttl_seconds\n        }\n\n\n# Inst√¢ncia singleton para uso global\nsearch_cache = SearchCache()", "mtime": 1756155043.6534107, "terms": ["def", "get_stats", "self", "dict", "str", "any", "retorna", "estat", "sticas", "do", "cache", "returns", "dicion", "rio", "com", "estat", "sticas", "do", "cache", "current_time", "time", "time", "expired_count", "sum", "for", "entry", "in", "self", "cache", "values", "if", "current_time", "entry", "timestamp", "self", "ttl_seconds", "return", "total_entries", "len", "self", "cache", "max_size", "self", "max_size", "expired_entries", "expired_count", "ttl_seconds", "self", "ttl_seconds", "inst", "ncia", "singleton", "para", "uso", "global", "search_cache", "searchcache"]}
{"chunk_id": "82497e02d05040ef40048507", "file_path": "mcp_system/cache/__init__.py", "start_line": 1, "end_line": 6, "content": "# MCP System - Cache Module\n\"\"\"\nSistema de cache para embeddings e √≠ndices BM25.\n\"\"\"\n\nfrom .search_cache import *", "mtime": 1755701282.161089, "terms": ["mcp", "system", "cache", "module", "sistema", "de", "cache", "para", "embeddings", "ndices", "bm25", "from", "search_cache", "import"]}
{"chunk_id": "ba991c54d630e62f500bc537", "file_path": "mcp_system/utils/__init__.py", "start_line": 1, "end_line": 7, "content": "# MCP System - Utils Module\n\"\"\"\nUtilit√°rios do sistema MCP: embeddings, file watcher, etc.\n\"\"\"\n\nfrom .embeddings import *\nfrom .file_watcher import *", "mtime": 1755701275.657908, "terms": ["mcp", "system", "utils", "module", "utilit", "rios", "do", "sistema", "mcp", "embeddings", "file", "watcher", "etc", "from", "embeddings", "import", "from", "file_watcher", "import"]}
{"chunk_id": "30daac062edbf0d2c61b5086", "file_path": "mcp_system/utils/embeddings.py", "start_line": 1, "end_line": 70, "content": "#!/usr/bin/env python3\n\"\"\"\nM√≥dulo para gera√ß√£o de embeddings usando Sentence Transformers\n\"\"\"\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nfrom typing import Union\n\n\nclass EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["usr", "bin", "env", "python3", "dulo", "para", "gera", "de", "embeddings", "usando", "sentence", "transformers", "from", "sentence_transformers", "import", "sentencetransformer", "import", "numpy", "as", "np", "from", "typing", "import", "union", "class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "41af9880584da37a250fc4a2", "file_path": "mcp_system/utils/embeddings.py", "start_line": 10, "end_line": 70, "content": "class EmbeddingModel:\n    \"\"\"Classe para gerenciar o modelo de embeddings\"\"\"\n    \n    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["class", "embeddingmodel", "classe", "para", "gerenciar", "modelo", "de", "embeddings", "def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "66bd7019da4646ec07d6b2a0", "file_path": "mcp_system/utils/embeddings.py", "start_line": 13, "end_line": 70, "content": "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n        \"\"\"\n        Inicializa o modelo de embeddings.\n        \n        Args:\n            model_name: Nome do modelo SentenceTransformer a ser usado\n        \"\"\"\n        self.model_name = model_name\n        self.model = None\n        self._load_model()\n    \n    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "__init__", "self", "model_name", "str", "all", "minilm", "l6", "v2", "inicializa", "modelo", "de", "embeddings", "args", "model_name", "nome", "do", "modelo", "sentencetransformer", "ser", "usado", "self", "model_name", "model_name", "self", "model", "none", "self", "_load_model", "def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "fdd17cea8efc2913e72e6547", "file_path": "mcp_system/utils/embeddings.py", "start_line": 24, "end_line": 70, "content": "    def _load_model(self):\n        \"\"\"Carrega o modelo de embeddings\"\"\"\n        try:\n            self.model = SentenceTransformer(self.model_name)\n            # Removendo todas as mensagens de impress√£o que podem interferir no parsing JSON\n        except Exception:\n            # Fallback para modelo mais simples\n            try:\n                self.model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            except Exception:\n                self.model = None\n    \n    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "_load_model", "self", "carrega", "modelo", "de", "embeddings", "try", "self", "model", "sentencetransformer", "self", "model_name", "removendo", "todas", "as", "mensagens", "de", "impress", "que", "podem", "interferir", "no", "parsing", "json", "except", "exception", "fallback", "para", "modelo", "mais", "simples", "try", "self", "model", "sentencetransformer", "all", "minilm", "l6", "v2", "except", "exception", "self", "model", "none", "def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "3dc38ce4a72483f2403b4462", "file_path": "mcp_system/utils/embeddings.py", "start_line": 36, "end_line": 70, "content": "    def embed_text(self, text: str) -> np.ndarray:\n        \"\"\"\n        Gera embedding para um texto.\n\n        Args:\n            text: Texto para gerar embedding\n\n        Returns:\n            Array numpy com o embedding\n        \"\"\"\n        if self.model is None:\n            # Retorna embedding aleat√≥rio se modelo n√£o estiver dispon√≠vel\n            return np.random.rand(384).astype(np.float32)\n\n        try:\n            embedding = self.model.encode(text)\n            return np.array(embedding, dtype=np.float32)\n        except Exception:\n            # Retorna embedding aleat√≥rio em caso de erro\n            return np.random.rand(384).astype(np.float32)\n    \n    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "embed_text", "self", "text", "str", "np", "ndarray", "gera", "embedding", "para", "um", "texto", "args", "text", "texto", "para", "gerar", "embedding", "returns", "array", "numpy", "com", "embedding", "if", "self", "model", "is", "none", "retorna", "embedding", "aleat", "rio", "se", "modelo", "estiver", "dispon", "vel", "return", "np", "random", "rand", "astype", "np", "float32", "try", "embedding", "self", "model", "encode", "text", "return", "np", "array", "embedding", "dtype", "np", "float32", "except", "exception", "retorna", "embedding", "aleat", "rio", "em", "caso", "de", "erro", "return", "np", "random", "rand", "astype", "np", "float32", "def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "3fb90fa9e3e9400815788996", "file_path": "mcp_system/utils/embeddings.py", "start_line": 57, "end_line": 70, "content": "    def get_embedding_dimension(self) -> int:\n        \"\"\"\n        Retorna a dimens√£o dos embeddings gerados pelo modelo.\n        \n        Returns:\n            Dimens√£o dos embeddings\n        \"\"\"\n        if self.model is None:\n            return 384  # Dimens√£o padr√£o para modelos SentenceTransformer pequenos\n        return self.model.get_sentence_embedding_dimension()\n\n\n# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["def", "get_embedding_dimension", "self", "int", "retorna", "dimens", "dos", "embeddings", "gerados", "pelo", "modelo", "returns", "dimens", "dos", "embeddings", "if", "self", "model", "is", "none", "return", "dimens", "padr", "para", "modelos", "sentencetransformer", "pequenos", "return", "self", "model", "get_sentence_embedding_dimension", "inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "973af24920e98d12e092952c", "file_path": "mcp_system/utils/embeddings.py", "start_line": 69, "end_line": 70, "content": "# Inst√¢ncia singleton para uso global\nembedding_model = EmbeddingModel()", "mtime": 1755690518.2098587, "terms": ["inst", "ncia", "singleton", "para", "uso", "global", "embedding_model", "embeddingmodel"]}
{"chunk_id": "09d216e43f6f919e65f213a9", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 1, "end_line": 80, "content": "# src/utils/file_watcher.py\n\"\"\"\nSistema de monitoramento de arquivos para auto-indexa√ß√£o\nDetecta mudan√ßas e reindexar automaticamente arquivos modificados\n\"\"\"\n\nfrom __future__ import annotations\nimport os\nimport time\nimport threading\nfrom pathlib import Path\nfrom typing import Set, Callable, Dict, Optional\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\nimport hashlib\nimport pathlib\n\nimport sys\n\n# Obter o diret√≥rio do script atual\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\n\ntry:\n    from watchdog.observers import Observer\n    from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent\n    HAS_WATCHDOG = True\nexcept ImportError:\n    HAS_WATCHDOG = False\n    Observer = None\n    FileSystemEventHandler = None\n\n@dataclass\nclass IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False", "mtime": 1756155021.429876, "terms": ["src", "utils", "file_watcher", "py", "sistema", "de", "monitoramento", "de", "arquivos", "para", "auto", "indexa", "detecta", "mudan", "as", "reindexar", "automaticamente", "arquivos", "modificados", "from", "__future__", "import", "annotations", "import", "os", "import", "time", "import", "threading", "from", "pathlib", "import", "path", "from", "typing", "import", "set", "callable", "dict", "optional", "from", "dataclasses", "import", "dataclass", "from", "concurrent", "futures", "import", "threadpoolexecutor", "import", "hashlib", "import", "pathlib", "import", "sys", "obter", "diret", "rio", "do", "script", "atual", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "try", "from", "watchdog", "observers", "import", "observer", "from", "watchdog", "events", "import", "filesystemeventhandler", "filemodifiedevent", "filecreatedevent", "filedeletedevent", "has_watchdog", "true", "except", "importerror", "has_watchdog", "false", "observer", "none", "filesystemeventhandler", "none", "dataclass", "class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false"]}
{"chunk_id": "011ca07bac2167d13f0fc625", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 33, "end_line": 112, "content": "class IndexingTask:\n    file_path: Path\n    action: str  # 'created', 'modified', 'deleted'\n    timestamp: float\n\nclass FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce", "mtime": 1756155021.429876, "terms": ["class", "indexingtask", "file_path", "path", "action", "str", "created", "modified", "deleted", "timestamp", "float", "class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce"]}
{"chunk_id": "14912b31261e36f9f6af6810", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 38, "end_line": 117, "content": "class FileWatcher:\n    \"\"\"\n    Sistema de monitoramento de arquivos que detecta mudan√ßas\n    e agenda reindexa√ß√£o autom√°tica\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:", "mtime": 1756155021.429876, "terms": ["class", "filewatcher", "sistema", "de", "monitoramento", "de", "arquivos", "que", "detecta", "mudan", "as", "agenda", "reindexa", "autom", "tica", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer"]}
{"chunk_id": "c9d32597fbc33fe112445f6b", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 44, "end_line": 123, "content": "    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 debounce_seconds: float = 2.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.debounce_seconds = debounce_seconds\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.observer = None\n        self.event_handler = None\n        self.is_running = False\n        \n        # Sistema de debouncing\n        self.pending_tasks: Dict[str, IndexingTask] = {}\n        self.task_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"FileWatcher\")\n        self.debounce_timer = None\n        \n        # Estat√≠sticas\n        self.stats = {\n            'files_monitored': 0,\n            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "debounce_seconds", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "debounce_seconds", "debounce_seconds", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "observer", "none", "self", "event_handler", "none", "self", "is_running", "false", "sistema", "de", "debouncing", "self", "pending_tasks", "dict", "str", "indexingtask", "self", "task_executor", "threadpoolexecutor", "max_workers", "thread_name_prefix", "filewatcher", "self", "debounce_timer", "none", "estat", "sticas", "self", "stats", "files_monitored", "events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks"]}
{"chunk_id": "edd472a9074578137a38711a", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 69, "end_line": 148, "content": "            'events_processed': 0,\n            'last_indexing': None,\n            'errors': 0\n        }\n        \n        if not HAS_WATCHDOG:\n            print(\"‚ö†Ô∏è  watchdog n√£o encontrado. Auto-indexa√ß√£o desabilitada.\", file=sys.stderr)\n    \n    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    ", "mtime": 1756155021.429876, "terms": ["events_processed", "last_indexing", "none", "errors", "if", "not", "has_watchdog", "print", "watchdog", "encontrado", "auto", "indexa", "desabilitada", "file", "sys", "stderr", "def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear"]}
{"chunk_id": "0f754497948ef15b4065dfe6", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 77, "end_line": 156, "content": "    def _should_process_file(self, file_path: Path) -> bool:\n        \"\"\"Verifica se arquivo deve ser processado\"\"\"\n        if not file_path.is_file():\n            return False\n            \n        # Verifica extens√£o\n        if file_path.suffix not in self.include_extensions:\n            return False\n        \n        # Ignora arquivos em diret√≥rios espec√≠ficos\n        ignore_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'dist', 'build'}\n        if any(part in ignore_dirs for part in file_path.parts):\n            return False\n            \n        # Ignora arquivos tempor√°rios\n        if file_path.name.startswith('.') and file_path.suffix in {'.tmp', '.swp', '.bak'}:\n            return False\n            \n        return True\n    \n    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return", "mtime": 1756155021.429876, "terms": ["def", "_should_process_file", "self", "file_path", "path", "bool", "verifica", "se", "arquivo", "deve", "ser", "processado", "if", "not", "file_path", "is_file", "return", "false", "verifica", "extens", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "return", "false", "ignora", "arquivos", "em", "diret", "rios", "espec", "ficos", "ignore_dirs", "git", "node_modules", "__pycache__", "venv", "dist", "build", "if", "any", "part", "in", "ignore_dirs", "for", "part", "in", "file_path", "parts", "return", "false", "ignora", "arquivos", "tempor", "rios", "if", "file_path", "name", "startswith", "and", "file_path", "suffix", "in", "tmp", "swp", "bak", "return", "false", "return", "true", "def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return"]}
{"chunk_id": "4a4fdd2c991b434d8a103e07", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 97, "end_line": 176, "content": "    def _add_indexing_task(self, file_path: Path, action: str):\n        \"\"\"Adiciona tarefa de indexa√ß√£o com debouncing\"\"\"\n        if not self._should_process_file(file_path):\n            return\n            \n        task = IndexingTask(\n            file_path=file_path,\n            action=action,\n            timestamp=time.time()\n        )\n        \n        # Usa caminho absoluto como chave para debouncing\n        key = str(file_path.resolve())\n        self.pending_tasks[key] = task\n        \n        # Agenda processamento com debounce\n        self._schedule_processing()\n    \n    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n", "mtime": 1756155021.429876, "terms": ["def", "_add_indexing_task", "self", "file_path", "path", "action", "str", "adiciona", "tarefa", "de", "indexa", "com", "debouncing", "if", "not", "self", "_should_process_file", "file_path", "return", "task", "indexingtask", "file_path", "file_path", "action", "action", "timestamp", "time", "time", "usa", "caminho", "absoluto", "como", "chave", "para", "debouncing", "key", "str", "file_path", "resolve", "self", "pending_tasks", "key", "task", "agenda", "processamento", "com", "debounce", "self", "_schedule_processing", "def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr"]}
{"chunk_id": "37256eaccd29aafd6cca356c", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 115, "end_line": 194, "content": "    def _schedule_processing(self):\n        \"\"\"Agenda processamento das tarefas pendentes com debounce\"\"\"\n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n        \n        self.debounce_timer = threading.Timer(\n            self.debounce_seconds,\n            self._process_pending_tasks\n        )\n        self.debounce_timer.daemon = True\n        self.debounce_timer.start()\n    \n    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')", "mtime": 1756155021.429876, "terms": ["def", "_schedule_processing", "self", "agenda", "processamento", "das", "tarefas", "pendentes", "com", "debounce", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "debounce_timer", "threading", "timer", "self", "debounce_seconds", "self", "_process_pending_tasks", "self", "debounce_timer", "daemon", "true", "self", "debounce_timer", "start", "def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted"]}
{"chunk_id": "15cdb388361c58c077d75159", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 127, "end_line": 206, "content": "    def _process_pending_tasks(self):\n        \"\"\"Processa todas as tarefas pendentes\"\"\"\n        if not self.pending_tasks:\n            return\n            \n        # Agrupa tarefas por a√ß√£o\n        tasks_by_action = {'created': [], 'modified': [], 'deleted': []}\n        \n        for task in self.pending_tasks.values():\n            if task.action in tasks_by_action:\n                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)", "mtime": 1756155021.429876, "terms": ["def", "_process_pending_tasks", "self", "processa", "todas", "as", "tarefas", "pendentes", "if", "not", "self", "pending_tasks", "return", "agrupa", "tarefas", "por", "tasks_by_action", "created", "modified", "deleted", "for", "task", "in", "self", "pending_tasks", "values", "if", "task", "action", "in", "tasks_by_action", "tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr"]}
{"chunk_id": "f09d96deefb43f34c7347baa", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 137, "end_line": 216, "content": "                tasks_by_action[task.action].append(task.file_path)\n        \n        # Processa tarefas\n        try:\n            self._execute_indexing_tasks(tasks_by_action)\n            self.stats['last_indexing'] = time.time()\n        except Exception as e:\n            print(f\"‚ùå Erro no processamento de tarefas: {e}\")\n            self.stats['errors'] += 1\n        finally:\n            self.pending_tasks.clear()\n    \n    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )", "mtime": 1756155021.429876, "terms": ["tasks_by_action", "task", "action", "append", "task", "file_path", "processa", "tarefas", "try", "self", "_execute_indexing_tasks", "tasks_by_action", "self", "stats", "last_indexing", "time", "time", "except", "exception", "as", "print", "erro", "no", "processamento", "de", "tarefas", "self", "stats", "errors", "finally", "self", "pending_tasks", "clear", "def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true"]}
{"chunk_id": "793a8eff9eccfd7666282c5e", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 149, "end_line": 228, "content": "    def _execute_indexing_tasks(self, tasks_by_action: Dict[str, list]):\n        \"\"\"Executa as tarefas de indexa√ß√£o agrupadas\"\"\"\n        if not self.indexer_callback:\n            return\n\n        total_files = sum(len(files) for files in tasks_by_action.values())\n        if total_files == 0:\n            return\n\n        print(f\"üîÑ Processando {total_files} arquivo(s) modificado(s)...\", file=sys.stderr)\n\n        # Processa arquivos criados/modificados\n        files_to_index = tasks_by_action['created'] + tasks_by_action['modified']\n        if files_to_index:\n            try:\n                result = self.indexer_callback(files_to_index)\n                indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(files_to_index)\n                print(f\"‚úÖ {indexed_count} arquivo(s) indexado(s)\", file=sys.stderr)\n                self.stats['events_processed'] += indexed_count\n            except Exception as e:\n                print(f\"‚ùå Erro ao indexar arquivos: {e}\", file=sys.stderr)\n                self.stats['errors'] += 1\n\n        # TODO: Implementar remo√ß√£o de chunks para arquivos deletados\n        deleted_files = tasks_by_action['deleted']\n        if deleted_files:\n            print(f\"‚ÑπÔ∏è  {len(deleted_files)} arquivo(s) deletado(s) (remo√ß√£o de √≠ndice n√£o implementada)\", file=sys.stderr)\n\nclass WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)", "mtime": 1756155021.429876, "terms": ["def", "_execute_indexing_tasks", "self", "tasks_by_action", "dict", "str", "list", "executa", "as", "tarefas", "de", "indexa", "agrupadas", "if", "not", "self", "indexer_callback", "return", "total_files", "sum", "len", "files", "for", "files", "in", "tasks_by_action", "values", "if", "total_files", "return", "print", "processando", "total_files", "arquivo", "modificado", "file", "sys", "stderr", "processa", "arquivos", "criados", "modificados", "files_to_index", "tasks_by_action", "created", "tasks_by_action", "modified", "if", "files_to_index", "try", "result", "self", "indexer_callback", "files_to_index", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "files_to_index", "print", "indexed_count", "arquivo", "indexado", "file", "sys", "stderr", "self", "stats", "events_processed", "indexed_count", "except", "exception", "as", "print", "erro", "ao", "indexar", "arquivos", "file", "sys", "stderr", "self", "stats", "errors", "todo", "implementar", "remo", "de", "chunks", "para", "arquivos", "deletados", "deleted_files", "tasks_by_action", "deleted", "if", "deleted_files", "print", "len", "deleted_files", "arquivo", "deletado", "remo", "de", "ndice", "implementada", "file", "sys", "stderr", "class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr"]}
{"chunk_id": "5648195fbcefb785eedf8b50", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 177, "end_line": 256, "content": "class WatchdogHandler(FileSystemEventHandler):\n    \"\"\"Handler para eventos do watchdog\"\"\"\n    \n    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:", "mtime": 1756155021.429876, "terms": ["class", "watchdoghandler", "filesystemeventhandler", "handler", "para", "eventos", "do", "watchdog", "def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as"]}
{"chunk_id": "24b82ec795de19a23c981cd8", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 180, "end_line": 259, "content": "    def __init__(self, watcher: FileWatcher):\n        self.watcher = watcher\n        super().__init__()\n    \n    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watcher", "filewatcher", "self", "watcher", "watcher", "super", "__init__", "def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict"]}
{"chunk_id": "84f0c44c477e3317dd6a16ad", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 184, "end_line": 263, "content": "    def on_created(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'created')\n    \n    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,", "mtime": 1756155021.429876, "terms": ["def", "on_created", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "created", "def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running"]}
{"chunk_id": "6ae19459e25e66a982d7b9e5", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 188, "end_line": 267, "content": "    def on_modified(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'modified')\n    \n    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats", "mtime": 1756155021.429876, "terms": ["def", "on_modified", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "modified", "def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats"]}
{"chunk_id": "4fbc0db3cf208d0aa61b9b57", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 192, "end_line": 271, "content": "    def on_deleted(self, event):\n        if not event.is_directory:\n            self.watcher._add_indexing_task(Path(event.src_path), 'deleted')\n\nclass FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"", "mtime": 1756155021.429876, "terms": ["def", "on_deleted", "self", "event", "if", "not", "event", "is_directory", "self", "watcher", "_add_indexing_task", "path", "event", "src_path", "deleted", "class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher"]}
{"chunk_id": "5cc1643c33d4eaae76df7067", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 196, "end_line": 275, "content": "class FileWatcher(FileWatcher):\n    \"\"\"Extens√£o da classe FileWatcher com watchdog\"\"\"\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    ", "mtime": 1756155021.429876, "terms": ["class", "filewatcher", "filewatcher", "extens", "da", "classe", "filewatcher", "com", "watchdog", "def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as"]}
{"chunk_id": "26c1b5de96520a92ac1fcb68", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 199, "end_line": 278, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o monitoramento de arquivos\"\"\"\n        if not HAS_WATCHDOG:\n            print(\"‚ùå watchdog n√£o dispon√≠vel. Auto-indexa√ß√£o n√£o pode ser iniciada.\", file=sys.stderr)\n            return False\n\n        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,", "mtime": 1756155021.429876, "terms": ["def", "start", "self", "bool", "inicia", "monitoramento", "de", "arquivos", "if", "not", "has_watchdog", "print", "watchdog", "dispon", "vel", "auto", "indexa", "pode", "ser", "iniciada", "file", "sys", "stderr", "return", "false", "if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none"]}
{"chunk_id": "b4dda0319994c5372a6e4af0", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 205, "end_line": 284, "content": "        if self.is_running:\n            print(\"‚ö†Ô∏è  File watcher j√° est√° rodando\", file=sys.stderr)\n            return True\n            \n        try:\n            self.event_handler = WatchdogHandler(self)\n            self.observer = Observer()\n            self.observer.schedule(\n                self.event_handler, \n                str(self.watch_path), \n                recursive=True\n            )\n            self.observer.start()\n            self.is_running = True\n            \n            # Conta arquivos monitorados\n            self._count_monitored_files()\n            \n            print(f\"‚úÖ File watcher iniciado em: {self.watch_path}\", file=sys.stderr)\n            print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\", file=sys.stderr)\n            return True\n\n        except Exception as e:\n            print(f\"‚ùå Erro ao iniciar file watcher: {e}\", file=sys.stderr)\n            return False\n    \n    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {", "mtime": 1756155021.429876, "terms": ["if", "self", "is_running", "print", "file", "watcher", "est", "rodando", "file", "sys", "stderr", "return", "true", "try", "self", "event_handler", "watchdoghandler", "self", "self", "observer", "observer", "self", "observer", "schedule", "self", "event_handler", "str", "self", "watch_path", "recursive", "true", "self", "observer", "start", "self", "is_running", "true", "conta", "arquivos", "monitorados", "self", "_count_monitored_files", "print", "file", "watcher", "iniciado", "em", "self", "watch_path", "file", "sys", "stderr", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "file", "sys", "stderr", "return", "true", "except", "exception", "as", "print", "erro", "ao", "iniciar", "file", "watcher", "file", "sys", "stderr", "return", "false", "def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or"]}
{"chunk_id": "08514bd3db9742a5f9caf378", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 231, "end_line": 310, "content": "    def stop(self):\n        \"\"\"Para o monitoramento de arquivos\"\"\"\n        if not self.is_running:\n            return\n            \n        if self.observer:\n            self.observer.stop()\n            self.observer.join(timeout=5)\n            \n        if self.debounce_timer:\n            self.debounce_timer.cancel()\n            \n        self.task_executor.shutdown(wait=True)\n        self.is_running = False\n        \n        print(\"‚úÖ File watcher parado\", file=sys.stderr)\n    \n    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []", "mtime": 1756155021.429876, "terms": ["def", "stop", "self", "para", "monitoramento", "de", "arquivos", "if", "not", "self", "is_running", "return", "if", "self", "observer", "self", "observer", "stop", "self", "observer", "join", "timeout", "if", "self", "debounce_timer", "self", "debounce_timer", "cancel", "self", "task_executor", "shutdown", "wait", "true", "self", "is_running", "false", "print", "file", "watcher", "parado", "file", "sys", "stderr", "def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files"]}
{"chunk_id": "8b64965c58475378f6acd0f3", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 248, "end_line": 327, "content": "    def _count_monitored_files(self):\n        \"\"\"Conta arquivos que est√£o sendo monitorados\"\"\"\n        count = 0\n        try:\n            for file_path in self.watch_path.rglob(\"*\"):\n                if self._should_process_file(file_path):\n                    count += 1\n            self.stats['files_monitored'] = count\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Erro ao contar arquivos: {e}\", file=sys.stderr)\n\n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou", "mtime": 1756155021.429876, "terms": ["def", "_count_monitored_files", "self", "conta", "arquivos", "que", "est", "sendo", "monitorados", "count", "try", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "self", "_should_process_file", "file_path", "count", "self", "stats", "files_monitored", "count", "except", "exception", "as", "print", "erro", "ao", "contar", "arquivos", "file", "sys", "stderr", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou"]}
{"chunk_id": "0f663d2eece14bd0e36fbfcf", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 259, "end_line": 338, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do file watcher\"\"\"\n        return {\n            'enabled': HAS_WATCHDOG,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'debounce_seconds': self.debounce_seconds,\n            'pending_tasks': len(self.pending_tasks),\n            **self.stats\n        }\n\nclass SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        ", "mtime": 1756155021.429876, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "file", "watcher", "return", "enabled", "has_watchdog", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "debounce_seconds", "self", "debounce_seconds", "pending_tasks", "len", "self", "pending_tasks", "self", "stats", "class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files"]}
{"chunk_id": "61ad2a38d1db52d21ef5471a", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 270, "end_line": 349, "content": "class SimpleFileWatcher:\n    \"\"\"\n    Fallback simples para quando watchdog n√£o est√° dispon√≠vel\n    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:", "mtime": 1756155021.429876, "terms": ["class", "simplefilewatcher", "fallback", "simples", "para", "quando", "watchdog", "est", "dispon", "vel", "usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files"]}
{"chunk_id": "07fc1cc6e8e8d2db8a1b4a6c", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 273, "end_line": 352, "content": "    Usa polling para detectar mudan√ßas\n    \"\"\"\n    \n    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n", "mtime": 1756155021.429876, "terms": ["usa", "polling", "para", "detectar", "mudan", "as", "def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr"]}
{"chunk_id": "501fe5697dc4944c3596c5fb", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 276, "end_line": 355, "content": "    def __init__(self, \n                 watch_path: str = str(CURRENT_DIR.parent.parent),\n                 indexer_callback: Optional[Callable] = None,\n                 poll_interval: float = 30.0,\n                 include_extensions: Optional[Set[str]] = None):\n        self.watch_path = Path(watch_path).resolve()\n        self.indexer_callback = indexer_callback\n        self.poll_interval = poll_interval\n        self.include_extensions = include_extensions or {\n            '.py', '.js', '.ts', '.tsx', '.jsx', '.java', '.go', '.rb', '.php',\n            '.c', '.cpp', '.h', '.hpp', '.cs', '.rs', '.swift', '.kt'\n        }\n        \n        self.file_hashes: Dict[str, str] = {}\n        self.is_running = False\n        self.poll_thread = None\n        \n        self.stats = {\n            'files_monitored': 0,\n            'polls_completed': 0,\n            'changes_detected': 0,\n            'last_poll': None\n        }\n    \n    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)", "mtime": 1756155021.429876, "terms": ["def", "__init__", "self", "watch_path", "str", "str", "current_dir", "parent", "parent", "indexer_callback", "optional", "callable", "none", "poll_interval", "float", "include_extensions", "optional", "set", "str", "none", "self", "watch_path", "path", "watch_path", "resolve", "self", "indexer_callback", "indexer_callback", "self", "poll_interval", "poll_interval", "self", "include_extensions", "include_extensions", "or", "py", "js", "ts", "tsx", "jsx", "java", "go", "rb", "php", "cpp", "hpp", "cs", "rs", "swift", "kt", "self", "file_hashes", "dict", "str", "str", "self", "is_running", "false", "self", "poll_thread", "none", "self", "stats", "files_monitored", "polls_completed", "changes_detected", "last_poll", "none", "def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files"]}
{"chunk_id": "091c98bf21cd4d72123ce821", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 300, "end_line": 379, "content": "    def _get_file_hash(self, file_path: Path) -> Optional[str]:\n        \"\"\"Calcula hash do arquivo para detectar mudan√ßas\"\"\"\n        try:\n            with open(file_path, 'rb') as f:\n                return hashlib.md5(f.read()).hexdigest()\n        except Exception:\n            return None\n    \n    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        ", "mtime": 1756155021.429876, "terms": ["def", "_get_file_hash", "self", "file_path", "path", "optional", "str", "calcula", "hash", "do", "arquivo", "para", "detectar", "mudan", "as", "try", "with", "open", "file_path", "rb", "as", "return", "hashlib", "md5", "read", "hexdigest", "except", "exception", "return", "none", "def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start"]}
{"chunk_id": "68181808871efe705d0aa982", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 308, "end_line": 387, "content": "    def _scan_for_changes(self):\n        \"\"\"Escaneia diret√≥rio em busca de mudan√ßas\"\"\"\n        changed_files = []\n        current_files = {}\n        \n        # Escaneia todos os arquivos relevantes\n        for file_path in self.watch_path.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            if file_path.suffix not in self.include_extensions:\n                continue\n                \n            str_path = str(file_path)\n            file_hash = self._get_file_hash(file_path)\n            \n            if file_hash:\n                current_files[str_path] = file_hash\n                \n                # Verifica se arquivo mudou\n                if str_path in self.file_hashes:\n                    if self.file_hashes[str_path] != file_hash:\n                        changed_files.append(file_path)\n                else:\n                    # Arquivo novo\n                    changed_files.append(file_path)\n        \n        # Atualiza cache de hashes\n        self.file_hashes = current_files\n        self.stats['files_monitored'] = len(current_files)\n        \n        return changed_files\n    \n    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():", "mtime": 1756155021.429876, "terms": ["def", "_scan_for_changes", "self", "escaneia", "diret", "rio", "em", "busca", "de", "mudan", "as", "changed_files", "current_files", "escaneia", "todos", "os", "arquivos", "relevantes", "for", "file_path", "in", "self", "watch_path", "rglob", "if", "not", "file_path", "is_file", "continue", "if", "file_path", "suffix", "not", "in", "self", "include_extensions", "continue", "str_path", "str", "file_path", "file_hash", "self", "_get_file_hash", "file_path", "if", "file_hash", "current_files", "str_path", "file_hash", "verifica", "se", "arquivo", "mudou", "if", "str_path", "in", "self", "file_hashes", "if", "self", "file_hashes", "str_path", "file_hash", "changed_files", "append", "file_path", "else", "arquivo", "novo", "changed_files", "append", "file_path", "atualiza", "cache", "de", "hashes", "self", "file_hashes", "current_files", "self", "stats", "files_monitored", "len", "current_files", "return", "changed_files", "def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive"]}
{"chunk_id": "5151408d4258af305dea539f", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 341, "end_line": 410, "content": "    def _poll_loop(self):\n        \"\"\"Loop principal de polling\"\"\"\n        while self.is_running:\n            try:\n                changed_files = self._scan_for_changes()\n                self.stats['polls_completed'] += 1\n                self.stats['last_poll'] = time.time()\n                \n                if changed_files:\n                    self.stats['changes_detected'] += len(changed_files)\n                    print(f\"üîÑ Detectadas mudan√ßas em {len(changed_files)} arquivo(s)\", file=sys.stderr)\n\n                    if self.indexer_callback:\n                        try:\n                            result = self.indexer_callback(changed_files)\n                            indexed_count = result.get('files_indexed', 0) if isinstance(result, dict) else len(changed_files)\n                            print(f\"‚úÖ {indexed_count} arquivo(s) reindexado(s)\", file=sys.stderr)\n                        except Exception as e:\n                            print(f\"‚ùå Erro ao reindexar: {e}\", file=sys.stderr)\n\n            except Exception as e:\n                print(f\"‚ùå Erro no polling: {e}\", file=sys.stderr)\n\n            # Aguarda pr√≥ximo poll\n            time.sleep(self.poll_interval)\n    \n    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "_poll_loop", "self", "loop", "principal", "de", "polling", "while", "self", "is_running", "try", "changed_files", "self", "_scan_for_changes", "self", "stats", "polls_completed", "self", "stats", "last_poll", "time", "time", "if", "changed_files", "self", "stats", "changes_detected", "len", "changed_files", "print", "detectadas", "mudan", "as", "em", "len", "changed_files", "arquivo", "file", "sys", "stderr", "if", "self", "indexer_callback", "try", "result", "self", "indexer_callback", "changed_files", "indexed_count", "result", "get", "files_indexed", "if", "isinstance", "result", "dict", "else", "len", "changed_files", "print", "indexed_count", "arquivo", "reindexado", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "ao", "reindexar", "file", "sys", "stderr", "except", "exception", "as", "print", "erro", "no", "polling", "file", "sys", "stderr", "aguarda", "pr", "ximo", "poll", "time", "sleep", "self", "poll_interval", "def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "eae42d0848cb3793944b84da", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 367, "end_line": 410, "content": "    def start(self) -> bool:\n        \"\"\"Inicia o polling de arquivos\"\"\"\n        if self.is_running:\n            print(\"‚ö†Ô∏è  Simple file watcher j√° est√° rodando\")\n            return True\n            \n        # Scan inicial\n        self._scan_for_changes()\n        \n        self.is_running = True\n        self.poll_thread = threading.Thread(target=self._poll_loop, daemon=True)\n        self.poll_thread.start()\n        \n        print(f\"‚úÖ Simple file watcher iniciado (polling a cada {self.poll_interval}s)\")\n        print(f\"üìä Monitorando {self.stats['files_monitored']} arquivos\")\n        return True\n    \n    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "start", "self", "bool", "inicia", "polling", "de", "arquivos", "if", "self", "is_running", "print", "simple", "file", "watcher", "est", "rodando", "return", "true", "scan", "inicial", "self", "_scan_for_changes", "self", "is_running", "true", "self", "poll_thread", "threading", "thread", "target", "self", "_poll_loop", "daemon", "true", "self", "poll_thread", "start", "print", "simple", "file", "watcher", "iniciado", "polling", "cada", "self", "poll_interval", "print", "monitorando", "self", "stats", "files_monitored", "arquivos", "return", "true", "def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "35e89241e437db4b0eada589", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 384, "end_line": 410, "content": "    def stop(self):\n        \"\"\"Para o polling\"\"\"\n        self.is_running = False\n        if self.poll_thread and self.poll_thread.is_alive():\n            self.poll_thread.join(timeout=5)\n        print(\"‚úÖ Simple file watcher parado\")\n    \n    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "stop", "self", "para", "polling", "self", "is_running", "false", "if", "self", "poll_thread", "and", "self", "poll_thread", "is_alive", "self", "poll_thread", "join", "timeout", "print", "simple", "file", "watcher", "parado", "def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "983627edbbeebb5d615f1ea8", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 391, "end_line": 410, "content": "    def get_stats(self) -> Dict:\n        \"\"\"Retorna estat√≠sticas do simple file watcher\"\"\"\n        return {\n            'enabled': True,\n            'running': self.is_running,\n            'watch_path': str(self.watch_path),\n            'poll_interval': self.poll_interval,\n            'type': 'simple_polling',\n            **self.stats\n        }\n\ndef create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "get_stats", "self", "dict", "retorna", "estat", "sticas", "do", "simple", "file", "watcher", "return", "enabled", "true", "running", "self", "is_running", "watch_path", "str", "self", "watch_path", "poll_interval", "self", "poll_interval", "type", "simple_polling", "self", "stats", "def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "092c1c41268f95f32c87238d", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 402, "end_line": 410, "content": "def create_file_watcher(watch_path: str = str(CURRENT_DIR.parent.parent), **kwargs) -> FileWatcher | SimpleFileWatcher:\n    \"\"\"\n    Factory function que cria o melhor file watcher dispon√≠vel\n    \"\"\"\n    if HAS_WATCHDOG:\n        return FileWatcher(watch_path=watch_path, **kwargs)\n    else:\n        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["def", "create_file_watcher", "watch_path", "str", "str", "current_dir", "parent", "parent", "kwargs", "filewatcher", "simplefilewatcher", "factory", "function", "que", "cria", "melhor", "file", "watcher", "dispon", "vel", "if", "has_watchdog", "return", "filewatcher", "watch_path", "watch_path", "kwargs", "else", "print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "8ca2c4ca3f7448a93a858702", "file_path": "mcp_system/utils/file_watcher.py", "start_line": 409, "end_line": 410, "content": "        print(\"üìù Usando fallback SimpleFileWatcher (instale watchdog para melhor performance)\", file=sys.stderr)\n        return SimpleFileWatcher(watch_path=watch_path, **kwargs)", "mtime": 1756155021.429876, "terms": ["print", "usando", "fallback", "simplefilewatcher", "instale", "watchdog", "para", "melhor", "performance", "file", "sys", "stderr", "return", "simplefilewatcher", "watch_path", "watch_path", "kwargs"]}
{"chunk_id": "9bf02a2b46f6885877b7b405", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 1, "end_line": 80, "content": "\n#!/usr/bin/env python3\n\"\"\"\nResumo de m√©tricas do MCP a partir de arquivos CSV na pasta mcp_system/.mcp_index\n\nAgora com suporte a m√∫ltiplas fontes por padr√£o:\n- metrics_context.csv (consultas/context_pack)\n- metrics_index.csv (indexa√ß√µes)\n- metrics.csv (legado)\n\nCampos esperados nos CSVs:\n  Contexto: ts, query, chunk_count, total_tokens, budget_tokens, budget_utilization, latency_ms\n  Indexa√ß√£o: ts, op, path, index_dir, files_indexed, chunks, recursive, include_globs, exclude_globs, elapsed_s\n\nUso b√°sico:\n  python summarize_metrics.py                # l√™ todas as fontes acima automaticamente\n  python summarize_metrics.py --file mcp_system/.mcp_index/metrics_context.csv\n  python summarize_metrics.py --since 7\n  python summarize_metrics.py --filter \"minha funcao\"\n  python summarize_metrics.py --json\n  python summarize_metrics.py --tz local|utc  # agrupamento por dia no fuso desejado (default: local)\n\"\"\"\n\nimport os, sys, csv, argparse, datetime as dt, statistics as st, json\nfrom math import floor\nfrom typing import List, Dict, Any, Optional\nimport pathlib\n\n# Diret√≥rios de refer√™ncia\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()  # mcp_system/scripts\nROOT_DIR = CURRENT_DIR.parent  # mcp_system\nINDEX_DIR = ROOT_DIR / \".mcp_index\"\nDEFAULT_CONTEXT_PATH = INDEX_DIR / \"metrics_context.csv\"\nDEFAULT_INDEX_PATH = INDEX_DIR / \"metrics_index.csv\"\nLEGACY_METRICS_PATH = INDEX_DIR / \"metrics.csv\"\n\n\ndef p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef parse_dt(s: str) -> Optional[dt.datetime]:\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            d = dt.datetime.strptime(s, fmt)\n            # Se n√£o tem timezone (%z), assume UTC para manter consist√™ncia\n            if \"%z\" not in fmt:\n                d = d.replace(tzinfo=dt.timezone.utc)\n            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local", "mtime": 1756161320.4610665, "terms": ["usr", "bin", "env", "python3", "resumo", "de", "tricas", "do", "mcp", "partir", "de", "arquivos", "csv", "na", "pasta", "mcp_system", "mcp_index", "agora", "com", "suporte", "ltiplas", "fontes", "por", "padr", "metrics_context", "csv", "consultas", "context_pack", "metrics_index", "csv", "indexa", "es", "metrics", "csv", "legado", "campos", "esperados", "nos", "csvs", "contexto", "ts", "query", "chunk_count", "total_tokens", "budget_tokens", "budget_utilization", "latency_ms", "indexa", "ts", "op", "path", "index_dir", "files_indexed", "chunks", "recursive", "include_globs", "exclude_globs", "elapsed_s", "uso", "sico", "python", "summarize_metrics", "py", "todas", "as", "fontes", "acima", "automaticamente", "python", "summarize_metrics", "py", "file", "mcp_system", "mcp_index", "metrics_context", "csv", "python", "summarize_metrics", "py", "since", "python", "summarize_metrics", "py", "filter", "minha", "funcao", "python", "summarize_metrics", "py", "json", "python", "summarize_metrics", "py", "tz", "local", "utc", "agrupamento", "por", "dia", "no", "fuso", "desejado", "default", "local", "import", "os", "sys", "csv", "argparse", "datetime", "as", "dt", "statistics", "as", "st", "json", "from", "math", "import", "floor", "from", "typing", "import", "list", "dict", "any", "optional", "import", "pathlib", "diret", "rios", "de", "refer", "ncia", "current_dir", "pathlib", "path", "__file__", "parent", "absolute", "mcp_system", "scripts", "root_dir", "current_dir", "parent", "mcp_system", "index_dir", "root_dir", "mcp_index", "default_context_path", "index_dir", "metrics_context", "csv", "default_index_path", "index_dir", "metrics_index", "csv", "legacy_metrics_path", "index_dir", "metrics", "csv", "def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "str", "optional", "dt", "datetime", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "dt", "datetime", "strptime", "fmt", "se", "tem", "timezone", "assume", "utc", "para", "manter", "consist", "ncia", "if", "not", "in", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local"]}
{"chunk_id": "4a897767c6db3681704a9572", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 38, "end_line": 117, "content": "def p95(vals: List[float]) -> float:\n    if not vals:\n        return 0.0\n    s = sorted(vals)\n    return s[floor(0.95 * (len(s) - 1))]\n\n\ndef coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef parse_dt(s: str) -> Optional[dt.datetime]:\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            d = dt.datetime.strptime(s, fmt)\n            # Se n√£o tem timezone (%z), assume UTC para manter consist√™ncia\n            if \"%z\" not in fmt:\n                d = d.replace(tzinfo=dt.timezone.utc)\n            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),", "mtime": 1756161320.4610665, "terms": ["def", "p95", "vals", "list", "float", "float", "if", "not", "vals", "return", "sorted", "vals", "return", "floor", "len", "def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "str", "optional", "dt", "datetime", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "dt", "datetime", "strptime", "fmt", "se", "tem", "timezone", "assume", "utc", "para", "manter", "consist", "ncia", "if", "not", "in", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s"]}
{"chunk_id": "47a178aafd03c4a6a78d3093", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 45, "end_line": 124, "content": "def coerce_int(s, default=0):\n    try:\n        return int(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef parse_dt(s: str) -> Optional[dt.datetime]:\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            d = dt.datetime.strptime(s, fmt)\n            # Se n√£o tem timezone (%z), assume UTC para manter consist√™ncia\n            if \"%z\" not in fmt:\n                d = d.replace(tzinfo=dt.timezone.utc)\n            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:", "mtime": 1756161320.4610665, "terms": ["def", "coerce_int", "default", "try", "return", "int", "except", "valueerror", "typeerror", "return", "default", "def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "str", "optional", "dt", "datetime", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "dt", "datetime", "strptime", "fmt", "se", "tem", "timezone", "assume", "utc", "para", "manter", "consist", "ncia", "if", "not", "in", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any"]}
{"chunk_id": "9c8e3bfd3864024280c1abd2", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 52, "end_line": 131, "content": "def coerce_float(s, default=0.0):\n    try:\n        return float(s)\n    except (ValueError, TypeError):\n        return default\n\n\ndef parse_dt(s: str) -> Optional[dt.datetime]:\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            d = dt.datetime.strptime(s, fmt)\n            # Se n√£o tem timezone (%z), assume UTC para manter consist√™ncia\n            if \"%z\" not in fmt:\n                d = d.replace(tzinfo=dt.timezone.utc)\n            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)", "mtime": 1756161320.4610665, "terms": ["def", "coerce_float", "default", "try", "return", "float", "except", "valueerror", "typeerror", "return", "default", "def", "parse_dt", "str", "optional", "dt", "datetime", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "dt", "datetime", "strptime", "fmt", "se", "tem", "timezone", "assume", "utc", "para", "manter", "consist", "ncia", "if", "not", "in", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv"]}
{"chunk_id": "372731bb70a0c3f6cb79ae0c", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 59, "end_line": 138, "content": "def parse_dt(s: str) -> Optional[dt.datetime]:\n    if not s:\n        return None\n    # Tentar diferentes formatos de data\n    for fmt in [\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%d %H:%M:%S\"]:\n        try:\n            d = dt.datetime.strptime(s, fmt)\n            # Se n√£o tem timezone (%z), assume UTC para manter consist√™ncia\n            if \"%z\" not in fmt:\n                d = d.replace(tzinfo=dt.timezone.utc)\n            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto", "mtime": 1756161320.4610665, "terms": ["def", "parse_dt", "str", "optional", "dt", "datetime", "if", "not", "return", "none", "tentar", "diferentes", "formatos", "de", "data", "for", "fmt", "in", "dt", "dt", "try", "dt", "datetime", "strptime", "fmt", "se", "tem", "timezone", "assume", "utc", "para", "manter", "consist", "ncia", "if", "not", "in", "fmt", "replace", "tzinfo", "dt", "timezone", "utc", "return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto"]}
{"chunk_id": "065ee819a3d648584348e09d", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 69, "end_line": 148, "content": "            return d\n        except ValueError:\n            continue\n    return None\n\n\ndef _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")", "mtime": 1756161320.4610665, "terms": ["return", "except", "valueerror", "continue", "return", "none", "def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print"]}
{"chunk_id": "5a5c37901d724ef66c2d8592", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 75, "end_line": 154, "content": "def _as_tz(d: dt.datetime, tz_mode: str) -> dt.datetime:\n    if not d:\n        return d\n    if tz_mode == \"utc\":\n        return d.astimezone(dt.timezone.utc)\n    # Local\n    try:\n        local_tz = dt.datetime.now().astimezone().tzinfo\n        return d.astimezone(local_tz)\n    except Exception:\n        return d  # fallback\n\n\ndef _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")\n        print(\"\\nDica: gere m√©tricas executando consultas (context_pack) ou reindexa√ß√µes; ou informe o caminho via --file.\")\n        sys.exit(1)\n\n    all_rows: List[Dict[str, Any]] = []\n    for p in found:\n        try:", "mtime": 1756161320.4610665, "terms": ["def", "_as_tz", "dt", "datetime", "tz_mode", "str", "dt", "datetime", "if", "not", "return", "if", "tz_mode", "utc", "return", "astimezone", "dt", "timezone", "utc", "local", "try", "local_tz", "dt", "datetime", "now", "astimezone", "tzinfo", "return", "astimezone", "local_tz", "except", "exception", "return", "fallback", "def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print", "print", "ndica", "gere", "tricas", "executando", "consultas", "context_pack", "ou", "reindexa", "es", "ou", "informe", "caminho", "via", "file", "sys", "exit", "all_rows", "list", "dict", "str", "any", "for", "in", "found", "try"]}
{"chunk_id": "b37b60c7306b54f5eead0c6c", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 88, "end_line": 167, "content": "def _resolve_path(path_arg: str) -> pathlib.Path:\n    expanded = os.path.expandvars(os.path.expanduser(path_arg))\n    p = pathlib.Path(expanded)\n    if not p.is_absolute():\n        # Tentar relativo ao ROOT_DIR e ao CWD\n        candidates = [ROOT_DIR / p, pathlib.Path.cwd() / p]\n        for c in candidates:\n            if c.exists():\n                return c\n        return candidates[0]\n    return p\n\n\ndef _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")\n        print(\"\\nDica: gere m√©tricas executando consultas (context_pack) ou reindexa√ß√µes; ou informe o caminho via --file.\")\n        sys.exit(1)\n\n    all_rows: List[Dict[str, Any]] = []\n    for p in found:\n        try:\n            all_rows.extend(_load_context_csv(p))\n        except Exception as e:\n            print(f\"[aviso] Falha ao ler {p}: {e}\")\n\n    return all_rows\n\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r.get(\"ts\")) and parse_dt(r.get(\"ts\")) >= cutoff]\n\n    if query_filter:", "mtime": 1756161320.4610665, "terms": ["def", "_resolve_path", "path_arg", "str", "pathlib", "path", "expanded", "os", "path", "expandvars", "os", "path", "expanduser", "path_arg", "pathlib", "path", "expanded", "if", "not", "is_absolute", "tentar", "relativo", "ao", "root_dir", "ao", "cwd", "candidates", "root_dir", "pathlib", "path", "cwd", "for", "in", "candidates", "if", "exists", "return", "return", "candidates", "return", "def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print", "print", "ndica", "gere", "tricas", "executando", "consultas", "context_pack", "ou", "reindexa", "es", "ou", "informe", "caminho", "via", "file", "sys", "exit", "all_rows", "list", "dict", "str", "any", "for", "in", "found", "try", "all_rows", "extend", "_load_context_csv", "except", "exception", "as", "print", "aviso", "falha", "ao", "ler", "return", "all_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "get", "ts", "and", "parse_dt", "get", "ts", "cutoff", "if", "query_filter"]}
{"chunk_id": "fbc5cb1486c9714c40fa6b00", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 101, "end_line": 180, "content": "def _load_context_csv(path: pathlib.Path) -> List[Dict[str, Any]]:\n    rows: List[Dict[str, Any]] = []\n    with open(path, encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        # Valida presen√ßa de coluna \"query\" para confirmar tipo\n        has_query = \"query\" in (reader.fieldnames or [])\n        for row in reader:\n            if not has_query:\n                # Converter m√©trica de indexa√ß√£o para formato de contexto\n                rows.append({\n                    \"ts\": row.get(\"ts\", \"\"),\n                    \"query\": f\"index_{row.get('op', 'unknown')}\",\n                    \"chunk_count\": str(coerce_int(row.get(\"chunks\", 0))),\n                    \"total_tokens\": str(coerce_int(row.get(\"chunks\", 0)) * 50),  # Estimativa\n                    \"budget_tokens\": \"8000\",\n                    \"budget_utilization\": str(coerce_float(row.get(\"chunks\", 0)) * 50 / 8000 * 100),\n                    \"latency_ms\": str(coerce_float(row.get(\"elapsed_s\", 0)) * 1000),\n                })\n            else:\n                rows.append(row)\n    return rows\n\n\ndef load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")\n        print(\"\\nDica: gere m√©tricas executando consultas (context_pack) ou reindexa√ß√µes; ou informe o caminho via --file.\")\n        sys.exit(1)\n\n    all_rows: List[Dict[str, Any]] = []\n    for p in found:\n        try:\n            all_rows.extend(_load_context_csv(p))\n        except Exception as e:\n            print(f\"[aviso] Falha ao ler {p}: {e}\")\n\n    return all_rows\n\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r.get(\"ts\")) and parse_dt(r.get(\"ts\")) >= cutoff]\n\n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in str(r.get(\"query\", \"\")).lower()]\n\n    return rows\n\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return", "mtime": 1756161320.4610665, "terms": ["def", "_load_context_csv", "path", "pathlib", "path", "list", "dict", "str", "any", "rows", "list", "dict", "str", "any", "with", "open", "path", "encoding", "utf", "as", "reader", "csv", "dictreader", "valida", "presen", "de", "coluna", "query", "para", "confirmar", "tipo", "has_query", "query", "in", "reader", "fieldnames", "or", "for", "row", "in", "reader", "if", "not", "has_query", "converter", "trica", "de", "indexa", "para", "formato", "de", "contexto", "rows", "append", "ts", "row", "get", "ts", "query", "index_", "row", "get", "op", "unknown", "chunk_count", "str", "coerce_int", "row", "get", "chunks", "total_tokens", "str", "coerce_int", "row", "get", "chunks", "estimativa", "budget_tokens", "budget_utilization", "str", "coerce_float", "row", "get", "chunks", "latency_ms", "str", "coerce_float", "row", "get", "elapsed_s", "else", "rows", "append", "row", "return", "rows", "def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print", "print", "ndica", "gere", "tricas", "executando", "consultas", "context_pack", "ou", "reindexa", "es", "ou", "informe", "caminho", "via", "file", "sys", "exit", "all_rows", "list", "dict", "str", "any", "for", "in", "found", "try", "all_rows", "extend", "_load_context_csv", "except", "exception", "as", "print", "aviso", "falha", "ao", "ler", "return", "all_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "get", "ts", "and", "parse_dt", "get", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "str", "get", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return"]}
{"chunk_id": "13ba1bde2249856bc9350927", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 124, "end_line": 203, "content": "def load_rows(args) -> List[Dict[str, Any]]:\n    # Se --file for informado, l√™ apenas aquele arquivo\n    if args.file:\n        p = _resolve_path(args.file)\n        if not p.exists():\n            print(f\"[erro] CSV n√£o encontrado: {p}\")\n            sys.exit(1)\n        return _load_context_csv(p)\n\n    # Sem --file: tentar m√∫ltiplas fontes (contexto, index e legado) tanto em mcp_system quanto na raiz\n    candidates = [\n        DEFAULT_CONTEXT_PATH,\n        DEFAULT_INDEX_PATH,\n        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")\n        print(\"\\nDica: gere m√©tricas executando consultas (context_pack) ou reindexa√ß√µes; ou informe o caminho via --file.\")\n        sys.exit(1)\n\n    all_rows: List[Dict[str, Any]] = []\n    for p in found:\n        try:\n            all_rows.extend(_load_context_csv(p))\n        except Exception as e:\n            print(f\"[aviso] Falha ao ler {p}: {e}\")\n\n    return all_rows\n\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r.get(\"ts\")) and parse_dt(r.get(\"ts\")) >= cutoff]\n\n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in str(r.get(\"query\", \"\")).lower()]\n\n    return rows\n\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Converter timestamps e ordenar\n    tz_mode = args.tz\n    parsed_rows = []\n    for r in rows:\n        d = parse_dt(r.get(\"ts\"))\n        if not d:\n            continue\n        parsed_rows.append((r, d))\n    parsed_rows.sort(key=lambda x: x[1])\n\n    # Resumo geral\n    total_rows = len(parsed_rows)\n    first_ts = parsed_rows[0][1] if parsed_rows else None\n    last_ts = parsed_rows[-1][1] if parsed_rows else None\n\n    # Converter valores para n√∫meros\n    chunk_counts = [coerce_int(r[0][\"chunk_count\"]) for r in parsed_rows]\n    total_tokens_list = [coerce_int(r[0][\"total_tokens\"]) for r in parsed_rows]\n    budget_utilizations = [coerce_float(r[0][\"budget_utilization\"]) for r in parsed_rows]\n    latencies = [coerce_float(r[0][\"latency_ms\"]) for r in parsed_rows]\n\n    # Calcular estat√≠sticas", "mtime": 1756161320.4610665, "terms": ["def", "load_rows", "args", "list", "dict", "str", "any", "se", "file", "for", "informado", "apenas", "aquele", "arquivo", "if", "args", "file", "_resolve_path", "args", "file", "if", "not", "exists", "print", "erro", "csv", "encontrado", "sys", "exit", "return", "_load_context_csv", "sem", "file", "tentar", "ltiplas", "fontes", "contexto", "index", "legado", "tanto", "em", "mcp_system", "quanto", "na", "raiz", "candidates", "default_context_path", "default_index_path", "legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print", "print", "ndica", "gere", "tricas", "executando", "consultas", "context_pack", "ou", "reindexa", "es", "ou", "informe", "caminho", "via", "file", "sys", "exit", "all_rows", "list", "dict", "str", "any", "for", "in", "found", "try", "all_rows", "extend", "_load_context_csv", "except", "exception", "as", "print", "aviso", "falha", "ao", "ler", "return", "all_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "get", "ts", "and", "parse_dt", "get", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "str", "get", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "converter", "timestamps", "ordenar", "tz_mode", "args", "tz", "parsed_rows", "for", "in", "rows", "parse_dt", "get", "ts", "if", "not", "continue", "parsed_rows", "append", "parsed_rows", "sort", "key", "lambda", "resumo", "geral", "total_rows", "len", "parsed_rows", "first_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "last_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "parsed_rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "parsed_rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "parsed_rows", "latencies", "coerce_float", "latency_ms", "for", "in", "parsed_rows", "calcular", "estat", "sticas"]}
{"chunk_id": "d0c8eed164ec436c2456c52e", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 137, "end_line": 216, "content": "        LEGACY_METRICS_PATH,\n        # Fallbacks na raiz do projeto\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_context.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics_index.csv\",\n        ROOT_DIR.parent / \".mcp_index\" / \"metrics.csv\",\n    ]\n\n    found = [p for p in candidates if p.exists()]\n    if not found:\n        print(\"[erro] Nenhum CSV de m√©tricas encontrado. Locais esperados:\")\n        for c in candidates:\n            print(f\"  - {c}\")\n        print(\"\\nDica: gere m√©tricas executando consultas (context_pack) ou reindexa√ß√µes; ou informe o caminho via --file.\")\n        sys.exit(1)\n\n    all_rows: List[Dict[str, Any]] = []\n    for p in found:\n        try:\n            all_rows.extend(_load_context_csv(p))\n        except Exception as e:\n            print(f\"[aviso] Falha ao ler {p}: {e}\")\n\n    return all_rows\n\n\ndef filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r.get(\"ts\")) and parse_dt(r.get(\"ts\")) >= cutoff]\n\n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in str(r.get(\"query\", \"\")).lower()]\n\n    return rows\n\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Converter timestamps e ordenar\n    tz_mode = args.tz\n    parsed_rows = []\n    for r in rows:\n        d = parse_dt(r.get(\"ts\"))\n        if not d:\n            continue\n        parsed_rows.append((r, d))\n    parsed_rows.sort(key=lambda x: x[1])\n\n    # Resumo geral\n    total_rows = len(parsed_rows)\n    first_ts = parsed_rows[0][1] if parsed_rows else None\n    last_ts = parsed_rows[-1][1] if parsed_rows else None\n\n    # Converter valores para n√∫meros\n    chunk_counts = [coerce_int(r[0][\"chunk_count\"]) for r in parsed_rows]\n    total_tokens_list = [coerce_int(r[0][\"total_tokens\"]) for r in parsed_rows]\n    budget_utilizations = [coerce_float(r[0][\"budget_utilization\"]) for r in parsed_rows]\n    latencies = [coerce_float(r[0][\"latency_ms\"]) for r in parsed_rows]\n\n    # Calcular estat√≠sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n\n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n\n    # Agrupar por dia (no fuso escolhido)\n    daily_stats: Dict[str, Dict[str, List[float]]] = {}\n    for r, d in parsed_rows:", "mtime": 1756161320.4610665, "terms": ["legacy_metrics_path", "fallbacks", "na", "raiz", "do", "projeto", "root_dir", "parent", "mcp_index", "metrics_context", "csv", "root_dir", "parent", "mcp_index", "metrics_index", "csv", "root_dir", "parent", "mcp_index", "metrics", "csv", "found", "for", "in", "candidates", "if", "exists", "if", "not", "found", "print", "erro", "nenhum", "csv", "de", "tricas", "encontrado", "locais", "esperados", "for", "in", "candidates", "print", "print", "ndica", "gere", "tricas", "executando", "consultas", "context_pack", "ou", "reindexa", "es", "ou", "informe", "caminho", "via", "file", "sys", "exit", "all_rows", "list", "dict", "str", "any", "for", "in", "found", "try", "all_rows", "extend", "_load_context_csv", "except", "exception", "as", "print", "aviso", "falha", "ao", "ler", "return", "all_rows", "def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "get", "ts", "and", "parse_dt", "get", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "str", "get", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "converter", "timestamps", "ordenar", "tz_mode", "args", "tz", "parsed_rows", "for", "in", "rows", "parse_dt", "get", "ts", "if", "not", "continue", "parsed_rows", "append", "parsed_rows", "sort", "key", "lambda", "resumo", "geral", "total_rows", "len", "parsed_rows", "first_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "last_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "parsed_rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "parsed_rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "parsed_rows", "latencies", "coerce_float", "latency_ms", "for", "in", "parsed_rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "no", "fuso", "escolhido", "daily_stats", "dict", "str", "dict", "str", "list", "float", "for", "in", "parsed_rows"]}
{"chunk_id": "86c2ff844d0a9293b322d815", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 162, "end_line": 241, "content": "def filter_rows(rows: List[Dict], since_days: int = 0, query_filter: str = \"\") -> List[Dict]:\n    if since_days > 0:\n        cutoff = dt.datetime.now(dt.timezone.utc) - dt.timedelta(days=since_days)\n        rows = [r for r in rows if parse_dt(r.get(\"ts\")) and parse_dt(r.get(\"ts\")) >= cutoff]\n\n    if query_filter:\n        rows = [r for r in rows if query_filter.lower() in str(r.get(\"query\", \"\")).lower()]\n\n    return rows\n\n\ndef format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Converter timestamps e ordenar\n    tz_mode = args.tz\n    parsed_rows = []\n    for r in rows:\n        d = parse_dt(r.get(\"ts\"))\n        if not d:\n            continue\n        parsed_rows.append((r, d))\n    parsed_rows.sort(key=lambda x: x[1])\n\n    # Resumo geral\n    total_rows = len(parsed_rows)\n    first_ts = parsed_rows[0][1] if parsed_rows else None\n    last_ts = parsed_rows[-1][1] if parsed_rows else None\n\n    # Converter valores para n√∫meros\n    chunk_counts = [coerce_int(r[0][\"chunk_count\"]) for r in parsed_rows]\n    total_tokens_list = [coerce_int(r[0][\"total_tokens\"]) for r in parsed_rows]\n    budget_utilizations = [coerce_float(r[0][\"budget_utilization\"]) for r in parsed_rows]\n    latencies = [coerce_float(r[0][\"latency_ms\"]) for r in parsed_rows]\n\n    # Calcular estat√≠sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n\n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n\n    # Agrupar por dia (no fuso escolhido)\n    daily_stats: Dict[str, Dict[str, List[float]]] = {}\n    for r, d in parsed_rows:\n        ts_local = _as_tz(d, tz_mode)\n        day = format_dt(ts_local)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": [],\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(r[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(r[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(r[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(r[\"latency_ms\"]))\n\n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),", "mtime": 1756161320.4610665, "terms": ["def", "filter_rows", "rows", "list", "dict", "since_days", "int", "query_filter", "str", "list", "dict", "if", "since_days", "cutoff", "dt", "datetime", "now", "dt", "timezone", "utc", "dt", "timedelta", "days", "since_days", "rows", "for", "in", "rows", "if", "parse_dt", "get", "ts", "and", "parse_dt", "get", "ts", "cutoff", "if", "query_filter", "rows", "for", "in", "rows", "if", "query_filter", "lower", "in", "str", "get", "query", "lower", "return", "rows", "def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "converter", "timestamps", "ordenar", "tz_mode", "args", "tz", "parsed_rows", "for", "in", "rows", "parse_dt", "get", "ts", "if", "not", "continue", "parsed_rows", "append", "parsed_rows", "sort", "key", "lambda", "resumo", "geral", "total_rows", "len", "parsed_rows", "first_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "last_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "parsed_rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "parsed_rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "parsed_rows", "latencies", "coerce_float", "latency_ms", "for", "in", "parsed_rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "no", "fuso", "escolhido", "daily_stats", "dict", "str", "dict", "str", "list", "float", "for", "in", "parsed_rows", "ts_local", "_as_tz", "tz_mode", "day", "format_dt", "ts_local", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens"]}
{"chunk_id": "6fae75ad1153fbd7c8b0f268", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 173, "end_line": 252, "content": "def format_dt(d: dt.datetime) -> str:\n    return d.strftime(\"%Y-%m-%d\")\n\n\ndef summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Converter timestamps e ordenar\n    tz_mode = args.tz\n    parsed_rows = []\n    for r in rows:\n        d = parse_dt(r.get(\"ts\"))\n        if not d:\n            continue\n        parsed_rows.append((r, d))\n    parsed_rows.sort(key=lambda x: x[1])\n\n    # Resumo geral\n    total_rows = len(parsed_rows)\n    first_ts = parsed_rows[0][1] if parsed_rows else None\n    last_ts = parsed_rows[-1][1] if parsed_rows else None\n\n    # Converter valores para n√∫meros\n    chunk_counts = [coerce_int(r[0][\"chunk_count\"]) for r in parsed_rows]\n    total_tokens_list = [coerce_int(r[0][\"total_tokens\"]) for r in parsed_rows]\n    budget_utilizations = [coerce_float(r[0][\"budget_utilization\"]) for r in parsed_rows]\n    latencies = [coerce_float(r[0][\"latency_ms\"]) for r in parsed_rows]\n\n    # Calcular estat√≠sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n\n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n\n    # Agrupar por dia (no fuso escolhido)\n    daily_stats: Dict[str, Dict[str, List[float]]] = {}\n    for r, d in parsed_rows:\n        ts_local = _as_tz(d, tz_mode)\n        day = format_dt(ts_local)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": [],\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(r[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(r[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(r[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(r[\"latency_ms\"]))\n\n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n\n    # Sa√≠da JSON se solicitado\n    if args.json:\n        output = {", "mtime": 1756161320.4610665, "terms": ["def", "format_dt", "dt", "datetime", "str", "return", "strftime", "def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "converter", "timestamps", "ordenar", "tz_mode", "args", "tz", "parsed_rows", "for", "in", "rows", "parse_dt", "get", "ts", "if", "not", "continue", "parsed_rows", "append", "parsed_rows", "sort", "key", "lambda", "resumo", "geral", "total_rows", "len", "parsed_rows", "first_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "last_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "parsed_rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "parsed_rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "parsed_rows", "latencies", "coerce_float", "latency_ms", "for", "in", "parsed_rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "no", "fuso", "escolhido", "daily_stats", "dict", "str", "dict", "str", "list", "float", "for", "in", "parsed_rows", "ts_local", "_as_tz", "tz_mode", "day", "format_dt", "ts_local", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output"]}
{"chunk_id": "245f174126912e973a8a5fae", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 177, "end_line": 256, "content": "def summarize(rows: List[Dict], args):\n    if not rows:\n        print(\"Nenhum dado encontrado.\")\n        return\n\n    # Converter timestamps e ordenar\n    tz_mode = args.tz\n    parsed_rows = []\n    for r in rows:\n        d = parse_dt(r.get(\"ts\"))\n        if not d:\n            continue\n        parsed_rows.append((r, d))\n    parsed_rows.sort(key=lambda x: x[1])\n\n    # Resumo geral\n    total_rows = len(parsed_rows)\n    first_ts = parsed_rows[0][1] if parsed_rows else None\n    last_ts = parsed_rows[-1][1] if parsed_rows else None\n\n    # Converter valores para n√∫meros\n    chunk_counts = [coerce_int(r[0][\"chunk_count\"]) for r in parsed_rows]\n    total_tokens_list = [coerce_int(r[0][\"total_tokens\"]) for r in parsed_rows]\n    budget_utilizations = [coerce_float(r[0][\"budget_utilization\"]) for r in parsed_rows]\n    latencies = [coerce_float(r[0][\"latency_ms\"]) for r in parsed_rows]\n\n    # Calcular estat√≠sticas\n    avg_chunks = st.mean(chunk_counts) if chunk_counts else 0\n    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n\n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n\n    # Agrupar por dia (no fuso escolhido)\n    daily_stats: Dict[str, Dict[str, List[float]]] = {}\n    for r, d in parsed_rows:\n        ts_local = _as_tz(d, tz_mode)\n        day = format_dt(ts_local)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": [],\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(r[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(r[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(r[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(r[\"latency_ms\"]))\n\n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n\n    # Sa√≠da JSON se solicitado\n    if args.json:\n        output = {\n            \"summary\": {\n                \"period_start\": format_dt(_as_tz(first_ts, tz_mode)) if first_ts else \"\",\n                \"period_end\": format_dt(_as_tz(last_ts, tz_mode)) if last_ts else \"\",\n                \"total_queries\": total_rows,", "mtime": 1756161320.4610665, "terms": ["def", "summarize", "rows", "list", "dict", "args", "if", "not", "rows", "print", "nenhum", "dado", "encontrado", "return", "converter", "timestamps", "ordenar", "tz_mode", "args", "tz", "parsed_rows", "for", "in", "rows", "parse_dt", "get", "ts", "if", "not", "continue", "parsed_rows", "append", "parsed_rows", "sort", "key", "lambda", "resumo", "geral", "total_rows", "len", "parsed_rows", "first_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "last_ts", "parsed_rows", "if", "parsed_rows", "else", "none", "converter", "valores", "para", "meros", "chunk_counts", "coerce_int", "chunk_count", "for", "in", "parsed_rows", "total_tokens_list", "coerce_int", "total_tokens", "for", "in", "parsed_rows", "budget_utilizations", "coerce_float", "budget_utilization", "for", "in", "parsed_rows", "latencies", "coerce_float", "latency_ms", "for", "in", "parsed_rows", "calcular", "estat", "sticas", "avg_chunks", "st", "mean", "chunk_counts", "if", "chunk_counts", "else", "avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "no", "fuso", "escolhido", "daily_stats", "dict", "str", "dict", "str", "list", "float", "for", "in", "parsed_rows", "ts_local", "_as_tz", "tz_mode", "day", "format_dt", "ts_local", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output", "summary", "period_start", "format_dt", "_as_tz", "first_ts", "tz_mode", "if", "first_ts", "else", "period_end", "format_dt", "_as_tz", "last_ts", "tz_mode", "if", "last_ts", "else", "total_queries", "total_rows"]}
{"chunk_id": "9c3a48272142174119401c8f", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 205, "end_line": 284, "content": "    avg_tokens = st.mean(total_tokens_list) if total_tokens_list else 0\n    avg_util = st.mean(budget_utilizations) if budget_utilizations else 0\n    avg_latency = st.mean(latencies) if latencies else 0\n\n    p95_chunks = p95(chunk_counts)\n    p95_tokens = p95(total_tokens_list)\n    p95_util = p95(budget_utilizations)\n    p95_latency = p95(latencies)\n\n    # Agrupar por dia (no fuso escolhido)\n    daily_stats: Dict[str, Dict[str, List[float]]] = {}\n    for r, d in parsed_rows:\n        ts_local = _as_tz(d, tz_mode)\n        day = format_dt(ts_local)\n        if day not in daily_stats:\n            daily_stats[day] = {\n                \"chunk_counts\": [],\n                \"total_tokens\": [],\n                \"budget_utilizations\": [],\n                \"latencies\": [],\n            }\n        daily_stats[day][\"chunk_counts\"].append(coerce_int(r[\"chunk_count\"]))\n        daily_stats[day][\"total_tokens\"].append(coerce_int(r[\"total_tokens\"]))\n        daily_stats[day][\"budget_utilizations\"].append(coerce_float(r[\"budget_utilization\"]))\n        daily_stats[day][\"latencies\"].append(coerce_float(r[\"latency_ms\"]))\n\n    # Converter para formato de tabela\n    daily_rows = []\n    for day, stats in sorted(daily_stats.items()):\n        daily_rows.append({\n            \"day\": day,\n            \"avg_chunks\": st.mean(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"median_chunks\": st.median(stats[\"chunk_counts\"]) if stats[\"chunk_counts\"] else 0,\n            \"p95_chunks\": p95(stats[\"chunk_counts\"]),\n            \"avg_tokens\": st.mean(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"median_tokens\": st.median(stats[\"total_tokens\"]) if stats[\"total_tokens\"] else 0,\n            \"p95_tokens\": p95(stats[\"total_tokens\"]),\n            \"avg_util\": st.mean(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"median_util\": st.median(stats[\"budget_utilizations\"]) if stats[\"budget_utilizations\"] else 0,\n            \"p95_util\": p95(stats[\"budget_utilizations\"]),\n            \"avg_latency\": st.mean(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"median_latency\": st.median(stats[\"latencies\"]) if stats[\"latencies\"] else 0,\n            \"p95_latency\": p95(stats[\"latencies\"]),\n        })\n\n    # Sa√≠da JSON se solicitado\n    if args.json:\n        output = {\n            \"summary\": {\n                \"period_start\": format_dt(_as_tz(first_ts, tz_mode)) if first_ts else \"\",\n                \"period_end\": format_dt(_as_tz(last_ts, tz_mode)) if last_ts else \"\",\n                \"total_queries\": total_rows,\n                \"avg_chunks\": avg_chunks,\n                \"avg_tokens\": avg_tokens,\n                \"avg_budget_utilization_pct\": avg_util,\n                \"avg_latency_ms\": avg_latency,\n                \"p95_chunks\": p95_chunks,\n                \"p95_tokens\": p95_tokens,\n                \"p95_budget_utilization_pct\": p95_util,\n                \"p95_latency_ms\": p95_latency,\n            },\n            \"daily\": daily_rows,\n        }\n        print(json.dumps(output, indent=2))\n        return\n\n    # Sa√≠da em texto formatado\n    print(\"=== RESUMO DE M√âTRICAS DO MCP ===\")\n    print(\n        f\"Per√≠odo: {format_dt(_as_tz(first_ts, tz_mode)) if first_ts else 'N/A'} a {format_dt(_as_tz(last_ts, tz_mode)) if last_ts else 'N/A'} ({total_rows} entradas)\"\n    )\n    print()\n    print(\"M√©dia Geral:\")\n    print(f\"  Chunks:     {avg_chunks:.1f} (p95: {p95_chunks:.1f})\")\n    print(f\"  Tokens:     {avg_tokens:.0f} (p95: {p95_tokens:.0f})\")\n    print(f\"  Utiliza√ß√£o: {avg_util:.1f}% (p95: {p95_util:.1f}%)\")\n    print(f\"  Lat√™ncia:   {avg_latency:.1f}ms (p95: {p95_latency:.1f}ms)\")\n    print()\n    print(\"Di√°rio:\")\n    print(f\"{'Dia':<12} {'Chunks':<24} {'Tokens':<24} {'Utiliza√ß√£o':<24} {'Lat√™ncia (ms)':<24}\")", "mtime": 1756161320.4610665, "terms": ["avg_tokens", "st", "mean", "total_tokens_list", "if", "total_tokens_list", "else", "avg_util", "st", "mean", "budget_utilizations", "if", "budget_utilizations", "else", "avg_latency", "st", "mean", "latencies", "if", "latencies", "else", "p95_chunks", "p95", "chunk_counts", "p95_tokens", "p95", "total_tokens_list", "p95_util", "p95", "budget_utilizations", "p95_latency", "p95", "latencies", "agrupar", "por", "dia", "no", "fuso", "escolhido", "daily_stats", "dict", "str", "dict", "str", "list", "float", "for", "in", "parsed_rows", "ts_local", "_as_tz", "tz_mode", "day", "format_dt", "ts_local", "if", "day", "not", "in", "daily_stats", "daily_stats", "day", "chunk_counts", "total_tokens", "budget_utilizations", "latencies", "daily_stats", "day", "chunk_counts", "append", "coerce_int", "chunk_count", "daily_stats", "day", "total_tokens", "append", "coerce_int", "total_tokens", "daily_stats", "day", "budget_utilizations", "append", "coerce_float", "budget_utilization", "daily_stats", "day", "latencies", "append", "coerce_float", "latency_ms", "converter", "para", "formato", "de", "tabela", "daily_rows", "for", "day", "stats", "in", "sorted", "daily_stats", "items", "daily_rows", "append", "day", "day", "avg_chunks", "st", "mean", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "median_chunks", "st", "median", "stats", "chunk_counts", "if", "stats", "chunk_counts", "else", "p95_chunks", "p95", "stats", "chunk_counts", "avg_tokens", "st", "mean", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "median_tokens", "st", "median", "stats", "total_tokens", "if", "stats", "total_tokens", "else", "p95_tokens", "p95", "stats", "total_tokens", "avg_util", "st", "mean", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "median_util", "st", "median", "stats", "budget_utilizations", "if", "stats", "budget_utilizations", "else", "p95_util", "p95", "stats", "budget_utilizations", "avg_latency", "st", "mean", "stats", "latencies", "if", "stats", "latencies", "else", "median_latency", "st", "median", "stats", "latencies", "if", "stats", "latencies", "else", "p95_latency", "p95", "stats", "latencies", "sa", "da", "json", "se", "solicitado", "if", "args", "json", "output", "summary", "period_start", "format_dt", "_as_tz", "first_ts", "tz_mode", "if", "first_ts", "else", "period_end", "format_dt", "_as_tz", "last_ts", "tz_mode", "if", "last_ts", "else", "total_queries", "total_rows", "avg_chunks", "avg_chunks", "avg_tokens", "avg_tokens", "avg_budget_utilization_pct", "avg_util", "avg_latency_ms", "avg_latency", "p95_chunks", "p95_chunks", "p95_tokens", "p95_tokens", "p95_budget_utilization_pct", "p95_util", "p95_latency_ms", "p95_latency", "daily", "daily_rows", "print", "json", "dumps", "output", "indent", "return", "sa", "da", "em", "texto", "formatado", "print", "resumo", "de", "tricas", "do", "mcp", "print", "per", "odo", "format_dt", "_as_tz", "first_ts", "tz_mode", "if", "first_ts", "else", "format_dt", "_as_tz", "last_ts", "tz_mode", "if", "last_ts", "else", "total_rows", "entradas", "print", "print", "dia", "geral", "print", "chunks", "avg_chunks", "p95", "p95_chunks", "print", "tokens", "avg_tokens", "p95", "p95_tokens", "print", "utiliza", "avg_util", "p95", "p95_util", "print", "lat", "ncia", "avg_latency", "ms", "p95", "p95_latency", "ms", "print", "print", "di", "rio", "print", "dia", "chunks", "tokens", "utiliza", "lat", "ncia", "ms"]}
{"chunk_id": "994b72cdd66e64b0e90c4cfb", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 273, "end_line": 322, "content": "    print(\n        f\"Per√≠odo: {format_dt(_as_tz(first_ts, tz_mode)) if first_ts else 'N/A'} a {format_dt(_as_tz(last_ts, tz_mode)) if last_ts else 'N/A'} ({total_rows} entradas)\"\n    )\n    print()\n    print(\"M√©dia Geral:\")\n    print(f\"  Chunks:     {avg_chunks:.1f} (p95: {p95_chunks:.1f})\")\n    print(f\"  Tokens:     {avg_tokens:.0f} (p95: {p95_tokens:.0f})\")\n    print(f\"  Utiliza√ß√£o: {avg_util:.1f}% (p95: {p95_util:.1f}%)\")\n    print(f\"  Lat√™ncia:   {avg_latency:.1f}ms (p95: {p95_latency:.1f}ms)\")\n    print()\n    print(\"Di√°rio:\")\n    print(f\"{'Dia':<12} {'Chunks':<24} {'Tokens':<24} {'Utiliza√ß√£o':<24} {'Lat√™ncia (ms)':<24}\")\n    print(f\"{'':<12} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24} {'avg/median/p95':<24}\")\n    print(\"-\" * 100)\n\n    for row in daily_rows:\n        chunks_str = f\"{row['avg_chunks']:.1f}/{row['median_chunks']:.1f}/{row['p95_chunks']:.1f}\"\n        tokens_str = f\"{row['avg_tokens']:.0f}/{row['median_tokens']:.0f}/{row['p95_tokens']:.0f}\"\n        util_str = f\"{row['avg_util']:.1f}/{row['median_util']:.1f}/{row['p95_util']:.1f}\"\n        latency_str = f\"{row['avg_latency']:.1f}/{row['median_latency']:.1f}/{row['p95_latency']:.1f}\"\n        print(f\"{row['day']:<12} {chunks_str:<24} {tokens_str:<24} {util_str:<24} {latency_str:<24}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP\")\n    parser.add_argument(\n        \"--file\",\n        default=\"\",\n        help=\"Arquivo CSV de m√©tricas espec√≠fico. Se omitido, l√™ todas as fontes padr√µes (context, index e legado).\",\n    )\n    parser.add_argument(\"--since\", type=int, default=0, help=\"Filtrar por dias recentes\")\n    parser.add_argument(\"--filter\", default=\"\", help=\"Filtrar por termo na query\")\n    parser.add_argument(\"--tz\", choices=[\"local\", \"utc\"], default=\"local\", help=\"Fuso hor√°rio para agrupamento di√°rio\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Sa√≠da em JSON\")\n\n    args = parser.parse_args()\n\n    try:\n        rows = load_rows(args)\n        rows = filter_rows(rows, args.since, args.filter)\n        summarize(rows, args)\n    except SystemExit:\n        raise\n    except Exception as e:\n        print(f\"[erro] {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1756161320.4610665, "terms": ["print", "per", "odo", "format_dt", "_as_tz", "first_ts", "tz_mode", "if", "first_ts", "else", "format_dt", "_as_tz", "last_ts", "tz_mode", "if", "last_ts", "else", "total_rows", "entradas", "print", "print", "dia", "geral", "print", "chunks", "avg_chunks", "p95", "p95_chunks", "print", "tokens", "avg_tokens", "p95", "p95_tokens", "print", "utiliza", "avg_util", "p95", "p95_util", "print", "lat", "ncia", "avg_latency", "ms", "p95", "p95_latency", "ms", "print", "print", "di", "rio", "print", "dia", "chunks", "tokens", "utiliza", "lat", "ncia", "ms", "print", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "avg", "median", "p95", "print", "for", "row", "in", "daily_rows", "chunks_str", "row", "avg_chunks", "row", "median_chunks", "row", "p95_chunks", "tokens_str", "row", "avg_tokens", "row", "median_tokens", "row", "p95_tokens", "util_str", "row", "avg_util", "row", "median_util", "row", "p95_util", "latency_str", "row", "avg_latency", "row", "median_latency", "row", "p95_latency", "print", "row", "day", "chunks_str", "tokens_str", "util_str", "latency_str", "def", "main", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "parser", "add_argument", "file", "default", "help", "arquivo", "csv", "de", "tricas", "espec", "fico", "se", "omitido", "todas", "as", "fontes", "padr", "es", "context", "index", "legado", "parser", "add_argument", "since", "type", "int", "default", "help", "filtrar", "por", "dias", "recentes", "parser", "add_argument", "filter", "default", "help", "filtrar", "por", "termo", "na", "query", "parser", "add_argument", "tz", "choices", "local", "utc", "default", "local", "help", "fuso", "hor", "rio", "para", "agrupamento", "di", "rio", "parser", "add_argument", "json", "action", "store_true", "help", "sa", "da", "em", "json", "args", "parser", "parse_args", "try", "rows", "load_rows", "args", "rows", "filter_rows", "rows", "args", "since", "args", "filter", "summarize", "rows", "args", "except", "systemexit", "raise", "except", "exception", "as", "print", "erro", "sys", "exit", "if", "__name__", "__main__", "main"]}
{"chunk_id": "4b55c22950bfc5211b5df830", "file_path": "mcp_system/scripts/summarize_metrics.py", "start_line": 296, "end_line": 322, "content": "def main():\n    parser = argparse.ArgumentParser(description=\"Resumo de m√©tricas do MCP\")\n    parser.add_argument(\n        \"--file\",\n        default=\"\",\n        help=\"Arquivo CSV de m√©tricas espec√≠fico. Se omitido, l√™ todas as fontes padr√µes (context, index e legado).\",\n    )\n    parser.add_argument(\"--since\", type=int, default=0, help=\"Filtrar por dias recentes\")\n    parser.add_argument(\"--filter\", default=\"\", help=\"Filtrar por termo na query\")\n    parser.add_argument(\"--tz\", choices=[\"local\", \"utc\"], default=\"local\", help=\"Fuso hor√°rio para agrupamento di√°rio\")\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Sa√≠da em JSON\")\n\n    args = parser.parse_args()\n\n    try:\n        rows = load_rows(args)\n        rows = filter_rows(rows, args.since, args.filter)\n        summarize(rows, args)\n    except SystemExit:\n        raise\n    except Exception as e:\n        print(f\"[erro] {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()", "mtime": 1756161320.4610665, "terms": ["def", "main", "parser", "argparse", "argumentparser", "description", "resumo", "de", "tricas", "do", "mcp", "parser", "add_argument", "file", "default", "help", "arquivo", "csv", "de", "tricas", "espec", "fico", "se", "omitido", "todas", "as", "fontes", "padr", "es", "context", "index", "legado", "parser", "add_argument", "since", "type", "int", "default", "help", "filtrar", "por", "dias", "recentes", "parser", "add_argument", "filter", "default", "help", "filtrar", "por", "termo", "na", "query", "parser", "add_argument", "tz", "choices", "local", "utc", "default", "local", "help", "fuso", "hor", "rio", "para", "agrupamento", "di", "rio", "parser", "add_argument", "json", "action", "store_true", "help", "sa", "da", "em", "json", "args", "parser", "parse_args", "try", "rows", "load_rows", "args", "rows", "filter_rows", "rows", "args", "since", "args", "filter", "summarize", "rows", "args", "except", "systemexit", "raise", "except", "exception", "as", "print", "erro", "sys", "exit", "if", "__name__", "__main__", "main"]}
{"chunk_id": "14821db9c5d23b2b01aa2daf", "file_path": "mcp_system/scripts/mcp_client_stats.py", "start_line": 1, "end_line": 51, "content": "#!/usr/bin/env python3\n\"\"\"\nCliente MCP para obter estat√≠sticas do sistema\n\"\"\"\n\nimport json\nimport asyncio\nimport os\nimport time\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nasync def get_stats():\n    \"\"\"Obt√©m estat√≠sticas do servidor MCP\"\"\"\n    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[\"-u\", \"-m\", \"mcp_system.mcp_server_enhanced\"],\n        env={\n            \"INDEX_DIR\": os.path.join(base_dir, \".mcp_index\"),\n            \"INDEX_ROOT\": os.path.abspath(os.path.join(base_dir, '..'))\n        }\n    )\n\n    max_retries = 10\n    retry_delay = 1  # segundos\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            for attempt in range(max_retries):\n                try:\n                    tools = await session.list_tools()\n                    print(\"Ferramentas dispon√≠veis:\")\n                    for tool in tools:\n                        print(f\"  - {tool.name}: {tool.description}\")\n\n                    print(\"\\nExecutando comando get_stats...\")\n                    result = await session.call_tool(\"get_stats\", {})\n                    print(\"Resultado:\")\n                    print(json.dumps(result, indent=2, ensure_ascii=False))\n                    break\n                except Exception as e:\n                    print(f\"Tentativa {attempt+1} falhou: {e}\")\n                    if attempt == max_retries - 1:\n                        print(\"Falha ao executar comando ap√≥s v√°rias tentativas.\")\n                        break\n                    await asyncio.sleep(retry_delay)\n\nif __name__ == \"__main__\":\n    asyncio.run(get_stats())", "mtime": 1756157736.4330692, "terms": ["usr", "bin", "env", "python3", "cliente", "mcp", "para", "obter", "estat", "sticas", "do", "sistema", "import", "json", "import", "asyncio", "import", "os", "import", "time", "from", "mcp", "import", "clientsession", "stdioserverparameters", "from", "mcp", "client", "stdio", "import", "stdio_client", "async", "def", "get_stats", "obt", "estat", "sticas", "do", "servidor", "mcp", "base_dir", "os", "path", "abspath", "os", "path", "join", "os", "path", "dirname", "__file__", "server_params", "stdioserverparameters", "command", "python", "args", "mcp_system", "mcp_server_enhanced", "env", "index_dir", "os", "path", "join", "base_dir", "mcp_index", "index_root", "os", "path", "abspath", "os", "path", "join", "base_dir", "max_retries", "retry_delay", "segundos", "async", "with", "stdio_client", "server_params", "as", "read", "write", "async", "with", "clientsession", "read", "write", "as", "session", "for", "attempt", "in", "range", "max_retries", "try", "tools", "await", "session", "list_tools", "print", "ferramentas", "dispon", "veis", "for", "tool", "in", "tools", "print", "tool", "name", "tool", "description", "print", "nexecutando", "comando", "get_stats", "result", "await", "session", "call_tool", "get_stats", "print", "resultado", "print", "json", "dumps", "result", "indent", "ensure_ascii", "false", "break", "except", "exception", "as", "print", "tentativa", "attempt", "falhou", "if", "attempt", "max_retries", "print", "falha", "ao", "executar", "comando", "ap", "rias", "tentativas", "break", "await", "asyncio", "sleep", "retry_delay", "if", "__name__", "__main__", "asyncio", "run", "get_stats"]}
{"chunk_id": "9e88b4f970088b5939f4ff2f", "file_path": "mcp_system/scripts/get_stats.py", "start_line": 1, "end_line": 34, "content": "#!/usr/bin/env python3\n\"\"\"\nScript para obter estat√≠sticas do sistema MCP\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\n\ndef get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estat√≠sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisi√ß√£o para o servidor MCP...\")\n    print(f\"Requisi√ß√£o: {request_str}\")\n    \n    # Aqui voc√™ precisaria se conectar ao servidor MCP\n    # Esta √© uma implementa√ß√£o simplificada\n    print(\"\\nPara executar este comando, voc√™ pode:\")\n    print(\"1. Usar a interface do VS Code com a extens√£o MCP\")\n    print(\"2. Enviar a requisi√ß√£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["usr", "bin", "env", "python3", "script", "para", "obter", "estat", "sticas", "do", "sistema", "mcp", "import", "json", "import", "subprocess", "import", "sys", "def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
{"chunk_id": "a3cb00211e2d5d818758fc89", "file_path": "mcp_system/scripts/get_stats.py", "start_line": 10, "end_line": 34, "content": "def get_mcp_stats():\n    \"\"\"Executa o comando get_stats no servidor MCP\"\"\"\n    # Comando JSON-RPC para obter estat√≠sticas\n    request = {\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"get_stats\",\n        \"params\": {},\n        \"id\": 1\n    }\n    \n    # Converte para string JSON\n    request_str = json.dumps(request)\n    \n    print(\"Enviando requisi√ß√£o para o servidor MCP...\")\n    print(f\"Requisi√ß√£o: {request_str}\")\n    \n    # Aqui voc√™ precisaria se conectar ao servidor MCP\n    # Esta √© uma implementa√ß√£o simplificada\n    print(\"\\nPara executar este comando, voc√™ pode:\")\n    print(\"1. Usar a interface do VS Code com a extens√£o MCP\")\n    print(\"2. Enviar a requisi√ß√£o JSON-RPC diretamente para o servidor\")\n    print(\"3. Usar uma ferramenta como curl ou Postman se o servidor expuser uma API HTTP\")\n\nif __name__ == \"__main__\":\n    get_mcp_stats()", "mtime": 1755726375.9119744, "terms": ["def", "get_mcp_stats", "executa", "comando", "get_stats", "no", "servidor", "mcp", "comando", "json", "rpc", "para", "obter", "estat", "sticas", "request", "jsonrpc", "method", "get_stats", "params", "id", "converte", "para", "string", "json", "request_str", "json", "dumps", "request", "print", "enviando", "requisi", "para", "servidor", "mcp", "print", "requisi", "request_str", "aqui", "voc", "precisaria", "se", "conectar", "ao", "servidor", "mcp", "esta", "uma", "implementa", "simplificada", "print", "npara", "executar", "este", "comando", "voc", "pode", "print", "usar", "interface", "do", "vs", "code", "com", "extens", "mcp", "print", "enviar", "requisi", "json", "rpc", "diretamente", "para", "servidor", "print", "usar", "uma", "ferramenta", "como", "curl", "ou", "postman", "se", "servidor", "expuser", "uma", "api", "http", "if", "__name__", "__main__", "get_mcp_stats"]}
